{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Appendix A: Converting GPT to LLaMA2 🚀🐑**\n",
    "\n",
    "Welcome to the first appendix notebook of our journey through the world of language models! 🎉 After spending significant time mastering the intricacies of GPT architecture, it's time to venture into uncharted territory—or rather, into the terrain of another renowned model family: **LLaMA**, the famous open-source creation by Meta. 🦙✨\n",
    "\n",
    "This notebook is all about **transforming our understanding of GPT into the foundation for LLaMA2**. Why? Because building on what you already know makes everything more intuitive—and let's be honest, a bit more fun! 😄 \n",
    "\n",
    "While GPT2 and LLaMA2 share a lot of the same DNA (Transformers FTW!), there are a few **interesting tweaks** in the architecture that set LLaMA2 apart. Think of this as a chance to compare notes, explore optimizations, and learn new tricks—all while grounding ourselves in concepts we're already familiar with.\n",
    "\n",
    "By the end of this notebook, you'll have a clear understanding of how to:\n",
    "\n",
    "1. Adapt GPT2's core ideas into LLaMA2's structure.  \n",
    "2. Dive into architectural modifications like **RMSNorm**, **rotary embeddings**, and Meta's flavor of feed-forward layers.  \n",
    "3. Appreciate the thoughtful engineering behind LLaMA2's open-source power.\n",
    "\n",
    "So, let's get started and see how the GPT torch passes on to its cousin from the Meta family! Ready? Let’s code. 🧑‍💻\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/llama.png\" alt=\"LLaMA Image\" />\n",
    "</p>\n",
    "\n",
    "## NOTE: This notebook has minimal comments; it's just a code snippet. I rely on your previous experience with the project. Think of this as an exercise and not a tutorial. Have FUN!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PyTorch, a deep learning framework for tensor operations and building neural networks\n",
    "import torch \n",
    "import torch.nn as nn  # Importing the `nn` module for building neural network layers\n",
    "import torch.nn.functional as F  # Importing the functional API for PyTorch, which provides functions for activation layers, loss, etc.\n",
    "\n",
    "# Importing the Hugging Face Hub library for downloading and interacting with pre-trained models\n",
    "import huggingface_hub \n",
    "\n",
    "# Importing SentencePiece, a library for tokenization, used for subword units like Byte Pair Encoding (BPE) and Unigram\n",
    "import sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of Main Architectural Differences Between GPT and LLaMA**\n",
    "\n",
    "| **Aspect**               | **GPT**                                      | **LLaMA**                                   |\n",
    "|---------------------------|----------------------------------------------|---------------------------------------------|\n",
    "| **Activation Function**   | GELU (Gaussian Error Linear Unit)           | SiLU (Sigmoid Linear Unit)                 |\n",
    "| **Normalization**         | LayerNorm                                   | RMSNorm (Root Mean Square Normalization)   |\n",
    "| **Feedforward Network**   | Standard dense layers                       | Gated Linear Units (GLUs) for nonlinearity |\n",
    "| **Attention Mechanism**   | Standard scaled dot-product attention       | Includes Rotary Embeddings for efficiency  |\n",
    "| **Positional Encoding**   | Learned absolute positional embeddings      | Rotary positional embeddings               |\n",
    "\n",
    "### **Key Takeaways**\n",
    "- LLaMA's architectural tweaks, like RMSNorm and rotary embeddings, focus on efficiency while maintaining strong performance.\n",
    "- GPT relies on more traditional design choices, such as LayerNorm and absolute positional embeddings, which have proven effective but may not be as efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# from notebooks/3.GPT.ipynb\n",
    "#####################################\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps \n",
    "        self.scale  =nn.Parameter(torch.ones(emb_dim))\n",
    "        self.emd_dim = emb_dim\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = x * torch.rsqrt(mean + self.eps)\n",
    "        output = (self.scale * x_norm).to(dtype=x.dtype) \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.randn(2, 3, 4)\n",
    "\n",
    "our_rms = RMSNorm(emb_dim=batch.shape[-1])\n",
    "torch_rms = torch.nn.RMSNorm(batch.shape[-1], eps=1e-5)\n",
    "\n",
    "assert torch.allclose(our_rms(batch), torch_rms(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# from notebooks/3.GPT.ipynb\n",
    "#####################################\n",
    "\n",
    "# class GELU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return 0.5 * x * (1 + torch.tanh(\n",
    "#             torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "#             (x + 0.044715 * torch.pow(x, 3))\n",
    "#         ))\n",
    "\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_silu = SiLU()\n",
    "torch_silu = torch.nn.SiLU()\n",
    "\n",
    "assert torch.allclose(our_silu(batch), torch_silu(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# from notebooks/3.GPT.ipynb\n",
    "#####################################\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "#             GELU(),\n",
    "#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x2 = self.fc2(x1)\n",
    "        x = self.silu(x1) * x2        \n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
    "    # As written in the paragraph 3.2.2 of the paper\n",
    "    # >> In order to generalize our results in 2D to any xi ∈ Rd where **d is even**, [...]\n",
    "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
    "    # Build the theta parameter\n",
    "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ... dim/2]\n",
    "    # Shape: (Head_Dim / 2)\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    # Shape: (Head_Dim / 2)\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device) # (Dim / 2)\n",
    "    # Construct the positions (the \"m\" parameter)\n",
    "    # Shape: (Seq_Len)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    # Multiply each theta by each position using the outer product.\n",
    "    # Shape: (Seq_Len) outer_product* (Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    # We can compute complex numbers in the polar form c = R * exp(m * theta), where R = 1 as follows:\n",
    "    # (Seq_Len, Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n",
    "\n",
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "    # (B, Seq_Len, H, Head_Dim) -> (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. So we need to add the batch dimension and the head dimension\n",
    "    # (Seq_Len, Head_Dim/2) --> (1, Seq_Len, 1, Head_Dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
    "    # (B, Seq_Len, H, Head_Dim/2) * (1, Seq_Len, 1, Head_Dim/2) = (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # Convert the complex number back to the real number\n",
    "    # (B, Seq_Len, H, Head_Dim/2) -> (B, Seq_Len, H, Head_Dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # (B, Seq_Len, H, Head_Dim/2, 2) -> (B, Seq_Len, H, Head_Dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 10, 8, 16])\n",
      "Rotated shape: torch.Size([4, 10, 8, 16])\n",
      "tensor([[[[ 4.7546e-01, -1.8017e-01,  1.5203e-01,  ...,  1.3537e+00,\n",
      "            2.8199e-01, -2.5730e-01],\n",
      "          [ 4.4417e-01,  5.0913e-02,  1.3995e+00,  ..., -6.5992e-01,\n",
      "           -7.2023e-01, -6.0064e-01],\n",
      "          [-5.6531e-02, -1.1139e+00,  3.6032e-01,  ...,  3.5922e-01,\n",
      "            4.8927e-01, -2.7623e-01],\n",
      "          ...,\n",
      "          [ 1.2838e+00, -6.0748e-01, -6.2717e-01,  ..., -2.1262e-01,\n",
      "            1.5759e-02, -6.4197e-02],\n",
      "          [-9.8002e-01,  1.2238e+00, -2.2663e-01,  ..., -1.1541e+00,\n",
      "            2.6489e-02, -4.8589e-01],\n",
      "          [ 1.0867e+00, -2.3381e+00, -4.2838e-01,  ...,  3.9752e-02,\n",
      "           -4.3039e-01, -5.5511e-02]],\n",
      "\n",
      "         [[ 1.1230e+00, -3.1704e-01, -7.7769e-01,  ...,  1.2925e+00,\n",
      "           -3.4155e-01, -1.5815e-02],\n",
      "          [-1.3986e+00, -5.2680e-01, -1.5462e+00,  ...,  1.1274e+00,\n",
      "            1.4862e+00, -2.7242e-01],\n",
      "          [-2.8332e-01,  5.6965e-01,  5.5136e-01,  ...,  6.6288e-01,\n",
      "            8.8273e-01,  8.7738e-01],\n",
      "          ...,\n",
      "          [-1.8567e-01, -2.1498e-01,  2.6857e-01,  ..., -9.9471e-01,\n",
      "            5.7629e-01, -3.4718e-01],\n",
      "          [-1.1037e+00, -2.9779e-01,  7.4611e-01,  ...,  1.3551e+00,\n",
      "            1.3037e-02,  9.9707e-01],\n",
      "          [-3.9939e-01,  5.7626e-01,  6.7886e-01,  ...,  1.1641e-01,\n",
      "            9.7183e-02, -1.5940e-01]],\n",
      "\n",
      "         [[ 1.4980e+00, -1.3281e-01,  1.0612e+00,  ..., -7.9608e-02,\n",
      "           -3.3002e-01, -2.9946e-01],\n",
      "          [ 1.1130e-01, -2.7738e-01, -1.3597e+00,  ...,  5.0919e-01,\n",
      "           -1.2125e+00,  6.6099e-01],\n",
      "          [-5.1876e-01, -3.5231e-01,  5.7888e-01,  ...,  4.7569e-03,\n",
      "            1.0316e+00,  2.9113e-01],\n",
      "          ...,\n",
      "          [-1.5520e+00,  1.9941e-01, -1.7702e-01,  ..., -8.4367e-02,\n",
      "           -1.5064e-01,  2.7406e-02],\n",
      "          [-4.4896e-01,  1.6007e+00,  8.5655e-01,  ..., -2.8848e-01,\n",
      "           -3.0114e-01, -9.5297e-02],\n",
      "          [-7.6508e-01,  1.7808e-01, -9.9533e-02,  ..., -4.6372e-01,\n",
      "            9.5549e-01, -8.8338e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.7938e-01, -1.0535e+00,  8.3046e-01,  ..., -8.5041e-01,\n",
      "           -6.1329e-02, -1.7859e+00],\n",
      "          [-1.3219e-01,  7.1487e-01, -1.1031e+00,  ..., -2.7728e-01,\n",
      "            6.2667e-01, -8.6077e-01],\n",
      "          [ 1.1552e+00,  4.6030e-01, -1.3442e-01,  ..., -5.6290e-01,\n",
      "           -6.0814e-01, -2.6660e-01],\n",
      "          ...,\n",
      "          [-2.0252e+00, -1.5266e-02, -1.7792e-01,  ..., -7.1261e-01,\n",
      "            1.6481e+00, -2.0770e+00],\n",
      "          [ 1.3230e+00, -4.9845e-01, -1.1088e+00,  ..., -1.7749e-01,\n",
      "            1.7567e+00,  7.9242e-01],\n",
      "          [ 1.0476e+00, -7.9083e-01, -1.4621e+00,  ...,  6.5496e-01,\n",
      "           -1.1849e-01,  3.5970e-02]],\n",
      "\n",
      "         [[ 4.8458e-01,  7.4892e-01,  2.9264e-02,  ..., -1.0523e+00,\n",
      "           -2.5586e-01,  6.5390e-01],\n",
      "          [ 1.4713e-01,  4.3074e-01,  1.3988e+00,  ..., -2.0208e-01,\n",
      "           -4.4078e-01, -2.6964e+00],\n",
      "          [ 1.4289e-01, -2.5581e-01, -1.4353e-01,  ..., -8.1924e-01,\n",
      "           -7.4586e-02,  6.9562e-01],\n",
      "          ...,\n",
      "          [-1.0318e-01,  9.5642e-01,  1.7799e-03,  ..., -1.4779e+00,\n",
      "           -2.2756e-01, -7.3959e-01],\n",
      "          [-6.9052e-01,  1.0885e+00, -5.5633e-01,  ..., -1.0182e+00,\n",
      "            4.5690e-01, -9.3585e-01],\n",
      "          [ 8.5141e-01,  6.8191e-01, -1.1734e-01,  ...,  2.8108e+00,\n",
      "           -6.8572e-01, -4.4113e-02]],\n",
      "\n",
      "         [[-3.5858e-02, -1.3682e+00,  7.3063e-02,  ..., -1.3403e+00,\n",
      "            1.1288e+00,  8.7646e-01],\n",
      "          [-4.4976e-01,  6.4874e-01, -6.2117e-01,  ..., -2.3436e+00,\n",
      "            6.3172e-01,  8.8776e-02],\n",
      "          [ 5.2968e-01,  7.6828e-01,  1.4360e+00,  ..., -6.5655e-01,\n",
      "           -4.7098e-01,  9.9169e-01],\n",
      "          ...,\n",
      "          [-4.8465e-01,  7.9808e-01, -8.6454e-01,  ...,  1.1571e-01,\n",
      "            2.0069e-01,  8.6974e-01],\n",
      "          [-9.1843e-01, -1.8867e+00, -1.1632e+00,  ...,  5.8201e-01,\n",
      "           -4.8826e-01, -5.0963e-02],\n",
      "          [-4.7093e-01,  1.0158e+00,  1.7475e+00,  ...,  5.8566e-01,\n",
      "            5.8449e-01, -4.5557e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0407e-01,  8.5940e-01,  8.0562e-01,  ..., -6.5475e-01,\n",
      "            3.3781e-01,  1.3393e+00],\n",
      "          [-4.3144e-01, -2.5398e-01,  4.0016e-02,  ...,  4.6965e-01,\n",
      "            8.8735e-01, -1.3893e+00],\n",
      "          [-8.2306e-01,  6.6901e-01, -1.0449e+00,  ..., -5.5266e-01,\n",
      "            3.7157e-01, -7.3954e-01],\n",
      "          ...,\n",
      "          [ 9.5799e-01, -1.2491e+00, -1.8850e-01,  ...,  9.6278e-01,\n",
      "            1.0010e-01,  1.1937e-01],\n",
      "          [ 3.9162e-01, -1.8753e+00,  4.8933e-01,  ..., -1.4136e+00,\n",
      "           -3.4926e-01, -9.5939e-02],\n",
      "          [ 8.7425e-01,  7.9855e-01, -1.0177e+00,  ...,  3.1911e-01,\n",
      "            2.3264e-01, -1.0931e-01]],\n",
      "\n",
      "         [[ 4.2252e-01,  1.0273e+00,  6.7042e-01,  ...,  3.3276e-01,\n",
      "            6.8693e-01,  7.6463e-02],\n",
      "          [-7.0114e-01,  6.5188e-01, -3.1937e-01,  ...,  1.0924e+00,\n",
      "           -7.3238e-01,  1.0094e+00],\n",
      "          [-2.7602e-01, -4.5954e-01,  7.4790e-01,  ..., -7.3023e-01,\n",
      "           -1.0397e+00, -7.4980e-01],\n",
      "          ...,\n",
      "          [-2.2912e-01, -1.8087e+00,  4.1333e-01,  ...,  4.1995e-01,\n",
      "           -5.0335e-01,  7.1719e-01],\n",
      "          [-1.5975e+00, -1.1890e+00,  2.2140e+00,  ...,  2.8460e-01,\n",
      "           -1.4266e-01, -8.1613e-01],\n",
      "          [ 3.0661e+00, -2.1545e+00,  4.6024e-01,  ..., -4.0578e-01,\n",
      "           -2.0114e+00,  2.5394e-01]],\n",
      "\n",
      "         [[ 2.3690e-01,  7.9902e-02, -2.0166e-01,  ..., -5.9275e-01,\n",
      "           -9.0948e-01,  1.4764e+00],\n",
      "          [-1.4462e+00, -1.4299e+00,  1.6770e+00,  ..., -1.6224e+00,\n",
      "           -8.2690e-01,  2.7952e+00],\n",
      "          [-8.1256e-01, -8.1261e-01, -4.6023e-01,  ...,  1.5869e+00,\n",
      "           -5.1393e-01, -1.8928e-01],\n",
      "          ...,\n",
      "          [-2.5896e-01, -1.6567e-01, -1.0499e+00,  ..., -1.5226e-01,\n",
      "            2.0759e-03,  2.1649e+00],\n",
      "          [-2.9449e-01, -1.4211e+00, -9.2113e-01,  ...,  1.3040e-01,\n",
      "           -1.1838e+00,  5.8511e-01],\n",
      "          [-8.5467e-02,  4.1209e-01,  5.3203e-01,  ..., -8.0389e-01,\n",
      "            9.9549e-01, -8.3241e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3182e+00, -1.5184e-01,  1.7099e-01,  ..., -4.5182e-02,\n",
      "            1.8854e-01, -7.5323e-02],\n",
      "          [-6.0957e-01, -2.5435e-01,  1.8440e-01,  ...,  9.6751e-01,\n",
      "            1.8481e-01,  9.5506e-01],\n",
      "          [ 1.3868e-01,  2.4364e+00, -9.1837e-01,  ..., -4.6851e-01,\n",
      "           -8.7423e-01,  1.9328e-01],\n",
      "          ...,\n",
      "          [-1.9976e+00, -2.3254e-01, -6.7953e-01,  ..., -3.6109e-01,\n",
      "            2.2555e-01, -5.2776e-01],\n",
      "          [-4.6793e-01, -5.7221e-01,  1.0563e-02,  ...,  4.3806e-01,\n",
      "            1.0055e+00, -2.6923e-01],\n",
      "          [-1.6897e+00, -2.1201e+00,  3.9865e-01,  ..., -4.6984e-01,\n",
      "            7.1496e-01,  7.5214e-01]],\n",
      "\n",
      "         [[ 1.1271e+00, -1.3401e+00, -3.0579e-01,  ..., -6.3978e-01,\n",
      "           -1.1007e+00,  4.6671e-02],\n",
      "          [ 7.7380e-01,  1.0386e+00,  4.3772e-01,  ...,  7.4741e-01,\n",
      "            7.2957e-01,  1.1249e-01],\n",
      "          [-6.8458e-01, -4.5665e-01, -1.0224e+00,  ...,  1.0471e-01,\n",
      "            2.1686e-02,  7.3713e-01],\n",
      "          ...,\n",
      "          [-7.1285e-01,  1.2308e+00,  5.5749e-01,  ..., -4.9282e-01,\n",
      "            3.5119e-01,  8.6841e-01],\n",
      "          [-2.5420e-02, -1.3897e+00,  5.5628e-01,  ..., -1.0390e+00,\n",
      "           -3.2667e-01, -4.6959e-01],\n",
      "          [-8.8346e-01,  7.1498e-01, -6.8368e-01,  ..., -5.8881e-02,\n",
      "            9.2107e-01, -1.6511e-01]],\n",
      "\n",
      "         [[ 5.8326e-01, -1.1313e+00,  2.2483e-01,  ...,  2.3220e+00,\n",
      "            2.4868e+00, -3.6519e-01],\n",
      "          [ 7.0353e-01,  6.2462e-01, -2.1085e+00,  ...,  8.1457e-01,\n",
      "           -9.6322e-01,  7.6295e-01],\n",
      "          [ 8.5921e-01, -8.6186e-01,  9.6745e-01,  ...,  1.2033e+00,\n",
      "            2.1454e+00, -4.3637e-01],\n",
      "          ...,\n",
      "          [-1.1658e+00,  3.0023e-01,  1.1323e-01,  ...,  4.1055e-01,\n",
      "            2.7069e+00, -1.4978e+00],\n",
      "          [ 1.3680e+00,  1.0431e-01, -1.2690e+00,  ..., -9.6958e-01,\n",
      "           -1.2312e+00, -5.3655e-01],\n",
      "          [-1.5754e-01,  7.8740e-02,  1.0251e+00,  ..., -8.8226e-01,\n",
      "           -1.1548e+00,  3.1596e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.5544e-01,  1.3858e-01, -1.0245e+00,  ...,  2.2528e+00,\n",
      "           -1.3328e+00,  4.0550e-01],\n",
      "          [ 2.1944e-01,  5.8683e-01,  3.7604e-02,  ..., -3.0330e-01,\n",
      "           -7.9074e-01,  1.1116e-01],\n",
      "          [ 1.0983e+00,  4.5868e-01, -3.6004e-01,  ..., -2.0801e+00,\n",
      "            7.7125e-01,  1.5934e+00],\n",
      "          ...,\n",
      "          [-4.4835e-01, -9.4645e-01,  8.8895e-01,  ...,  9.7837e-01,\n",
      "           -9.6943e-01,  6.5193e-01],\n",
      "          [ 1.0120e+00, -3.0773e-01, -1.1997e+00,  ...,  4.1363e-01,\n",
      "           -1.9386e+00,  9.5698e-01],\n",
      "          [-4.2587e-01,  6.5360e-01, -6.5762e-01,  ...,  2.0228e-01,\n",
      "           -1.7299e+00,  5.8702e-01]],\n",
      "\n",
      "         [[ 1.0638e+00, -2.0341e+00, -1.7560e+00,  ...,  1.3649e-02,\n",
      "            1.2100e+00,  6.3694e-01],\n",
      "          [-3.3155e-01,  6.9679e-01,  1.6550e-01,  ...,  1.4204e+00,\n",
      "            3.0885e-01, -1.1214e+00],\n",
      "          [ 6.7829e-01, -1.7941e-01, -1.9694e-01,  ...,  3.4184e-01,\n",
      "            1.7227e+00, -1.0695e+00],\n",
      "          ...,\n",
      "          [ 1.5391e+00,  1.9074e+00,  1.1929e-01,  ...,  1.0809e-02,\n",
      "            2.9971e-01, -1.2724e+00],\n",
      "          [-4.4605e-01,  4.5030e-01, -4.3577e-03,  ...,  1.6671e-01,\n",
      "           -2.4168e-01,  1.1004e+00],\n",
      "          [-8.6708e-01, -2.6467e-01, -3.3417e-01,  ..., -9.9300e-01,\n",
      "           -1.4697e+00, -1.2819e-01]],\n",
      "\n",
      "         [[ 2.6805e-01,  3.1083e-01,  1.2817e-01,  ...,  1.3383e-01,\n",
      "            8.5663e-01,  1.2433e+00],\n",
      "          [ 2.5373e-01,  1.3943e+00, -3.2950e+00,  ..., -7.9765e-01,\n",
      "           -1.0392e+00,  1.6661e-01],\n",
      "          [-1.1302e+00,  1.6159e+00,  5.0871e-01,  ...,  6.6244e-01,\n",
      "            1.8680e-02,  1.0571e+00],\n",
      "          ...,\n",
      "          [ 3.6202e-01,  8.4749e-01, -1.3908e+00,  ...,  6.6737e-01,\n",
      "            7.3146e-01, -1.0172e+00],\n",
      "          [-5.2915e-01, -1.8917e+00,  1.8425e+00,  ...,  1.5732e+00,\n",
      "           -2.4060e+00, -7.1888e-01],\n",
      "          [-1.2872e-02,  2.5863e+00, -6.3241e-01,  ..., -2.5407e-02,\n",
      "           -5.2797e-01, -1.2125e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3957e+00,  6.5538e-01, -2.4848e-01,  ..., -2.4322e-01,\n",
      "           -4.9381e-01,  6.8657e-01],\n",
      "          [-1.2798e+00,  1.6908e-01, -1.9813e+00,  ...,  2.2664e-01,\n",
      "           -9.7892e-01, -4.5696e-01],\n",
      "          [ 4.0757e-01,  1.4253e-01,  1.5837e-01,  ..., -1.6294e+00,\n",
      "           -5.2676e-01,  6.4058e-02],\n",
      "          ...,\n",
      "          [ 2.1301e+00, -1.2200e+00,  1.0633e+00,  ..., -1.0468e+00,\n",
      "           -1.1636e+00,  1.3973e+00],\n",
      "          [ 1.7726e-01,  1.6256e+00,  4.5276e-01,  ..., -8.0823e-01,\n",
      "           -8.9714e-01, -9.2725e-01],\n",
      "          [ 1.3251e+00,  6.9084e-01, -1.3412e+00,  ...,  2.1877e+00,\n",
      "            6.7223e-01,  1.2812e+00]],\n",
      "\n",
      "         [[ 8.1267e-01, -3.1463e-01, -1.4990e+00,  ..., -2.4521e-01,\n",
      "            1.2399e+00,  1.2024e+00],\n",
      "          [ 4.2975e-01,  1.2195e+00, -7.3945e-01,  ...,  2.1496e+00,\n",
      "           -7.6260e-01,  7.5156e-01],\n",
      "          [-2.9473e+00, -9.4866e-01,  1.9556e+00,  ...,  1.8904e+00,\n",
      "           -3.7465e-01,  7.4574e-01],\n",
      "          ...,\n",
      "          [-9.5790e-01, -3.9572e-01, -7.7037e-01,  ...,  1.6916e+00,\n",
      "            3.1861e-01, -8.2477e-01],\n",
      "          [-9.0427e-01, -4.2478e-01,  1.8205e+00,  ..., -5.7763e-01,\n",
      "           -1.3365e+00, -2.7651e-01],\n",
      "          [-3.7445e-01, -4.2040e-01, -1.2756e+00,  ..., -8.7319e-03,\n",
      "           -6.7841e-01, -1.5093e+00]],\n",
      "\n",
      "         [[ 2.4521e-01, -1.7642e+00, -8.4026e-01,  ...,  1.5242e-01,\n",
      "           -9.3610e-01,  1.8075e+00],\n",
      "          [-1.2569e+00,  5.2826e-01, -1.2183e-01,  ...,  4.0385e-01,\n",
      "            1.9616e+00, -5.1397e-01],\n",
      "          [-1.3404e+00, -2.2865e-01,  1.0871e+00,  ...,  5.7400e-01,\n",
      "            5.7428e-01, -5.3601e-01],\n",
      "          ...,\n",
      "          [-1.1083e+00,  2.8727e-02,  5.7251e-01,  ...,  4.5517e-01,\n",
      "           -1.7179e+00,  5.5150e-01],\n",
      "          [ 1.8034e+00,  4.8586e-01, -1.6656e-01,  ...,  5.6301e-01,\n",
      "            7.9258e-01, -5.2527e-01],\n",
      "          [ 1.2166e-01,  8.8437e-01, -1.7716e-01,  ...,  1.6298e-02,\n",
      "            1.4357e+00,  1.6441e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5877e-01,  2.7112e-01, -3.3162e-01,  ...,  2.3949e+00,\n",
      "           -3.0571e-01,  4.0587e-01],\n",
      "          [ 5.2317e-01, -2.1356e+00, -2.0045e+00,  ...,  8.9884e-01,\n",
      "           -1.1529e+00,  3.0627e-01],\n",
      "          [-1.4784e-01,  1.0894e+00, -3.1036e-01,  ..., -1.5073e+00,\n",
      "           -1.3222e+00, -1.1589e-01],\n",
      "          ...,\n",
      "          [-8.2467e-01, -7.8492e-02,  1.0026e+00,  ..., -1.4667e+00,\n",
      "           -5.0696e-01, -7.9320e-01],\n",
      "          [-2.1215e+00, -5.4703e-01,  2.1977e+00,  ..., -7.9701e-02,\n",
      "            1.2143e+00,  9.5125e-01],\n",
      "          [-8.6465e-01, -5.3655e-02, -2.0712e+00,  ..., -4.7417e-01,\n",
      "           -1.1221e+00, -3.6970e-01]],\n",
      "\n",
      "         [[ 8.3753e-01,  1.4411e+00, -4.5804e-01,  ...,  5.7726e-01,\n",
      "            9.3272e-01, -1.6030e+00],\n",
      "          [ 2.0311e+00,  2.1055e-02,  2.9235e-02,  ...,  2.5643e-01,\n",
      "            3.1370e-01,  4.5784e-01],\n",
      "          [ 1.1694e-01,  9.7154e-01, -3.3291e-01,  ...,  1.5722e-01,\n",
      "           -3.4682e-01, -7.0938e-02],\n",
      "          ...,\n",
      "          [ 7.2245e-01, -1.1171e+00, -1.0202e-02,  ..., -1.7462e+00,\n",
      "           -5.2329e-01,  7.6815e-01],\n",
      "          [-2.0935e+00, -7.9152e-03,  1.1397e+00,  ..., -5.1927e-01,\n",
      "            2.8554e-01,  1.4600e+00],\n",
      "          [-2.0514e-01, -3.8802e-01,  9.3677e-02,  ...,  3.4044e+00,\n",
      "            4.1931e-01, -9.3263e-01]],\n",
      "\n",
      "         [[ 2.0542e+00, -5.6243e-01,  1.1109e+00,  ...,  4.1837e-01,\n",
      "           -3.0147e-01,  6.6539e-01],\n",
      "          [-1.0797e-01,  4.2041e-01, -4.0737e-01,  ...,  5.4028e-02,\n",
      "            8.5558e-01,  1.3526e+00],\n",
      "          [-1.6485e+00,  2.9384e-01, -6.3769e-01,  ..., -2.3661e-01,\n",
      "            2.5715e+00,  2.9017e-02],\n",
      "          ...,\n",
      "          [ 1.3687e+00,  2.4446e-01, -1.5936e+00,  ...,  1.2125e+00,\n",
      "            2.9017e-01,  4.8303e-01],\n",
      "          [-8.7247e-02, -1.1004e+00, -9.2555e-01,  ..., -2.0133e+00,\n",
      "           -5.4199e-02, -3.5384e-02],\n",
      "          [-2.6290e-02,  3.5950e-01,  1.1326e+00,  ..., -8.2067e-01,\n",
      "            1.5933e+00, -9.9317e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.9339e-01, -8.8843e-02,  3.1519e-01,  ...,  2.3606e-01,\n",
      "           -1.5149e+00, -1.4931e+00],\n",
      "          [-6.2398e-02, -3.4920e-01,  1.2966e+00,  ...,  5.0437e-01,\n",
      "            9.8102e-01,  7.0415e-02],\n",
      "          [-4.7706e-01, -8.1461e-01, -1.2980e+00,  ...,  7.3228e-02,\n",
      "           -1.0712e+00, -8.1656e-02],\n",
      "          ...,\n",
      "          [-1.5946e-01, -7.4743e-01,  1.2188e+00,  ..., -7.8422e-01,\n",
      "            1.4070e+00, -2.9094e-01],\n",
      "          [ 1.4629e+00, -3.6877e-01, -1.0881e+00,  ..., -3.0832e-01,\n",
      "            1.1931e+00,  1.7757e+00],\n",
      "          [-1.6833e-01, -5.7753e-01, -7.8053e-01,  ...,  6.8871e-01,\n",
      "           -1.1966e+00,  8.1256e-01]],\n",
      "\n",
      "         [[ 1.2744e+00, -1.1450e-01,  1.7005e-02,  ..., -1.1682e+00,\n",
      "           -1.1080e+00,  5.0157e-01],\n",
      "          [ 3.9523e-01, -6.0736e-01, -1.4163e+00,  ...,  2.3001e-01,\n",
      "           -9.9181e-01,  1.6420e+00],\n",
      "          [-3.7525e-01,  1.1132e+00,  2.6297e-01,  ...,  1.5858e+00,\n",
      "            1.4218e+00, -8.6125e-01],\n",
      "          ...,\n",
      "          [ 5.0274e-02,  4.8587e-01,  1.1697e+00,  ...,  1.4426e+00,\n",
      "           -1.7176e+00, -6.7968e-02],\n",
      "          [-8.9158e-01, -7.2632e-01, -1.0593e+00,  ...,  4.0264e-01,\n",
      "            4.3291e-01,  9.4538e-01],\n",
      "          [-3.4480e-01,  5.2482e-01, -1.7108e+00,  ...,  1.9455e+00,\n",
      "            1.4403e+00, -1.5819e+00]],\n",
      "\n",
      "         [[ 1.0817e+00, -1.2766e+00,  5.7450e-01,  ...,  3.3346e-01,\n",
      "           -1.0934e+00,  1.7766e+00],\n",
      "          [-2.2162e-01,  2.0186e+00, -7.4161e-01,  ..., -5.5004e-01,\n",
      "            1.7213e-01, -5.1074e-02],\n",
      "          [ 9.8210e-01,  1.3497e-01, -1.0030e-01,  ..., -1.2597e+00,\n",
      "            1.8676e+00, -1.4533e+00],\n",
      "          ...,\n",
      "          [-2.8929e-01, -1.3161e+00,  5.6088e-01,  ..., -1.3662e+00,\n",
      "           -1.9978e+00,  1.3851e+00],\n",
      "          [-6.6165e-01,  1.2192e+00, -2.8097e+00,  ..., -4.7284e-02,\n",
      "            2.8293e-01, -2.3958e-01],\n",
      "          [ 7.2829e-01,  6.0443e-01,  1.4087e-01,  ..., -4.2058e-02,\n",
      "           -1.5433e-01,  4.8401e-01]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "head_dim = 16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Generate random input tensor (batch_size, seq_len, num_heads, head_dim)\n",
    "x = torch.randn(batch_size, seq_len, 8, head_dim, device=device)\n",
    "\n",
    "# Precompute frequency complex numbers\n",
    "freqs_complex = precompute_theta_pos_frequencies(head_dim, seq_len, device)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "x_rot = apply_rotary_embeddings(x, freqs_complex, device)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Rotated shape:\", x_rot.shape)   \n",
    "print(x_rot[:5]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Define the MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, seq_len, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_in = d_in  # Input dimension\n",
    "        self.d_out = d_out  # Output dimension\n",
    "        self.seq_len = seq_len  # Sequence length\n",
    "        self.num_heads = num_heads  # Number of attention heads\n",
    "        self.head_dim = d_out // num_heads  # Dimension of each head\n",
    "        \n",
    "        assert self.head_dim * num_heads == d_out, \"Output dimension must be divisible by number of heads\"\n",
    "        \n",
    "        # Linear layers for query, key, and value\n",
    "        self.query = nn.Linear(d_in, d_out)\n",
    "        self.key = nn.Linear(d_in, d_out)\n",
    "        self.value = nn.Linear(d_in, d_out)\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        # Softmax for attention scores\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Precompute rotary embeddings (not implemented here, adjust as needed)\n",
    "        # self.freqs_complex = precompute_theta_pos_frequencies(...)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, D_in = x.shape  # Batch size, sequence length, input dimension\n",
    "        \n",
    "        assert T == self.seq_len, \"Input sequence length must match the configured sequence length\"\n",
    "        assert D_in == self.d_in, \"Input dimension must match the configured input dimension\"\n",
    "        \n",
    "        # Linear projections of query, key, and value\n",
    "        Q = self.query(x)  # (B, T, d_out)\n",
    "        K = self.key(x)  # (B, T, d_out)\n",
    "        V = self.value(x)  # (B, T, d_out)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, T, num_heads, head_dim)\n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1))  # (B, num_heads, T, T)\n",
    "        attention = self.softmax(energy / math.sqrt(self.head_dim))  # Normalize energy\n",
    "        \n",
    "        # Apply attention weights to value\n",
    "        out = torch.matmul(attention, V)  # (B, num_heads, T, head_dim)\n",
    "        \n",
    "        # Combine heads and pass through final linear layer\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_out)  # (B, T, d_out)\n",
    "        out = self.fc_out(out)  # (B, T, d_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Set up the model parameters\n",
    "batch_size = 32\n",
    "seq_len = 10  # Ensure seq_len matches the configured sequence length\n",
    "d_in = 64  # Input dimension\n",
    "d_out = 128  # Output dimension\n",
    "num_heads = 4  # Number of attention heads\n",
    "\n",
    "# Select the device: CPU or GPU (if available)\n",
    "device = 'cpu'  # You can use 'cuda' if you have a compatible GPU and CUDA is enabled\n",
    "\n",
    "# Initialize the MultiHeadAttention model and move it to the selected device\n",
    "mha = MultiHeadAttention(d_in, d_out, seq_len=seq_len, num_heads=num_heads).to(device)\n",
    "\n",
    "# Generate a random input tensor with shape (batch_size, seq_len, d_in)\n",
    "x = torch.randn(batch_size, seq_len, d_in, device=device)\n",
    "\n",
    "# Forward pass through the model\n",
    "output = mha(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_in = cfg[\"emb_dim\"],\n",
    "                                       d_out=cfg[\"emb_dim\"],\n",
    "                                        seq_len=cfg[\"context_length\"],\n",
    "                                        num_heads=cfg[\"num_heads\"]\n",
    "        )\n",
    "        \n",
    "        self.ff = FeedForward(cfg)\n",
    "        ##########################################\n",
    "        # self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ########################################\n",
    "\n",
    "        # self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x \n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = shortcut + x\n",
    "        # x = self.drop_resid(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPTModel(nn.Module):\n",
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        # self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[Transformer(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds  # + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        # x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32000,     # Vocabulary size\n",
    "    \"context_length\": 4096,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"num_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward\n",
    "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama2Model(LLAMA2_CONFIG_7B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that Meta AI requires that you accept the Llama 2 licensing terms before you can download the files; to do this, you have to create a Hugging Face Hub account and visit the meta-llama/Llama-2-7b repository to accept the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",\n",
    "    filename=\"tokenizer.model\",\n",
    "    local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class LlamaTokenizer:\n",
    "    def __init__(self, tokenizer_file):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(tokenizer_file)\n",
    "        self.tokenizer = sp\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode_as_ids(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode_pieces(ids)\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UTILS.finetune_utils import generate , text_to_token_ids, token_ids_to_text\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = hf_hub_download(\n",
    "   repo_id=\"meta-llama/Llama-2-7b\",\n",
    "   filename=\"consolidated.00.pth\",\n",
    "   local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weights_file, weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"tok_embeddings.weight\"])\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"layers.{l}.attention.wq.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"layers.{l}.attention.wk.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"layers.{l}.attention.wv.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"layers.{l}.attention.wo.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"layers.{l}.attention_norm.weight\"]\n",
    "        )\n",
    "\n",
    "        # Load FeedForward weights\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w1.weight\"]\n",
    "        )\n",
    "        # For some reason w2 and w3 are provided in the wrong order in the weights file\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w3.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w2.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"layers.{l}.ffn_norm.weight\"]\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"norm.weight\"])\n",
    "    model.out_head.weight = assign(model.out_head.weight, params[\"output.weight\"])\n",
    "\n",
    "\n",
    "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
