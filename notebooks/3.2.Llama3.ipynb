{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêé Notebook 3.2: Building LLaMA 3.2 (1B and 3B Parameters) ‚Äì A Reverse Journey!  \n",
    "\n",
    "### üåü What‚Äôs Inside?  \n",
    "\n",
    "1. **From Macro to Micro**:  \n",
    "   - We'll **reverse-engineer the entire model** by starting with the high-level LLaMA 3.2 architecture, gradually breaking it down to its core components.  \n",
    "   - This approach gives us a holistic understanding of large language models, focusing on **design choices and their impact on performance**.  \n",
    "\n",
    "2. **Core LLaMA 3.2 Features**:  \n",
    "   - **Grouped-Query Attention (GQA)** for efficient attention computation with key-value grouping.  \n",
    "   - **RMSNorm** as a lightweight and effective normalization method.  \n",
    "   - **Rotary Position Embeddings (RoPE)** with frequency scaling for efficient handling of extended context lengths.  \n",
    "\n",
    "3. **Defining and Implementing LLaMA 3.2 Architecture**:  \n",
    "   - We'll implement the **token embeddings**, **transformer stack**, and **feed-forward networks**, guided by the official configurations.  \n",
    "   - Incorporate **RoPE frequency scaling** and **key-value groups** for grouped-query attention, enhancing efficiency.  \n",
    "\n",
    "4. **Context Length and Scaling Innovations**:  \n",
    "   - LLaMA 3.2 pushes boundaries with a **context length of 131,072 tokens**, making it well-suited for long-form text tasks.  \n",
    "   - Explore how scaling context length and grouped attention influence performance and memory.  \n",
    "\n",
    "5. **Running Inference on LLaMA 3.2 Models**:  \n",
    "   - Test pre-trained weights for the 1B and 3B configurations, analyzing their performance on text generation tasks.  \n",
    "\n",
    "\n",
    "### üõ† **Architecture Recap (LLaMA 3.2 1B & 3B):**  \n",
    "\n",
    "#### LLaMA 3.2 (1B):  \n",
    "- **Vocabulary Size**: 128,256  \n",
    "- **Context Length**: 131,072 tokens  \n",
    "- **Embedding Dimension**: 2048  \n",
    "- **Number of Attention Heads**: 32  \n",
    "- **Number of Layers**: 16  \n",
    "- **Feed-Forward Dimension**: 8192  \n",
    "- **Grouped Key-Value Attention**: 8 groups  \n",
    "- **RoPE Base**: 500,000  \n",
    "- **Precision**: torch.bfloat16  \n",
    "\n",
    "#### LLaMA 3.2 (3B):  \n",
    "- **Vocabulary Size**: 128,256  \n",
    "- **Context Length**: 131,072 tokens  \n",
    "- **Embedding Dimension**: 3072  \n",
    "- **Number of Attention Heads**: 24  \n",
    "- **Number of Layers**: 28  \n",
    "- **Feed-Forward Dimension**: 8192  \n",
    "- **Grouped Key-Value Attention**: 8 groups  \n",
    "- **RoPE Base**: 500,000  \n",
    "- **Precision**: torch.bfloat16  \n",
    "\n",
    "### **Summary of Main Architectural Differences Between GPT and LLaMA**\n",
    "\n",
    "| **Aspect**               | **GPT**                                      | **LLaMA**                                   |\n",
    "|---------------------------|----------------------------------------------|---------------------------------------------|\n",
    "| **Activation Function**   | GELU (Gaussian Error Linear Unit)           | SiLU (Sigmoid Linear Unit)                 |\n",
    "| **Normalization**         | LayerNorm                                   | RMSNorm (Root Mean Square Normalization)   |\n",
    "| **Feedforward Network**   | Standard dense layers                       | Gated Linear Units (GLUs) for nonlinearity |\n",
    "| **Attention Mechanism**   | Standard scaled dot-product attention       | Includes Rotary Embeddings for efficiency  |\n",
    "| **Positional Encoding**   | Learned absolute positional embeddings      | Rotary positional embeddings               |\n",
    "\n",
    "### **Key Takeaways**\n",
    "- LLaMA's architectural tweaks, like RMSNorm and rotary embeddings, focus on efficiency while maintaining strong performance.\n",
    "- GPT relies on more traditional design choices, such as LayerNorm and absolute positional embeddings, which have proven effective but may not be as efficient.\n",
    "\n",
    "\n",
    "### üìö **Goals for this Notebook:**  \n",
    "\n",
    "1. **High-Level Architecture**:  \n",
    "   - Start with the **entire model structure**, understanding how different components interact.  \n",
    "   - Explore design principles like **GQA**, **RMSNorm**, and **RoPE scaling**.  \n",
    "\n",
    "2. **Micro-Level Implementation**:  \n",
    "   - Dive into each building block‚Äîtoken embeddings, transformer layers, feed-forward networks‚Äîand implement them following the LLaMA 3.2 specification.\n",
    "\n",
    "3. **Implementing a custom tokenizer by extending the `tiktoken` library**:  \n",
    "   - Extend the `tiktoken` library to support custom tokenization for LLaMA 3.2.  \n",
    "   - This allows for custom tokenization for special characters and characters not present in the original tokenizer.\n",
    "   \n",
    "\n",
    "4. **Efficienct weight loading**:  \n",
    "   - Load pre-trained weights for both configurations with memeory efficiency.\n",
    "\n",
    "\n",
    "\n",
    "### **Why a Reverse Journey?**  \n",
    "\n",
    "By building LLaMA 3.2 models from the top down, we aim to gain a new perspective on model design. This complements the bottom-up approach we used for GPT-2 and provides a unique opportunity to explore the scalability and efficiency of modern language models.  \n",
    "\n",
    "\n",
    "### **Next Steps:**  \n",
    "\n",
    "In this notebook, we embark on a journey to understand the design philosophy of **LLaMA 3.2** while implementing and testing its cutting-edge features. This will deepen our expertise in building and utilizing state-of-the-art large language models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **General Components in the `Llama3` Code**\n",
    "\n",
    "The `Llama3` class comprises several key components that define the architecture of the LLaMA 3 model. Here's an overview:\n",
    "\n",
    "1. **Token Embedding**:\n",
    "   - Converts token indices into dense vector representations using `nn.Embedding`.\n",
    "   - Represents the input sequence in a continuous embedding space.\n",
    "\n",
    "2. **Transformer Blocks**:\n",
    "   - A stack of transformer layers, implemented as `nn.Sequential`.\n",
    "   - Each block includes mechanisms like grouped-query attention, feed-forward networks, and normalization.\n",
    "\n",
    "3. **Final Normalization**:\n",
    "   - Uses `RMSNorm` (Root Mean Square Normalization) to stabilize outputs from the transformer stack.\n",
    "\n",
    "4. **Output Projection Layer**:\n",
    "   - A linear layer that maps the hidden states back to the vocabulary size for token prediction.\n",
    "   - Provides logits used to calculate probabilities over the vocabulary.\n",
    "\n",
    "5. **Forward Pass Logic**:\n",
    "   - Defines how input data flows through the model:\n",
    "     - Embedding ‚Üí Transformer Stack ‚Üí Normalization ‚Üí Output Projection.\n",
    "\n",
    "\n",
    "### üìö **Next Steps**\n",
    "\n",
    "We'll dive into each of these components, starting with the **Transformer Blocks**, to explore how they contribute to the overall model functionality!\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/llama3.png\" alt=\"My Image\" />\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Embedding layer to convert token indices into dense vector representations\n",
    "        self.token_emedding = nn.Embedding(\n",
    "            config['vocab_size'], config['emb_dim'], dtype=config['dtype']\n",
    "        )\n",
    "        # Stack of transformer blocks, forming the core of the model\n",
    "        self.tras_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config['n_layers'])]\n",
    "        )\n",
    "        # Final RMSNorm layer to normalize outputs of the transformer stack\n",
    "        self.final_norm = nn.RMSNorm(config['emb_dim'])\n",
    "        # Linear layer to project the hidden states back to the vocabulary size\n",
    "        self.output = nn.Linear(\n",
    "            config['emb_dim'], config['vocab_size'], bias=False, dtype=config['dtype']\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Token embedding: Convert input token indices to dense embeddings\n",
    "        tok_emb = self.token_emedding(x)\n",
    "        x = tok_emb\n",
    "        # Pass through the stack of transformer blocks\n",
    "        x = self.tras_blocks(tok_emb)\n",
    "        # Normalize the output of the transformer stack\n",
    "        x = self.final_norm(x)\n",
    "        # Project the normalized output to the vocabulary space and return logits\n",
    "        logits = self.output(x.to(torch.bfloat16))\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìö Understanding RMSNorm: Root Mean Square Normalization**\n",
    "\n",
    "RMSNorm, or Root Mean Square Normalization, is a normalization technique introduced as a simplified alternative to Layer Normalization (LayerNorm). Unlike LayerNorm, which normalizes activations using their mean and variance, RMSNorm focuses solely on the root mean square (RMS) of the inputs.\n",
    "\n",
    "\n",
    "### **üîç Key Differences Between RMSNorm and LayerNorm**\n",
    "| **Aspect**                | **LayerNorm**                                      | **RMSNorm**                                   |\n",
    "|---------------------------|---------------------------------------------------|-----------------------------------------------|\n",
    "| **Normalization**         | Centers data by subtracting the mean and scaling by variance. | Scales inputs based on their RMS only, without centering. |\n",
    "| **Parameters**            | Learns both scale (gamma) and shift (beta).       | Learns only scale (gamma).                   |\n",
    "| **Efficiency**            | Slightly more computational overhead due to mean computation. | Computationally simpler and faster.          |\n",
    "| **Centering of Inputs**   | Always centers inputs (zero mean).                | Does not center inputs (focuses only on magnitude). |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **‚ú® How RMSNorm Works**\n",
    "\n",
    "Given an input vector \\( x \\in \\mathbb{R}^d \\):\n",
    "\n",
    "1. **Compute the RMS of the input**:  \n",
    "   \\[\n",
    "   \\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^d x_i^2}\n",
    "   \\]\n",
    "   where \\( d \\) is the dimensionality of the input \\( x \\).\n",
    "\n",
    "2. **Normalize the input by dividing each element by the RMS**:  \n",
    "   \\[\n",
    "   \\hat{x}_i = \\frac{x_i}{\\text{RMS}(x)}\n",
    "   \\]\n",
    "\n",
    "3. **Scale the normalized input using a learned scale parameter \\( \\gamma \\)**:  \n",
    "   \\[\n",
    "   y_i = \\gamma \\cdot \\hat{x}_i\n",
    "   \\]\n",
    "\n",
    "\n",
    "\n",
    "Note: Unlike LayerNorm, RMSNorm omits the learned shift parameter \\( \\beta \\), simplifying its implementation and making it more efficient.\n",
    "\n",
    "\n",
    "### **üöÄ Advantages of RMSNorm**\n",
    "1. **Computational Efficiency**: Since it does not compute the mean, RMSNorm is faster and requires fewer operations than LayerNorm.\n",
    "2. **Stability in Training**: It provides sufficient normalization for tasks like natural language processing (NLP) without needing centering.\n",
    "3. **Alignment with LLaMA**: RMSNorm aligns well with the architectural design of LLaMA models, improving convergence in larger-scale settings.\n",
    "\n",
    "\n",
    "### **üîß Implementation Plan**\n",
    "1. **Implement RMSNorm from Scratch**:\n",
    "   - Calculate RMS manually.\n",
    "   - Normalize and scale the input.\n",
    "   - Validate the implementation with small inputs.\n",
    "\n",
    "2. **Compare with PyTorch's Built-in RMSNorm**:\n",
    "   - Use `torch.nn.RMSNorm` for comparison.\n",
    "   - Match results for identical inputs.\n",
    "\n",
    "3. **Analyze Performance**:\n",
    "   - Measure speed and memory efficiency between custom and PyTorch implementations.\n",
    "\n",
    "\n",
    "\n",
    "Let's start with the implementation of RMSNorm from scratch! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # Store the configuration and epsilon value for numerical stability\n",
    "        self.config = config\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Initialize the scale parameter (gamma), which will be learned during training\n",
    "        # It has the same shape as the embedding dimension (config['emb_dim'])\n",
    "        self.scale = nn.Parameter(torch.ones(config['emb_dim']))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the mean squared value of the input tensor across the last dimension\n",
    "        # This is used to calculate the RMS (Root Mean Square) of each element in the input\n",
    "        mean = x.pow(2).mean(dim=-1, keepdim=True)  # mean(x^2) along the last dimension\n",
    "        \n",
    "        # Normalize the input by dividing each element by the RMS of that element\n",
    "        # torch.rsqrt computes the inverse square root (1/sqrt(x))\n",
    "        x_norm = x * torch.rsqrt(mean + self.eps)  # Normalize using RMS\n",
    "        \n",
    "        # Apply the learnable scale parameter (gamma) to the normalized tensor\n",
    "        # This scale parameter allows the model to learn the optimal scaling factor\n",
    "        output = (self.scale * x_norm).to(dtype=x.dtype)  # Ensure the output has the same dtype as the input\n",
    "        \n",
    "        # Return the scaled and normalized tensor\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **‚ú® Comparison with PyTorch's RMSNorm Implementation**\n",
    "\n",
    "Now, let's compare our custom `RMSNorm` implementation with PyTorch's built-in `RMSNorm` (if available). We will check both outputs for a given input tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the outputs equal? True\n"
     ]
    }
   ],
   "source": [
    "# Define a sample configuration\n",
    "config = {'emb_dim': 2048}\n",
    "\n",
    "# Create a random input tensor\n",
    "x = torch.randn(16, 32, config['emb_dim'])\n",
    "\n",
    "# Create an instance of our custom RMSNorm\n",
    "custom_rmsnorm = RMSNorm(config)\n",
    "custom_output = custom_rmsnorm(x)\n",
    "\n",
    "# Try PyTorch's RMSNorm implementation (if available)\n",
    "try:\n",
    "    from torch.nn import RMSNorm as TorchRMSNorm\n",
    "    \n",
    "    # Create an instance of PyTorch's RMSNorm\n",
    "    torch_rmsnorm = TorchRMSNorm(config['emb_dim'], eps=1e-5)\n",
    "    torch_output = torch_rmsnorm(x)\n",
    "    \n",
    "    # Compare the outputs (check if they are the same)\n",
    "    outputs_are_equal = torch.allclose(custom_output, torch_output)\n",
    "    print(f\"Are the outputs equal? {outputs_are_equal}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch RMSNorm is not available. Custom RMSNorm implementation is being used.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer Block Overview**\n",
    "\n",
    "The **Transformer Block** in the Llama model consists of the following key components:\n",
    "\n",
    "1. **Masked Group-Query Attention with RoPE Scaling**:\n",
    "   - **Masked Attention** ensures each token only attends to itself and previous tokens.\n",
    "   - **Group-Query Attention** divides the query tensor into groups for more efficient computation.\n",
    "   - **RoPE Scaling** encodes positional information of tokens to capture relative positions efficiently.\n",
    "\n",
    "2. **Feedforward Network with SiLU Activation**:\n",
    "   - A two-layer feedforward network with a **SiLU (Sigmoid Linear Unit)** activation function, adding non-linearity.\n",
    "\n",
    "3. **RMSNorm**:\n",
    "   - Applied twice: once after the attention layer and once after the feedforward layer, ensuring stable training and better performance.\n",
    "\n",
    "This block processes the input data through attention, a feedforward network, and normalization, outputting the transformed sequence for further layers or the final output. We‚Äôll implement and dive into each component separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Initialize the multi-head attention layer\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=config[\"emb_dim\"],\n",
    "            d_out=config[\"emb_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            num_kv_groups=config[\"n_kv_groups\"],\n",
    "            rope_base=config[\"rope_base\"],\n",
    "            rope_config=config[\"rope_freq\"],\n",
    "            dtype=config[\"dtype\"]\n",
    "        )\n",
    "        # Initialize the feedforward layer\n",
    "        self.ff = FeedForward(config)  # Assuming FeedForward is defined elsewhere\n",
    "        # Initialize layer normalization layers\n",
    "        self.norm1 = nn.RMSNorm(config['emb_dim'])  # First normalization layer\n",
    "        self.norm2 = nn.RMSNorm(config['emb_dim'])  # Second normalization layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the multi-head attention layer\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16))\n",
    "        x = x + shortcut  # Residual connection\n",
    "        \n",
    "        # Pass the output through the feedforward layer\n",
    "        shortcut = x  # Update shortcut after attention\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + shortcut  # Residual connection\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- **FeedForward Layer with SiLU Activation**\n",
    "\n",
    "The **FeedForward layer** in a transformer block consists of two main components:\n",
    "\n",
    "1. **Linear Transformation**: A fully connected layer that applies a linear transformation to the input tensor.\n",
    "2. **Non-linear Activation**: After the linear transformation, a non-linear activation function is applied. In this case, we use **SiLU** (Sigmoid Linear Unit), which is a smooth and differentiable activation function that has shown strong performance in many deep learning models.\n",
    "\n",
    "#### **SiLU Activation**:\n",
    "SiLU, also known as the **Swish activation** function, is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{SiLU}(x) = x \\cdot \\sigma(x)\n",
    "\\]\n",
    "\n",
    "where \\( \\sigma(x) \\) is the sigmoid function:\n",
    "\n",
    "\\[\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\]\n",
    "\n",
    "This activation is differentiable and allows for smooth gradients, making it particularly well-suited for deep networks.\n",
    "\n",
    "#### **FeedForward Structure**:\n",
    "The FeedForward layer typically follows this structure:\n",
    "\n",
    "1. **First Linear Transformation**: A linear layer with a larger output dimension (often referred to as the \"hidden dimension\").\n",
    "2. **SiLU Activation**: The SiLU activation is applied to the output of the first linear layer.\n",
    "3. **Second Linear Transformation**: A linear layer that maps the output back to the original dimension.\n",
    "4. **Dropout** (optional but often used): To prevent overfitting, dropout can be applied between the layers.\n",
    "\n",
    "This layer is often followed by layer normalization and a residual connection to stabilize training and enable deeper architectures.\n",
    "\n",
    "### **FeedForward Layer in Code**:\n",
    "\n",
    "We'll implement this FeedForward layer with the SiLU activation. Let's go ahead and implement the `FeedForward` class using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Initialize the first linear layer: Input -> Hidden layer\n",
    "        self.fc1 = nn.Linear(config['emb_dim'], config['hidden_dim'], bias=False)\n",
    "        \n",
    "        # Initialize the second linear layer: Input -> Hidden layer (to be used in multiplication later)\n",
    "        self.fc2 = nn.Linear(config['emb_dim'], config['hidden_dim'], bias=False)\n",
    "        \n",
    "        # Initialize the third linear layer: Hidden layer -> Output (back to original dimension)\n",
    "        self.fc3 = nn.Linear(config['hidden_dim'], config['emb_dim'], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the first linear layer\n",
    "        x1 = self.fc1(x)\n",
    "        \n",
    "        # Pass the input through the second linear layer\n",
    "        x2 = self.fc2(x)\n",
    "        \n",
    "        # Apply SiLU activation (Sigmoid Linear Unit) to the output of the first linear layer\n",
    "        # Multiply the result with the output of the second linear layer\n",
    "        # This operation introduces element-wise multiplication between activations and transformed values\n",
    "        x = F.silu(x1) * x2 \n",
    "        \n",
    "        # Pass the result through the third linear layer to return to the original dimension\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of Activation Functions: SiLU, ReLU, and GELU**\n",
    "\n",
    "In this section, we'll compare three popular activation functions‚Äî**SiLU**, **ReLU**, and **GELU**‚Äîby plotting their behavior and understanding their differences.\n",
    "\n",
    "---\n",
    "\n",
    "### **SiLU Activation**\n",
    "\n",
    "SiLU, or **Sigmoid Linear Unit**, is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{SiLU}(x) = x \\cdot \\sigma(x)\n",
    "\\]\n",
    "\n",
    "where \\( \\sigma(x) \\) is the sigmoid function:\n",
    "\n",
    "\\[\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\]\n",
    "\n",
    "The SiLU function smoothly combines the input \\( x \\) with its sigmoid value, providing non-linearity while avoiding the sharp transition present in ReLU. This characteristic helps reduce issues such as vanishing gradients during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **ReLU Activation**\n",
    "\n",
    "ReLU (Rectified Linear Unit) is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\]\n",
    "\n",
    "It outputs the input directly for positive values, and zero for negative values. Although it is computationally efficient, ReLU can suffer from **dead neurons** when a large portion of the input is negative, which results in no gradient flow for these neurons.\n",
    "\n",
    "---\n",
    "\n",
    "### **GELU Activation**\n",
    "\n",
    "GELU (Gaussian Error Linear Unit) is a probabilistic activation function, commonly used in Transformer-based models:\n",
    "\n",
    "\\[\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "\\]\n",
    "\n",
    "where \\( \\Phi(x) \\) is the cumulative distribution function (CDF) of the standard normal distribution. GELU introduces a smoother non-linearity than ReLU and has become popular in modern deep learning architectures, particularly for its robustness and smoother gradient behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Plotting the Activation Functions**\n",
    "\n",
    "Now, let‚Äôs plot all three activation functions‚ÄîSiLU, ReLU, and GELU‚Äîover a range of inputs from -5 to 5.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create input tensor\n",
    "x = torch.linspace(-5, 5, steps=100)\n",
    "\n",
    "# SiLU implementation from scratch\n",
    "def silu(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "# Calculate activations\n",
    "silu_output = silu(x)\n",
    "relu_output = F.relu(x)\n",
    "gelu_output = F.gelu(x)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x.numpy(), silu_output.numpy(), label='SiLU', color='b')\n",
    "plt.plot(x.numpy(), relu_output.numpy(), label='ReLU', color='g')\n",
    "plt.plot(x.numpy(), gelu_output.numpy(), label='GELU', color='m')\n",
    "plt.title('Comparison of Activation Functions: SiLU, ReLU, GELU')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results and Observations**\n",
    "\n",
    "- **SiLU**: The output of SiLU is smooth and continuous. It does not cut off negative values entirely, instead producing a small non-zero output for negative inputs. This smoothness allows SiLU to provide a more gradual non-linearity compared to ReLU.\n",
    "  \n",
    "- **ReLU**: ReLU has a sharp transition at zero, mapping all negative inputs to zero. This can be beneficial for reducing computation time and handling sparse activations, but it can also lead to dead neurons, where neurons always output zero, stopping them from learning.\n",
    "\n",
    "- **GELU**: GELU is also smooth but differs from SiLU in that it approximates a normal distribution. The curve for GELU is slightly smoother and more probabilistic, offering small non-zero outputs for negative inputs, making it well-suited for complex deep learning architectures like Transformers.\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **SiLU** and **GELU** both offer smoother, more continuous activation curves compared to ReLU.\n",
    "- **ReLU** is more computationally efficient but can suffer from dead neurons and less smooth gradients.\n",
    "- **SiLU** provides a good balance between smoothness and computational efficiency.\n",
    "- **GELU** is more sophisticated and can be more effective in certain deep models, such as Transformers, due to its probabilistic nature.\n",
    "\n",
    "By comparing these activation functions, you can choose the most suitable one for your neural network model, depending on your specific requirements (e.g., computational efficiency vs. gradient smoothness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-Rotary Positional Embedding (RoPE) in Transformers**\n",
    "\n",
    "Rotary Positional Embedding (RoPE) is a method for encoding the position of tokens in a sequence, designed to improve the effectiveness of positional encodings in transformer models. It is used to capture the relative position information between tokens, which is crucial for handling sequential data. RoPE is particularly useful in transformer architectures and has been shown to enhance the performance of models in tasks such as language modeling and machine translation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why RoPE?**\n",
    "\n",
    "In traditional transformer models, positional encodings are added to the input embeddings to provide the model with information about the order of tokens in a sequence. The most common approach, **sinusoidal positional encoding**, uses fixed sinusoidal functions based on the position of the tokens.\n",
    "\n",
    "However, RoPE introduces a more flexible and efficient method for encoding the relative position between tokens by rotating the embedding vectors. This approach offers the following benefits:\n",
    "\n",
    "- **Scalability**: RoPE allows for better handling of long sequences, as it improves the ability of the transformer to generalize to longer sequences than those seen during training.\n",
    "- **Flexibility**: It provides more flexibility in how position information is encoded, improving model performance on tasks where relative positioning is important.\n",
    "- **Attention Masking**: RoPE works naturally with causal masking in autoregressive tasks, making it a good fit for language models.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/rope.png\" alt=\"My Image\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding RoPE (Rotary Positional Embedding)**\n",
    "\n",
    "To fully grasp how **Rotary Positional Embedding (RoPE)** works and why it has become a key method in transformer-based models, it's essential to understand several foundational concepts:\n",
    "\n",
    "### 1. **Relative vs Absolute Positioning**\n",
    "   - **Absolute Positioning**: In traditional transformers, each token in the sequence is assigned a unique positional encoding that is added to the token's embedding. This method, called **absolute positional encoding**, directly encodes the position of a token in the sequence (e.g., the first token gets position 0, the second token gets position 1, and so on). The position of each token is independent of others.\n",
    "   \n",
    "   - **Relative Positioning**: In contrast, **relative positional encoding** focuses on the relative distance between tokens. The idea is that the meaning of a token depends not just on its position in the sequence, but on its relationship with other tokens. RoPE, in particular, uses **rotations** to capture this relative positional information, making it more adaptable and flexible for tasks involving variable-length sequences.\n",
    "\n",
    "### 2. **Dot Product vs Inner Product**\n",
    "   - Both **dot product** and **inner product** refer to mathematical operations that involve two vectors. However, they are often used interchangeably in the context of linear algebra and machine learning. \n",
    "     - The **dot product** of two vectors \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\) is computed as:\n",
    "\n",
    "       \\[\n",
    "       \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i\n",
    "       \\]\n",
    "       \n",
    "       It results in a scalar value that can measure the similarity between two vectors.\n",
    "     \n",
    "     - The **inner product** is a more generalized form of the dot product, where the two vectors can be elements of a more general vector space (e.g., function spaces). In Euclidean space, the dot product is a special case of the inner product. The inner product measures how much one vector projects onto another.\n",
    "\n",
    "   - RoPE makes use of the dot product (specifically, the query and key vectors in the attention mechanism) but applies rotations to these vectors to encode the relative positions between tokens. The use of rotations preserves the vector structure while enabling the transformer model to understand positional relationships.\n",
    "\n",
    "### 3. **Rotation Matrix**\n",
    "   - A **rotation matrix** is a special orthogonal matrix used to perform a rotation of vectors in Euclidean space. In 2D, a rotation matrix for an angle \\( \\theta \\) is defined as:\n",
    "\n",
    "     \\[\n",
    "    \n",
    "     \\begin{bmatrix}\n",
    "     \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "     \\sin(\\theta) & \\cos(\\theta)\n",
    "     \\end{bmatrix}\n",
    "     \\]\n",
    "\n",
    "   - In higher dimensions, rotation matrices generalize this idea. RoPE applies a rotation matrix to each position embedding, where the rotation angle is dependent on the position of the token in the sequence. This allows the model to capture **relative positional information** between tokens in a computationally efficient way.\n",
    "   \n",
    "   - Instead of adding fixed positional encodings, RoPE rotates the query and key vectors in the attention mechanism by different amounts depending on the token's position. This method provides a flexible way to encode positional relationships without requiring large and static matrices.\n",
    "\n",
    "   *Note*: The general form of the equation is shown as follows:\n",
    "\n",
    "   <p align=\"center\">\n",
    "       <img src=\"images/gn_rope.png\" alt=\"Rotation Matrix Equation\" />\n",
    "   </p>\n",
    "\n",
    "   If you notice, the matrix includes a large number of zero values (many values turn to zero), which is a waste of computational resources. A better solution is to implement a more efficient way, as shown below:\n",
    "\n",
    "### 4. **RoPE as the Computationally Efficient Form**\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/eff_rope.png\" alt=\"Efficient RoPE\" />\n",
    "</p>\n",
    "\n",
    "RoPE offers a flexible, efficient, and scalable way to encode positional information in transformer models. Understanding key concepts such as **relative vs absolute positioning**, **dot product vs inner product**, and the **rotation matrix** are crucial for appreciating how RoPE works. Its computational efficiency stems from the fact that it encodes relative position information via rotations, avoiding the need for large positional embedding matrices and making it particularly effective for handling long sequences in NLP tasks.\n",
    "\n",
    "## But How to Do This Math Stuff in PyTorch?\n",
    "\n",
    "Remember, PyTorch is not just a deep learning framework, but also a powerful **computational library** that comes with a wide range of functionalities. These built-in operations can significantly simplify the math involved in tasks like **Rotary Positional Encoding (RoPE)**, making it easier to implement the complex mathematical concepts needed for transformers and attention mechanisms.\n",
    "\n",
    "Let's explore how to implement key mathematical concepts in PyTorch for RoPE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.5403+8.4147e-01j,  0.7318+6.8156e-01j,  0.8460+5.3317e-01j,\n",
      "          ...,  1.0000+2.3714e-04j,  1.0000+1.7783e-04j,\n",
      "          1.0000+1.3335e-04j],\n",
      "        [-0.4161+9.0930e-01j,  0.0709+9.9748e-01j,  0.4315+9.0213e-01j,\n",
      "          ...,  1.0000+4.7427e-04j,  1.0000+3.5566e-04j,\n",
      "          1.0000+2.6670e-04j],\n",
      "        ...,\n",
      "        [-0.9998+1.7612e-02j,  0.6164-7.8744e-01j, -0.7242+6.8960e-01j,\n",
      "          ...,  0.9708+2.3976e-01j,  0.9836+1.8057e-01j,\n",
      "          0.9907+1.3573e-01j],\n",
      "        [-0.5550-8.3182e-01j,  0.9877-1.5612e-01j, -0.9803+1.9732e-01j,\n",
      "          ...,  0.9708+2.3999e-01j,  0.9835+1.8074e-01j,\n",
      "          0.9907+1.3586e-01j],\n",
      "        [ 0.4001-9.1649e-01j,  0.8292+5.5900e-01j, -0.9346-3.5578e-01j,\n",
      "          ...,  0.9707+2.4022e-01j,  0.9835+1.8092e-01j,\n",
      "          0.9907+1.3600e-01j]])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the sequence length and head dimension\n",
    "# The sequence length represents the number of tokens (positions) in the sequence\n",
    "# The head dimension represents the number of embedding dimensions for each token\n",
    "sequence_length = 1024  # Length of the sequence (number of tokens)\n",
    "head_dim = 64           # Dimension of each token's embedding vector\n",
    "theta_base = 10000.0    # Base value used to calculate positional frequencies\n",
    "\n",
    "# Step 2: Create the position tensor `m` representing the position of each token in the sequence\n",
    "# `m` will have values from 0 to sequence_length - 1, representing the position of each token\n",
    "# Mathematically, `m` is a vector {0, 1, 2, ..., sequence_length-1}\n",
    "m = torch.arange(sequence_length).float()  # Positions in the sequence\n",
    "\n",
    "# Step 3: Calculate the theta values using the formula from the Reformer paper\n",
    "# We are calculating theta_i = 10000^(-2(i-1)/dim) for i = 1, 2, ..., head_dim/2\n",
    "# The `theta_numerator` represents the indices i for every alternate value (i.e., 0, 2, 4, ..., head_dim-2)\n",
    "theta_numerator = torch.arange(0, head_dim, 2).float()  # Indices for every alternate value\n",
    "theta = 1.0 / (theta_base ** (theta_numerator / head_dim))  # Compute the theta values for each embedding dimension\n",
    "\n",
    "# Step 4: Compute the outer product between `m` (positions) and `theta` (frequencies)\n",
    "# The outer product between `m` and `theta` gives us a matrix where each element is the product of position and frequency\n",
    "# Mathematically, freqs(i, j) = m_i * theta_j for each position i and dimension j\n",
    "freqs = torch.outer(m, theta).float()  # Outer product between m and theta, resulting in positional frequencies\n",
    "\n",
    "# Step 5: Convert the positional frequencies to complex numbers in polar form\n",
    "# In polar form, a complex number is represented as R * exp(i * theta), where R = 1 (magnitude)\n",
    "# We use `torch.polar` to create complex numbers with a magnitude of 1 and the computed frequencies as angles\n",
    "freqs_complex = torch.polar(torch.ones_like(freqs), freqs)  # Convert frequencies to complex numbers (polar form)\n",
    "\n",
    "# Now, `freqs_complex` contains the complex numbers representing the rotational encodings for each position\n",
    "print(freqs_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A COMPACT IMPLEMENTATION OF THE SAME CELL ABOVE \n",
    "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
    "    # As written in the paragraph 3.2.2 of the paper\n",
    "    # >> In order to generalize our results in 2D to any xi ‚àà Rd where **d is even**, [...]\n",
    "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
    "    # Build the theta parameter\n",
    "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ... dim/2]\n",
    "    # Shape: (Head_Dim / 2)\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    # Shape: (Head_Dim / 2)\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device) # (Dim / 2)\n",
    "    # Construct the positions (the \"m\" parameter)\n",
    "    # Shape: (Seq_Len)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    # Multiply each theta by each position using the outer product.\n",
    "    # Shape: (Seq_Len) outer_product* (Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    # We can compute complex numbers in the polar form c = R * exp(m * theta), where R = 1 as follows:\n",
    "    # (Seq_Len, Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Rotary Positional Encoding to Tokens\n",
    "\n",
    "Now that we have computed the positional frequencies (`freqs_complex`), we need to apply these to the input tokens by following these steps:\n",
    "\n",
    "#### Step 1: Group Every Two Tokens Side by Side\n",
    "\n",
    "In this step, we take the original vector of tokens and reshape it such that every two consecutive tokens are grouped together side by side. For example:\n",
    "\n",
    "- **Original:**\n",
    "  ```\n",
    "  x1\n",
    "  x2\n",
    "  x3\n",
    "  x4\n",
    "  ...\n",
    "  ```\n",
    "\n",
    "- **After Grouping:**\n",
    "  ```\n",
    "  x1  x2\n",
    "  x3  x4\n",
    "  ...\n",
    "  ```\n",
    "\n",
    "This operation is performed using tensor reshaping, where we group the tokens in the last dimension of the tensor:\n",
    "\n",
    "```python\n",
    "x = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "```\n",
    "\n",
    "#### Step 2: Convert `x` to Complex Numbers\n",
    "\n",
    "After reshaping, we convert `x` into complex numbers. This allows us to perform operations with both real and imaginary components. The `torch.view_as_complex` function transforms the reshaped tensor into a complex tensor:\n",
    "\n",
    "```python\n",
    "x_complex = torch.view_as_complex(x)\n",
    "```\n",
    "\n",
    "#### Step 3: Match the Shape of `freqs_complex` with `x_complex`\n",
    "\n",
    "Next, we need to adjust the shape of `freqs_complex` to match the shape of the manipulated input `x_complex`. Since `x_complex` has an additional dimension (representing the pairs of tokens), we need to add extra dimensions to `freqs_complex` so they align for multiplication.\n",
    "\n",
    "```python\n",
    "freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "```\n",
    "\n",
    "This operation adds the necessary dimensions, ensuring that both `x_complex` and `freqs_complex` can be multiplied together element-wise in the next step.\n",
    "\n",
    "#### Step 4: Multiply `x_complex` and `freqs_complex`\n",
    "\n",
    "Now, we perform element-wise multiplication between `x_complex` and `freqs_complex` to apply the rotary positional encoding. This rotation operation is performed for each pair of tokens in the input:\n",
    "\n",
    "```python\n",
    "x_rotated = x_complex * freqs_complex\n",
    "```\n",
    "\n",
    "#### Step 5: Convert Back to Real Numbers and Reshape\n",
    "\n",
    "Finally, we convert the result back to real numbers using `torch.view_as_real`, which separates the real and imaginary parts of the complex numbers. We then reshape the output tensor to match the input shape:\n",
    "\n",
    "```python\n",
    "x_out = torch.view_as_real(x_rotated)\n",
    "x_out = x_out.reshape(*x.shape)\n",
    "```\n",
    "\n",
    "We return the output tensor, ensuring it has the same type and device as the input `x`:\n",
    "\n",
    "```python\n",
    "return x_out.type_as(x).to(device)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Step 1**: Reshape `x` to group every two tokens side by side.\n",
    "- **Step 2**: Convert the reshaped tokens to complex numbers.\n",
    "- **Step 3**: Match the shape of `freqs_complex` to the manipulated `x`.\n",
    "- **Step 4**: Perform element-wise multiplication between `x_complex` and `freqs_complex` to apply the rotary encoding.\n",
    "- **Step 5**: Convert back to real numbers and reshape the output tensor to match the input dimensions.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "    # (B, Seq_Len, H, Head_Dim) -> (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. So we need to add the batch dimension and the head dimension\n",
    "    # (Seq_Len, Head_Dim/2) --> (1, Seq_Len, 1, Head_Dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
    "    # (B, Seq_Len, H, Head_Dim/2) * (1, Seq_Len, 1, Head_Dim/2) = (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # Convert the complex number back to the real number\n",
    "    # (B, Seq_Len, H, Head_Dim/2) -> (B, Seq_Len, H, Head_Dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # (B, Seq_Len, H, Head_Dim/2, 2) -> (B, Seq_Len, H, Head_Dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test it : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 10, 8, 16])\n",
      "Rotated shape: torch.Size([4, 10, 8, 16])\n",
      "tensor([[[[ 1.1577e+00,  1.3184e+00,  5.0834e-01,  ..., -9.7599e-01,\n",
      "            1.1586e+00,  1.9374e+00],\n",
      "          [-1.0720e+00, -5.4686e-01, -1.2111e-01,  ..., -5.0799e-01,\n",
      "            1.3958e-01, -2.0146e+00],\n",
      "          [-1.4590e+00, -1.6140e+00, -1.9291e-01,  ...,  1.5239e+00,\n",
      "           -1.2854e+00, -1.2796e+00],\n",
      "          ...,\n",
      "          [ 1.2009e+00,  3.6370e-02, -2.0441e+00,  ..., -1.4779e-01,\n",
      "            5.8014e-01,  6.5439e-03],\n",
      "          [-3.2665e-02,  5.5108e-01,  2.9841e-01,  ...,  3.1203e-01,\n",
      "           -2.4552e-01, -2.8379e+00],\n",
      "          [ 2.2076e+00, -9.4494e-01,  6.8589e-01,  ...,  3.5363e-01,\n",
      "            2.5038e+00, -1.2166e+00]],\n",
      "\n",
      "         [[ 2.0133e-01, -2.6638e-01, -3.7434e-01,  ..., -9.0972e-01,\n",
      "           -2.3336e+00, -2.0996e+00],\n",
      "          [-9.3490e-01, -1.6563e-01,  3.5745e-01,  ..., -5.0746e-01,\n",
      "            6.0015e-01, -8.6968e-01],\n",
      "          [ 1.6836e+00,  2.7123e-01, -8.8838e-02,  ...,  1.3011e+00,\n",
      "           -7.3459e-01,  1.3688e+00],\n",
      "          ...,\n",
      "          [-6.2003e-01, -7.7899e-01, -3.3440e-01,  ..., -6.8017e-01,\n",
      "           -1.6843e+00,  7.2314e-01],\n",
      "          [ 3.5814e-01,  8.7403e-01, -7.1773e-01,  ...,  1.0769e+00,\n",
      "            1.1600e+00,  1.3779e+00],\n",
      "          [ 4.8276e-01,  5.0956e-01, -1.3096e+00,  ..., -1.9006e+00,\n",
      "            2.0228e-01,  1.4638e+00]],\n",
      "\n",
      "         [[ 1.2125e+00,  1.2964e+00, -1.2550e+00,  ..., -1.0778e+00,\n",
      "           -1.1988e-02,  6.5047e-01],\n",
      "          [-5.0218e-01,  2.1983e-01,  1.3679e-01,  ...,  3.0288e-01,\n",
      "            2.7740e-01, -7.0723e-01],\n",
      "          [-3.1112e-01,  2.4431e-01,  9.3431e-01,  ..., -2.2314e+00,\n",
      "            7.2500e-01, -5.6610e-01],\n",
      "          ...,\n",
      "          [-1.8580e+00, -4.7117e-01, -2.0831e+00,  ...,  2.1106e+00,\n",
      "            9.3178e-01, -5.0389e-01],\n",
      "          [ 2.5601e-01, -6.1161e-01, -4.7227e-01,  ...,  1.5719e+00,\n",
      "           -1.7288e+00, -1.1670e+00],\n",
      "          [-9.0935e-02,  1.0361e+00, -4.8295e-01,  ..., -7.1230e-01,\n",
      "           -1.2328e+00, -2.1438e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6506e+00, -3.5331e-01, -9.1186e-01,  ..., -1.0846e-01,\n",
      "            4.2007e-01,  1.5919e+00],\n",
      "          [-1.0531e+00,  8.5850e-01,  9.0403e-01,  ..., -1.2925e-01,\n",
      "            1.3460e+00, -5.4085e-01],\n",
      "          [-5.3561e-01, -7.5523e-02, -1.0011e+00,  ..., -8.5350e-01,\n",
      "           -1.9868e+00, -1.7448e-01],\n",
      "          ...,\n",
      "          [-3.8143e-01, -4.6778e-01, -1.4459e+00,  ...,  6.7334e-01,\n",
      "            2.9690e-02,  3.0684e-01],\n",
      "          [-1.0921e+00,  2.2905e-01, -7.0767e-01,  ...,  1.6553e+00,\n",
      "           -8.4325e-01, -1.6299e+00],\n",
      "          [-6.6378e-01, -8.1490e-01,  2.2369e-01,  ...,  2.7355e-01,\n",
      "           -2.8367e-01,  5.4006e-01]],\n",
      "\n",
      "         [[-3.1211e-01, -5.5642e-01,  3.8562e-01,  ...,  2.8868e-01,\n",
      "            1.0015e+00, -2.1472e-01],\n",
      "          [ 1.6076e+00,  2.2173e+00,  7.4231e-01,  ..., -7.6171e-01,\n",
      "            9.0559e-01,  1.5307e-01],\n",
      "          [-6.7452e-01,  1.2359e+00, -6.9582e-01,  ..., -9.7776e-01,\n",
      "           -4.7648e-01,  9.9882e-01],\n",
      "          ...,\n",
      "          [ 8.6412e-02,  1.6175e-01,  1.5083e+00,  ..., -4.9535e-01,\n",
      "            5.0004e-01, -1.6896e+00],\n",
      "          [ 6.4008e-01, -3.1303e-01, -1.3726e+00,  ..., -5.1902e-03,\n",
      "            1.5555e+00, -1.2505e+00],\n",
      "          [ 1.1013e+00, -9.0277e-01, -4.8934e-01,  ...,  8.8622e-01,\n",
      "            3.7165e-01, -5.6120e-01]],\n",
      "\n",
      "         [[ 2.4939e+00,  8.4717e-01,  1.2082e+00,  ..., -6.8145e-01,\n",
      "            1.7302e+00,  1.1262e+00],\n",
      "          [-1.3773e-01,  1.3571e+00,  2.9870e-01,  ..., -1.4226e+00,\n",
      "            1.8653e+00, -1.2074e+00],\n",
      "          [ 1.1539e+00,  2.2176e-01, -3.3827e-02,  ..., -1.6348e-01,\n",
      "            4.4222e-02, -8.0980e-01],\n",
      "          ...,\n",
      "          [-1.2459e+00,  1.7983e+00, -1.8169e+00,  ...,  1.0324e+00,\n",
      "            3.7159e-01,  4.2572e-01],\n",
      "          [-5.4709e-01, -5.1936e-01,  6.6746e-01,  ..., -3.2245e-01,\n",
      "            1.3367e-01, -2.4407e+00],\n",
      "          [-6.2450e-02,  2.2437e+00, -1.3482e+00,  ..., -6.7623e-01,\n",
      "           -7.8541e-02,  1.4441e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2611e-01,  2.7664e-02,  1.1142e+00,  ...,  1.3303e+00,\n",
      "           -1.9811e+00, -2.0114e-01],\n",
      "          [ 1.0370e-01, -1.9710e-01, -1.9759e-01,  ...,  9.0983e-01,\n",
      "            4.0567e-01, -1.9478e+00],\n",
      "          [ 7.8241e-01, -1.3852e+00,  2.2403e-02,  ..., -2.3607e-01,\n",
      "           -3.8759e-01,  3.9448e-01],\n",
      "          ...,\n",
      "          [-1.9848e-01, -1.0335e+00,  1.3387e+00,  ...,  1.4055e+00,\n",
      "            2.4051e-02,  6.3718e-01],\n",
      "          [ 1.2510e+00,  1.0451e-01, -1.0594e+00,  ...,  1.0037e-01,\n",
      "           -6.1716e-01,  1.7953e+00],\n",
      "          [-6.7964e-01, -7.6587e-01,  2.0806e+00,  ...,  1.3158e-01,\n",
      "            2.2736e-01,  1.1149e+00]],\n",
      "\n",
      "         [[-6.7101e-01, -1.7518e+00, -1.4142e+00,  ...,  2.5334e-01,\n",
      "           -8.2275e-01, -2.9323e-01],\n",
      "          [-3.2432e-01, -1.2184e-01, -6.5721e-02,  ..., -1.0735e+00,\n",
      "            3.5720e-01,  1.3808e+00],\n",
      "          [-2.2722e-01, -6.0174e-01, -9.8091e-01,  ..., -1.2430e-01,\n",
      "           -1.2778e+00,  4.8783e-01],\n",
      "          ...,\n",
      "          [ 1.4004e+00, -7.8966e-01,  5.3759e-01,  ..., -7.0756e-01,\n",
      "           -6.2722e-01, -1.6651e+00],\n",
      "          [ 4.6651e-01, -4.3591e-01, -1.1671e+00,  ..., -1.8667e-01,\n",
      "           -6.5683e-01, -6.6862e-01],\n",
      "          [ 1.2358e+00, -7.2081e-01, -3.9808e-01,  ..., -1.0571e+00,\n",
      "            8.4029e-01,  2.3800e-01]],\n",
      "\n",
      "         [[ 5.1354e-01,  8.1673e-01, -1.6334e-01,  ...,  9.5508e-01,\n",
      "            1.2315e-01, -2.7789e-01],\n",
      "          [ 3.0337e+00,  1.8800e-01, -2.7133e-01,  ..., -6.7888e-01,\n",
      "            6.8872e-01, -7.7556e-01],\n",
      "          [-3.2485e-01, -8.0873e-01,  1.5820e-01,  ..., -2.2793e-01,\n",
      "            4.2247e-01,  3.0868e-01],\n",
      "          ...,\n",
      "          [ 1.0155e+00, -1.6610e-01,  3.8723e-01,  ...,  1.5413e-01,\n",
      "           -3.4950e-01,  4.7878e-01],\n",
      "          [-2.0943e+00, -2.1091e-01, -3.6616e-01,  ..., -6.8274e-01,\n",
      "           -7.5489e-01,  1.1539e-01],\n",
      "          [-1.2228e+00,  7.9704e-01,  1.3047e+00,  ...,  6.1045e-01,\n",
      "           -3.9124e-01,  3.5107e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0266e+00,  3.2047e-01,  5.9948e-01,  ...,  1.1804e+00,\n",
      "            7.8567e-01, -8.5518e-02],\n",
      "          [-6.2718e-02,  2.7655e-01, -2.1078e-01,  ...,  3.3795e-01,\n",
      "           -6.3234e-01,  1.0934e+00],\n",
      "          [-2.6003e-01, -1.2777e+00, -7.2948e-01,  ..., -1.2096e+00,\n",
      "            5.3820e-01, -7.7004e-01],\n",
      "          ...,\n",
      "          [ 6.5006e-01, -2.5522e-01,  4.9970e-01,  ...,  1.0580e+00,\n",
      "           -5.5420e-01, -1.0884e+00],\n",
      "          [-1.0827e+00,  5.9156e-01, -2.2232e+00,  ..., -5.7629e-01,\n",
      "            1.7217e+00, -2.1497e-02],\n",
      "          [ 4.9110e-01,  7.3669e-01,  1.4268e-01,  ...,  9.9123e-01,\n",
      "           -9.5630e-02,  4.2903e-01]],\n",
      "\n",
      "         [[-2.8667e+00, -9.8777e-01,  9.8968e-01,  ...,  1.2438e+00,\n",
      "            1.4531e+00,  1.8625e+00],\n",
      "          [-5.5528e-01, -1.1622e+00, -1.7381e+00,  ...,  8.0032e-01,\n",
      "            3.4265e-01,  9.1096e-01],\n",
      "          [-1.5808e+00,  8.3980e-02, -1.8410e+00,  ...,  2.3791e-01,\n",
      "           -1.5980e+00,  7.3411e-01],\n",
      "          ...,\n",
      "          [ 2.5394e-01, -2.7571e-01, -1.9168e-01,  ..., -1.8109e+00,\n",
      "           -9.1897e-01, -1.0510e+00],\n",
      "          [ 2.7621e-01, -7.2017e-01, -7.8954e-01,  ..., -1.2141e-01,\n",
      "           -8.1641e-01, -8.7196e-01],\n",
      "          [-1.2380e-01,  5.1796e-01,  2.8619e-01,  ...,  1.3884e-01,\n",
      "           -2.1039e+00,  1.3669e+00]],\n",
      "\n",
      "         [[ 6.0101e-01,  4.9573e-01,  1.6346e+00,  ..., -1.4100e-01,\n",
      "            1.0391e+00,  1.1002e+00],\n",
      "          [ 1.5665e-01,  7.7053e-02, -6.1073e-01,  ..., -2.5901e-01,\n",
      "           -1.4248e+00, -1.3259e+00],\n",
      "          [ 9.2261e-01, -1.7583e+00, -6.5151e-01,  ...,  3.6100e-01,\n",
      "           -3.4024e-01, -6.0247e-01],\n",
      "          ...,\n",
      "          [ 3.1537e-01,  3.1067e-01, -5.7957e-01,  ..., -5.7134e-01,\n",
      "            1.1176e+00,  1.4539e+00],\n",
      "          [ 6.1618e-01, -1.3407e+00,  1.2532e+00,  ..., -1.3590e+00,\n",
      "           -2.3010e-01,  2.7817e-01],\n",
      "          [ 1.1410e+00, -2.1157e+00, -3.8900e-01,  ..., -2.3665e-01,\n",
      "            1.3752e+00,  5.6324e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.2442e-01,  8.0081e-01, -9.4938e-01,  ..., -6.1214e-01,\n",
      "            3.9457e-01, -9.2752e-01],\n",
      "          [-6.4570e-01, -5.8388e-01, -1.2561e+00,  ..., -2.7445e-01,\n",
      "           -9.5538e-01,  3.4810e-01],\n",
      "          [-2.5634e-01,  1.9921e+00, -3.2180e-01,  ...,  8.9691e-01,\n",
      "           -1.2632e+00,  4.8661e-01],\n",
      "          ...,\n",
      "          [-2.8454e-01, -1.7922e-01,  5.2108e-01,  ..., -6.0204e-01,\n",
      "           -7.9485e-01,  4.7669e-01],\n",
      "          [-2.2137e-01,  9.4163e-01,  8.1482e-01,  ...,  2.1474e-01,\n",
      "           -8.4516e-01,  1.1252e+00],\n",
      "          [-3.3073e-01, -8.1977e-01,  1.2821e+00,  ...,  1.0107e+00,\n",
      "           -5.1831e-01, -6.7886e-02]],\n",
      "\n",
      "         [[-1.3373e+00,  1.4886e+00,  1.0871e+00,  ..., -9.7254e-02,\n",
      "            8.5291e-01, -5.5505e-01],\n",
      "          [ 3.5183e-01, -5.7008e-01,  1.0446e-01,  ...,  1.7225e+00,\n",
      "            1.6793e+00,  2.6015e-01],\n",
      "          [-1.5353e+00,  2.0040e-01,  5.8931e-01,  ..., -1.1100e+00,\n",
      "            4.7676e-01, -1.3314e+00],\n",
      "          ...,\n",
      "          [-7.4006e-01,  2.0305e-01, -6.1001e-01,  ...,  2.2245e+00,\n",
      "           -3.6724e-01, -6.3619e-01],\n",
      "          [ 2.4442e-01,  2.4875e-01, -9.5162e-01,  ..., -8.1046e-02,\n",
      "            3.3399e-01,  9.0965e-01],\n",
      "          [-2.7837e-01,  8.6637e-01, -1.2726e+00,  ..., -4.5759e-01,\n",
      "            1.2492e+00, -3.7428e-01]],\n",
      "\n",
      "         [[-1.3383e+00,  8.7939e-01, -3.0694e-01,  ...,  3.4889e-01,\n",
      "           -1.0204e+00, -1.3610e+00],\n",
      "          [ 2.3223e-01,  1.6580e-01,  8.7737e-01,  ...,  2.5744e-01,\n",
      "           -1.9073e+00, -5.0462e-01],\n",
      "          [-2.8714e-01, -1.7766e+00,  1.2708e-02,  ...,  1.7191e+00,\n",
      "            8.6878e-01,  9.3957e-01],\n",
      "          ...,\n",
      "          [ 1.0254e+00,  9.9084e-01, -3.8844e-02,  ...,  7.1262e-01,\n",
      "           -3.2698e-01, -2.3403e-01],\n",
      "          [ 1.2172e+00,  9.1696e-02, -4.6015e-01,  ...,  1.5228e-01,\n",
      "           -1.2977e+00, -8.3188e-01],\n",
      "          [ 8.0183e-01, -2.6241e-01,  3.8057e-01,  ...,  1.3952e+00,\n",
      "           -8.5745e-02,  4.9702e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8936e+00, -2.8320e+00,  1.2301e+00,  ...,  1.2731e+00,\n",
      "            4.2809e-01,  1.4243e+00],\n",
      "          [ 4.3602e-01,  1.7668e-01,  2.6599e-01,  ...,  8.6079e-01,\n",
      "           -7.0232e-02,  1.2524e+00],\n",
      "          [-5.1471e-01, -1.0384e-01, -8.6229e-01,  ..., -3.2616e-01,\n",
      "           -2.7318e-01,  1.3399e+00],\n",
      "          ...,\n",
      "          [ 5.4327e-01,  1.1236e+00,  2.0545e+00,  ...,  3.8993e-01,\n",
      "            1.5295e+00, -7.0674e-02],\n",
      "          [ 1.0914e+00, -4.4209e-01, -9.4138e-01,  ..., -4.1472e-01,\n",
      "           -2.4794e+00, -3.6665e-01],\n",
      "          [-2.3704e+00, -4.3375e-01,  5.1666e-01,  ...,  8.2110e-01,\n",
      "           -2.0781e+00,  1.0595e-01]],\n",
      "\n",
      "         [[-8.6951e-01, -4.0515e-01,  1.0342e+00,  ...,  7.2516e-01,\n",
      "           -2.2027e+00,  6.0977e-01],\n",
      "          [-5.1024e-01, -5.3234e-01,  4.5661e-02,  ..., -2.0132e+00,\n",
      "            2.3512e-04, -1.1735e+00],\n",
      "          [ 1.0925e+00, -3.7888e-01, -1.3657e+00,  ...,  2.6199e-01,\n",
      "            1.4007e+00,  8.6569e-01],\n",
      "          ...,\n",
      "          [-3.0544e-01,  2.0171e-02,  3.1944e-01,  ...,  4.1757e-01,\n",
      "           -4.1621e-01, -1.2490e+00],\n",
      "          [-8.0541e-02, -1.0136e+00,  1.0996e+00,  ...,  6.8335e-01,\n",
      "           -9.4791e-01,  1.6220e+00],\n",
      "          [-1.0610e+00, -9.0137e-01, -7.5743e-01,  ...,  1.3520e+00,\n",
      "           -1.2382e+00, -1.1774e+00]],\n",
      "\n",
      "         [[ 1.3975e+00,  7.8711e-01,  1.3857e-01,  ...,  1.5326e+00,\n",
      "            8.8493e-01,  7.7283e-01],\n",
      "          [ 1.6755e+00,  1.1498e+00,  2.0582e-01,  ...,  7.1811e-01,\n",
      "           -5.4456e-02,  7.4101e-02],\n",
      "          [ 9.5436e-01, -1.1237e-01, -1.9612e-01,  ...,  1.8015e-01,\n",
      "           -2.8328e-01, -1.5407e+00],\n",
      "          ...,\n",
      "          [-4.4193e-01,  6.4465e-01, -2.7262e-01,  ...,  3.2751e-01,\n",
      "           -6.3284e-01, -1.8467e-01],\n",
      "          [-4.1495e-01, -3.8814e-01,  5.5513e-01,  ...,  6.3152e-01,\n",
      "           -2.8244e+00,  1.0639e+00],\n",
      "          [ 6.1080e-02, -1.1378e+00,  2.6932e-01,  ...,  3.9251e-01,\n",
      "            3.4168e-01, -4.4605e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.9239e+00,  1.4214e+00,  1.1806e+00,  ..., -1.8433e-01,\n",
      "            5.1679e-01, -4.0193e-01],\n",
      "          [ 1.2318e-01, -1.9382e-01,  7.0058e-01,  ...,  8.0576e-01,\n",
      "            1.0412e+00, -1.8269e+00],\n",
      "          [ 2.2231e+00, -1.8630e+00,  7.5637e-01,  ..., -1.2187e-01,\n",
      "            3.1796e-01, -3.0751e+00],\n",
      "          ...,\n",
      "          [ 6.3331e-01, -8.0957e-01,  6.8050e-01,  ...,  5.3135e-01,\n",
      "           -5.0459e-01,  4.5409e-01],\n",
      "          [ 1.0245e+00, -3.5656e-01, -2.1232e-01,  ..., -1.0297e+00,\n",
      "            5.8985e-01,  8.6485e-01],\n",
      "          [-7.4870e-01,  7.5654e-01, -9.2079e-01,  ...,  4.7575e-01,\n",
      "            3.1388e-01,  3.1221e-01]],\n",
      "\n",
      "         [[-2.6771e-01,  1.2294e+00,  3.7061e-01,  ..., -3.9836e-01,\n",
      "           -8.1657e-01, -1.2075e-01],\n",
      "          [ 1.3270e+00, -1.6710e-01,  1.0892e+00,  ..., -5.7395e-01,\n",
      "            6.1702e-01, -3.3307e-01],\n",
      "          [ 7.9029e-01, -1.0186e+00, -3.2578e-01,  ...,  1.3729e+00,\n",
      "           -2.6175e-01,  3.6345e-01],\n",
      "          ...,\n",
      "          [-1.9275e+00, -4.4643e-01,  9.6860e-01,  ..., -3.2826e-01,\n",
      "           -1.1656e+00,  8.1773e-01],\n",
      "          [-7.9406e-01, -1.6171e-01, -4.6123e-01,  ..., -3.9676e-02,\n",
      "           -3.1438e-01, -8.3189e-01],\n",
      "          [-9.0846e-01,  3.5211e-01, -1.6744e+00,  ...,  5.6903e-02,\n",
      "            3.3132e-01, -1.2222e+00]],\n",
      "\n",
      "         [[-2.0969e+00, -1.3717e+00, -1.6704e+00,  ...,  1.0831e+00,\n",
      "            2.1550e-01,  8.8155e-01],\n",
      "          [-2.0373e+00, -2.1173e-01,  3.4220e-01,  ..., -1.3284e+00,\n",
      "           -5.3431e-01, -1.1674e+00],\n",
      "          [-9.2058e-01, -9.9515e-01,  4.2527e-02,  ..., -8.6535e-01,\n",
      "           -2.5718e-01, -2.9208e-01],\n",
      "          ...,\n",
      "          [ 2.3056e+00,  1.1081e+00,  8.7544e-01,  ..., -4.1949e-01,\n",
      "            8.0466e-01,  9.4198e-01],\n",
      "          [ 1.0016e+00, -5.9304e-01,  1.5748e+00,  ..., -3.1618e-01,\n",
      "            6.7941e-01,  1.0886e+00],\n",
      "          [-1.4950e+00,  1.3620e+00,  2.0289e+00,  ...,  6.3544e-02,\n",
      "            6.0990e-01, -6.1049e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7239e+00, -1.2984e-01, -2.0263e+00,  ..., -1.7657e-02,\n",
      "           -5.6255e-02, -2.4501e-02],\n",
      "          [-1.1787e+00, -7.4989e-01,  5.2681e-01,  ...,  2.3371e-01,\n",
      "           -1.7342e-01, -3.4008e-01],\n",
      "          [-1.7310e+00,  7.5034e-01,  7.6206e-02,  ..., -4.1452e-01,\n",
      "           -1.7543e+00,  3.9122e-01],\n",
      "          ...,\n",
      "          [ 1.9189e-01, -1.2508e-01, -1.0815e+00,  ...,  9.3439e-01,\n",
      "            2.1451e+00,  6.5092e-01],\n",
      "          [-2.6155e-02, -1.9923e-01, -6.6676e-01,  ...,  4.2207e-01,\n",
      "            1.3690e+00, -4.5477e-02],\n",
      "          [ 6.1776e-01, -2.1018e-01,  1.2856e+00,  ...,  3.3637e-01,\n",
      "            3.4595e-01,  3.0115e-01]],\n",
      "\n",
      "         [[-7.2547e-01, -5.7427e-01,  7.9717e-01,  ...,  4.2675e-01,\n",
      "            2.6409e+00,  9.0864e-02],\n",
      "          [ 1.2589e-01,  3.9411e-01,  1.4975e+00,  ...,  1.2358e+00,\n",
      "           -4.1325e-01, -1.5702e-01],\n",
      "          [ 2.8582e-01,  5.8411e-01, -7.3752e-01,  ...,  4.8152e-01,\n",
      "            1.4048e+00,  6.8338e-01],\n",
      "          ...,\n",
      "          [-1.4700e+00,  6.3737e-01, -1.3809e+00,  ..., -5.4359e-01,\n",
      "           -1.2717e+00, -5.1467e-01],\n",
      "          [-2.1265e-01,  6.3320e-01, -6.5910e-01,  ...,  1.4725e+00,\n",
      "           -2.7192e-01,  1.0673e+00],\n",
      "          [-1.1971e+00,  2.5737e-01,  1.0089e-01,  ..., -7.3388e-01,\n",
      "            3.6920e-01, -1.2880e+00]],\n",
      "\n",
      "         [[-1.4928e-01, -1.0197e+00, -7.1102e-01,  ...,  2.3307e-01,\n",
      "            1.7055e+00,  1.0219e+00],\n",
      "          [-4.8613e-01, -2.0175e-01, -7.2672e-01,  ...,  2.5843e-02,\n",
      "            1.5893e+00,  7.9793e-01],\n",
      "          [ 1.2977e-01, -5.8944e-01, -1.7279e+00,  ..., -4.4390e-01,\n",
      "           -1.3417e-01,  3.9722e-01],\n",
      "          ...,\n",
      "          [-5.8890e-01, -2.3839e-02, -2.9181e+00,  ...,  1.1520e+00,\n",
      "            5.0445e-01, -1.6004e+00],\n",
      "          [-1.5124e+00, -1.2041e+00,  5.0471e-01,  ..., -7.4748e-01,\n",
      "            9.7156e-01, -1.5468e+00],\n",
      "          [ 1.0907e+00,  8.0117e-01, -2.6519e-01,  ...,  7.5415e-01,\n",
      "           -6.1773e-02, -1.4632e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "head_dim = 16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Generate random input tensor (batch_size, seq_len, num_heads, head_dim)\n",
    "x = torch.randn(batch_size, seq_len, 8, head_dim, device=device)\n",
    "\n",
    "# Precompute frequency complex numbers\n",
    "freqs_complex = precompute_theta_pos_frequencies(head_dim, seq_len, device)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "x_rot = apply_rotary_embeddings(x, freqs_complex, device)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Rotated shape:\", x_rot.shape)   \n",
    "print(x_rot[:5]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped Query Attention (GQA)\n",
    "\n",
    "**Grouped Query Attention (GQA)** is a variation of the standard **Multi-Head Attention (MHA)** mechanism used in transformer models. Instead of having multiple attention heads (as in MHA), GQA organizes queries into a smaller number of **query groups**, while the keys and values are not grouped. This approach reduces computational complexity while retaining expressive power.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/GQA.png\" alt=\"Efficient RoPE\" />\n",
    "</p>\n",
    "\n",
    "#### Key Components of GQA:\n",
    "\n",
    "1. **Query Grouping**:\n",
    "   - Queries are divided into groups, leading to fewer query computations compared to MHA.\n",
    "   - Keys and values are not grouped, allowing them to attend to all input tokens.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Reduces the memory and compute requirements of the attention mechanism compared to MHA.\n",
    "   - Particularly advantageous for large models or when memory efficiency is critical.\n",
    "\n",
    "\n",
    "#### Differences Between GQA and MHA:\n",
    "\n",
    "| **Feature**              | **Multi-Head Attention (MHA)**                  | **Grouped Query Attention (GQA)**         |\n",
    "|--------------------------|------------------------------------------------|-------------------------------------------|\n",
    "| **Query Organization**    | Multiple independent attention heads           | Queries are grouped into fewer groups     |\n",
    "| **Key & Value Behavior**  | Separate keys and values for each attention head| Shared keys and values across query groups|\n",
    "| **Complexity**            | Higher due to multiple heads                   | Lower due to reduced number of query groups|\n",
    "| **Use Case**              | Suitable for tasks requiring high expressiveness | Suitable for memory-efficient scenarios   |\n",
    "\n",
    "\n",
    "#### Practical Implication:\n",
    "\n",
    "While MHA excels in providing diverse attention mechanisms across multiple heads, GQA trades off some of this flexibility to improve efficiency. It is commonly used in modern large-scale transformer architectures, like the **LLaMA** series, where scaling efficiency is critical without significantly compromising performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedBuffers:\n",
    "    # Dictionary to store precomputed buffers\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32, device=\"cpu\"):\n",
    "        \n",
    "        # Unique key based on the provided parameters to identify buffer configurations\n",
    "        key = (\n",
    "            context_length,\n",
    "            head_dim,\n",
    "            rope_base,\n",
    "            tuple(freq_config.values()) if freq_config else freq_config,\n",
    "            dtype\n",
    "        )\n",
    "\n",
    "        # Check if the buffers for this configuration already exist\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            # If not, create the buffers\n",
    "\n",
    "            # 1. Create a causal mask (upper triangular matrix)\n",
    "            # Ensures future tokens do not attend to past tokens in the sequence\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "            # 2. Compute RoPE frequencies using a custom precompute function\n",
    "            # This function is assumed to generate complex frequencies for positional encoding\n",
    "            freqs_complex = precompute_theta_pos_frequencies(\n",
    "                head_dim, context_length, device, rope_base\n",
    "            )\n",
    "\n",
    "            # 3. Convert the RoPE frequencies to the desired data type if specified\n",
    "            if dtype is not None:\n",
    "                freqs_complex = freqs_complex.to(dtype)\n",
    "\n",
    "            # 4. Cache the mask and frequencies in the shared dictionary\n",
    "            SharedBuffers._buffers[key] = (mask, freqs_complex)\n",
    "\n",
    "        # Return the cached or newly created buffers\n",
    "        return SharedBuffers._buffers[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SharedBuffers Class**\n",
    "\n",
    "The `SharedBuffers` class optimizes the efficiency of transformer models by precomputing and sharing reusable resources like causal masks and rotary position encoding frequencies. This avoids redundant computations and saves memory when processing sequences of different configurations.\n",
    "\n",
    "### **Why Implement This?**\n",
    "- **Improves Performance:** Precomputes frequently used tensors like causal masks and RoPE frequencies to avoid repeated calculations.\n",
    "- **Saves Memory:** Stores shared resources in a centralized cache, preventing duplication.\n",
    "- **Handles Multiple Configurations:** Manages buffers for varying sequence lengths, head dimensions, and positional encoding settings.\n",
    "\n",
    "### **Benefits**\n",
    "1. **Speed:** Precomputes and caches buffers for quick reuse.\n",
    "2. **Flexibility:** Supports varying configurations without conflicts.\n",
    "3. **Scalability:** Ideal for large transformer models like GPT variants.\n",
    "\n",
    "This approach streamlines the transformer pipeline by efficiently managing shared resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups, rope_base=10_000, rope_config=None,\n",
    "            dtype=None, device='cpu'\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validations for divisibility constraints\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        # Initialization of key attributes\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Dimension per head\n",
    "        self.device = device  # Store device for tensor operations\n",
    "\n",
    "        # Linear transformations for keys, values, and queries\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        # Output projection layer for combining attention results\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        # Number of heads per group\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        # Fetch shared buffers for causal masks and RoPE frequencies\n",
    "        mask, freqs_complex = SharedBuffers.get_buffers(\n",
    "            context_length, self.head_dim, rope_base, rope_config, dtype=dtype, device=device\n",
    "        )\n",
    "        self.register_buffer(\"mask\", mask)  # Cache causal mask as a buffer\n",
    "        self.register_buffer(\"freqs_complex\", freqs_complex)  # Cache RoPE frequencies as a buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Input tensor of shape (batch_size, num_tokens, d_in)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Compute keys, values, and queries\n",
    "        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape keys, values, and queries for attention heads\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        # Transpose for compatibility with attention computation\n",
    "        keys = keys.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        # Apply rotary position encoding (RoPE) to keys and queries\n",
    "        keys = apply_rotary_embeddings(keys, self.freqs_complex, self.device)\n",
    "        queries = apply_rotary_embeddings(queries, self.freqs_complex, self.device)\n",
    "\n",
    "        # Expand keys and values to match the number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        # Compute scaled dot-product attention with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]  # Adjust mask size to current input length\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)  # Apply causal mask\n",
    "\n",
    "        # Normalize scores with softmax\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)  # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Compute attention output\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        # Combine attention heads into final output shape\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # Apply optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Configration of Llama 3.2 for : \n",
    "# 1- 1B Patameters \n",
    "# 2- 3B Parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.2 1B\n",
    "\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # Context length\n",
    "    \"emb_dim\": 2048,            # Embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 16,             # Number of layers\n",
    "    \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Llama 3.2 3B\n",
    "\n",
    "# LLAMA32_CONFIG = {\n",
    "#     \"vocab_size\": 128_256,      # Vocabulary size\n",
    "#     \"context_length\": 131_072,  # Context length\n",
    "#     \"emb_dim\": 3072,            # Embedding dimension\n",
    "#     \"n_heads\": 24,              # Number of attention heads\n",
    "#     \"n_layers\": 28,             # Number of layers\n",
    "#     \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "#     \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "#     \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "#     \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "#     \"rope_freq\": {              # RoPE frequency scaling\n",
    "#         \"factor\": 32.0,\n",
    "#         \"low_freq_factor\": 1.0,\n",
    "#         \"high_freq_factor\": 4.0,\n",
    "#         \"original_context_length\": 8192,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "LLAMA_SIZE_STR = \"1B\" if LLAMA32_CONFIG[\"emb_dim\"] == 2048 else \"3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the context length so the model would work fine  (if you have more RAM, feel free to comment out the lines below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RoPE theta: 31250.0\n"
     ]
    }
   ],
   "source": [
    "old_context_length = LLAMA32_CONFIG[\"context_length\"]\n",
    "LLAMA32_CONFIG[\"context_length\"] = 8192\n",
    "\n",
    "\n",
    "def rescale_theta(theta_old, context_length_old, context_length_new):\n",
    "    scaling_factor = context_length_new / context_length_old\n",
    "    theta_new = theta_old * scaling_factor\n",
    "    return theta_new\n",
    "\n",
    "LLAMA32_CONFIG[\"rope_base\"] = rescale_theta(\n",
    "    LLAMA32_CONFIG[\"rope_base\"],\n",
    "    old_context_length,\n",
    "    LLAMA32_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"New RoPE theta:\", LLAMA32_CONFIG[\"rope_base\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here a compact implemntaion of Llama 3.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama3.py\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# FeedForward Class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize the first linear layer: projects input embedding to hidden dimension\n",
    "        self.fc1 = nn.Linear(config['emb_dim'], config['hidden_dim'], bias=False)\n",
    "        \n",
    "        # Initialize the second linear layer: creates additional projections for gating mechanism\n",
    "        self.fc2 = nn.Linear(config['emb_dim'], config['hidden_dim'], bias=False)\n",
    "        \n",
    "        # Initialize the third linear layer: maps back from hidden dimension to original embedding dimension\n",
    "        self.fc3 = nn.Linear(config['hidden_dim'], config['emb_dim'], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute first projection\n",
    "        x1 = self.fc1(x)\n",
    "        # Compute second projection for gating\n",
    "        x2 = self.fc2(x)\n",
    "        # Element-wise multiplication after SiLU activation introduces non-linear interactions\n",
    "        x = F.silu(x1) * x2 \n",
    "        # Project back to the original embedding space\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# GroupedQueryAttention Class\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups, rope_base=10_000, rope_config=None,\n",
    "            dtype=None, device='cpu'  # Device is specified with a default value\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        # Initialize essential parameters\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.device = device\n",
    "\n",
    "        # Define linear layers for keys, values, and queries\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        \n",
    "        # Final projection layer after attention\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        # Configure the grouping structure for keys and values\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        # Shared buffers for causal mask and rotary embeddings\n",
    "        mask, freqs_complex = SharedBuffers.get_buffers(\n",
    "            context_length, self.head_dim, rope_base, rope_config, dtype=dtype, device=device\n",
    "        )\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"freqs_complex\", freqs_complex)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Compute queries, keys, and values\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape to support multi-head processing\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        # Transpose for compatibility with attention computation\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # Apply rotary embeddings for positional information\n",
    "        keys = apply_rotary_embeddings(keys, self.freqs_complex, self.device)\n",
    "        queries = apply_rotary_embeddings(queries, self.freqs_complex, self.device)\n",
    "\n",
    "        # Expand keys and values to align with query groups\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Compute attention scores and apply causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Normalize attention scores and compute weighted sum of values\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads and apply final projection\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "# TransformerBlock Class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize multi-head attention layer\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=config[\"emb_dim\"],\n",
    "            d_out=config[\"emb_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            num_kv_groups=config[\"n_kv_groups\"],\n",
    "            rope_base=config[\"rope_base\"],\n",
    "            rope_config=config[\"rope_freq\"],\n",
    "            dtype=config[\"dtype\"]\n",
    "        )\n",
    "        \n",
    "        # Initialize feedforward network\n",
    "        self.ff = FeedForward(config)\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.norm1 = nn.RMSNorm(config['emb_dim'])\n",
    "        self.norm2 = nn.RMSNorm(config['emb_dim'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention with residual connection\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16))\n",
    "        x = x + shortcut\n",
    "        \n",
    "        # Apply feedforward network with residual connection\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "# Llama3 Class\n",
    "class Llama3(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embedding layer to transform input tokens to dense vectors\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            config['vocab_size'], config['emb_dim'], dtype=config['dtype']\n",
    "        )\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config['n_layers'])]\n",
    "        )\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.final_norm = nn.RMSNorm(config['emb_dim'])\n",
    "        \n",
    "        # Output projection to vocabulary size\n",
    "        self.out_head = nn.Linear(\n",
    "            config['emb_dim'], config['vocab_size'], bias=False, dtype=config['dtype']\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert token indices to embeddings\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        x = tok_emb\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        x = self.trf_blocks(tok_emb)\n",
    "        \n",
    "        # Normalize and project to logits\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 128256, 'context_length': 8192, 'emb_dim': 2048, 'n_heads': 32, 'n_layers': 16, 'hidden_dim': 8192, 'n_kv_groups': 8, 'rope_base': 31250.0, 'dtype': torch.bfloat16, 'rope_freq': {'factor': 32.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_context_length': 8192}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13560\\1194935297.py:16: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Copy.cpp:305.)\n",
      "  freqs_complex = freqs_complex.to(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama3(\n",
      "  (token_emedding): Embedding(128256, 2048)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (12): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (13): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (14): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "    (15): TransformerBlock(\n",
      "      (att): GroupedQueryAttention(\n",
      "        (W_key): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_value): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (W_query): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (fc3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (norm1): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "      (norm2): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm((2048,), eps=None, elementwise_affine=True)\n",
      "  (out_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(LLAMA32_CONFIG)\n",
    "model = Llama3(LLAMA32_CONFIG)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effiect methods for Mmemory saving : \n",
    "1- Lets verify that shared buffer class is working proplerly and there is no wasted or redundant computaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\n",
    "print(model.trf_blocks[0].att.freqs_complex is model.trf_blocks[-1].att.freqs_complex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Lets check the total number of the unique parametrs with the rescpect of the whole parametrs (will we use this info later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,498,482,688\n",
      "\n",
      "Total number of unique parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.token_emedding.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- the effect of torch.bfloat16 compared to torch.float32 , as you will see the diifrence is huge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 11.42 GB\n",
      "bfloat16: 5.71 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        # Ensure that the provided model path is a valid file\n",
    "        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n",
    "        \n",
    "        # Load the mergeable ranks (Byte Pair Encoding) specific to the model\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        # Define special tokens for LLaMA3\n",
    "        # These tokens help manage text boundaries, reserved spaces, and other control features in the text generation process\n",
    "        self.special_tokens = {\n",
    "            \"<|begin_of_text|>\": 128000,  # Token to signify the beginning of the text\n",
    "            \"<|end_of_text|>\": 128001,    # Token to signify the end of the text\n",
    "            \"<|start_header_id|>\": 128006,  # Token for starting a header section\n",
    "            \"<|end_header_id|>\": 128007,   # Token for ending a header section\n",
    "            \"<|eot_id|>\": 128009,         # Token for end-of-text (e.g., indicating the end of a content block)\n",
    "        }\n",
    "        \n",
    "        # Additional reserved tokens for model-specific functionality (e.g., controlling certain operations)\n",
    "        self.special_tokens.update({\n",
    "            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n",
    "        })\n",
    "\n",
    "        # Define the tokenization pattern\n",
    "        # This regular expression is more complex compared to GPT-2 and captures more varied tokenization patterns\n",
    "        # Handles contractions, special characters, and complex text formats like headers or sequences\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,  # Use the name of the model file as the encoding name\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",  # Regex pattern for tokenization\n",
    "            mergeable_ranks=mergeable_ranks,  # Load the BPE mergeable ranks specific to the model\n",
    "            special_tokens=self.special_tokens  # Include special tokens defined earlier\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n",
    "        \"\"\"\n",
    "        Encodes a given text input into tokens, optionally including special tokens like bos (beginning of sequence)\n",
    "        and eos (end of sequence). The `allowed_special` and `disallowed_special` parameters allow for fine control \n",
    "        over which special tokens are included.\n",
    "        \"\"\"\n",
    "        \n",
    "        tokens = []\n",
    "\n",
    "        # Optionally add the <|begin_of_text|> token if 'bos' is True (indicating the start of the sequence)\n",
    "        if bos:\n",
    "            tokens = [self.special_tokens[\"<|begin_of_text|>\"]]\n",
    "\n",
    "        # Use the tokenizer's `encode` method to tokenize the input text\n",
    "        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "\n",
    "        # Optionally add the <|end_of_text|> token if 'eos' is True (indicating the end of the sequence)\n",
    "        if eos:\n",
    "            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "        Decodes a sequence of tokens back into the original text.\n",
    "        \"\"\"\n",
    "        return self.model.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tiktoken for Llam 3.2\n",
    "\n",
    "When comparing the tokenizer used in **LLaMA3** to that in **GPT-2**, there are several notable differences in how they handle tokenization and special token management. Here‚Äôs a breakdown of why these customizations are specific to **LLaMA3**:\n",
    "\n",
    "# 1. **Special Tokens Handling**\n",
    "\n",
    "In **GPT-2**, tokenization with `tiktoken` primarily involves using basic special tokens like `bos` (beginning of sequence) and `eos` (end of sequence). However, in **LLaMA3**, the tokenizer introduces custom special tokens:\n",
    "\n",
    "- **Custom Special Tokens**:\n",
    "  - `<|begin_of_text|>`: Marks the beginning of the sequence.\n",
    "  - `<|end_of_text|>`: Marks the end of the sequence.\n",
    "  - `<|start_header_id|>` and `<|end_header_id|>`: Used for identifying sections or headers within documents.\n",
    "  - `<|eot_id|>`: A special end-of-text token.\n",
    "  - **Reserved Tokens**: A series of reserved tokens (e.g., `<|reserved_0|>`, `<|reserved_1|>`, etc.) for managing model-specific operations.\n",
    "  \n",
    "These tokens help manage text boundaries and reserved spaces within the sequence, allowing for more controlled text generation. **GPT-2** didn‚Äôt need these since its primary concern was managing simple `bos` and `eos` tokens.\n",
    "\n",
    "# 2. **Custom Tokenization Pattern**\n",
    "\n",
    "The `pat_str` (regular expression) used in **LLaMA3** is more complex and nuanced compared to **GPT-2**:\n",
    "\n",
    "- **Complex Pattern**:\n",
    "  - The regex pattern in **LLaMA3** accounts for a wider variety of tokens, including contractions like `'s`, `'t`, `'ve`, etc., non-word characters, and specific sequences (e.g., `\\p{L}` for letters, `\\p{N}` for numbers).\n",
    "  - This complex pattern is designed to capture tokenization nuances needed by **LLaMA3** for diverse, complex text inputs that might not adhere to standardized text structures. It ensures proper tokenization across a broader set of inputs.\n",
    "  \n",
    "  **GPT-2**, on the other hand, used a simpler pattern that assumed more standardized text data.\n",
    "\n",
    "# 3. **Model-Specific Token Mergeable Ranks (BPE)**\n",
    "\n",
    "- In **LLaMA3**, a custom BPE (Byte Pair Encoding) mergeable rank table is loaded from the model path using `load_tiktoken_bpe(model_path)`.\n",
    "- This customization reflects **LLaMA3**‚Äôs unique tokenization scheme, which might be trained to handle specific token merging requirements, vocabulary specifics, and other model-specific data.\n",
    "- **GPT-2** used a standard BPE rank table, as its training data was more general and less model-specific.\n",
    "\n",
    "# 4. **Configurable Special Token Behavior in Encoding**\n",
    "\n",
    "- **LLaMA3**‚Äôs `encode` method allows for fine-grained control over special tokens:\n",
    "  - `bos` (beginning-of-sequence): Optionally include `<|begin_of_text|>` at the start of a sequence.\n",
    "  - `eos` (end-of-sequence): Optionally append `<|end_of_text|>` at the end.\n",
    "  - `allowed_special` and `disallowed_special`: These parameters allow the user to control which special tokens can or cannot be used during encoding. This feature is particularly useful when fine-tuning the model for specific tasks or generating text with constraints.\n",
    "- **GPT-2** didn‚Äôt offer this level of flexibility; its configuration was more rigid and predefined, focusing only on basic `bos` and `eos` tokens.\n",
    "\n",
    "# Why These Additions for LLaMA3?\n",
    "\n",
    "These customizations in **LLaMA3**'s tokenizer reflect its more **complex text processing requirements**:\n",
    "\n",
    "1. **Greater control over text boundaries**: With tokens like `<|start_header_id|>` and `<|end_header_id|>`, **LLaMA3** can manage sections or headers within documents, which is crucial for structured data or document-level tasks.\n",
    "2. **Advanced tokenization flexibility**: The complex `pat_str`, configurable token behavior, and customized BPE patterns in **LLaMA3** allow for nuanced handling of text inputs, enabling tasks like document processing, complex text generation, and preserving context throughout the input sequence.\n",
    "3. **Fine-grained management of special tokens**: The ability to define `allowed_special` and `disallowed_special` tokens allows **LLaMA3** to manage sequences more precisely, especially during inference and fine-tuning.\n",
    "4. **Model-Specific Training Requirements**: The customizations in **LLaMA3**‚Äôs tokenizer are tailored to fit its training data and architecture. This includes a need for specific token merge behaviors, vocabulary coverage, and how tokens are processed during tokenization.\n",
    "\n",
    "In summary, the tokenizer used in **LLaMA3** has been customized to handle complex text processing needs like managing sections, headers, complex token merging, and flexible token behavior. These additions make **LLaMA3** more flexible and capable in handling a diverse set of text tasks compared to **GPT-2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the open weights using Hugging Face\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/gf1.png\" alt=\"Efficient RoPE\" />\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/gf2.png\" alt=\"Efficient RoPE\" />\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/gf3.png\" alt=\"Efficient RoPE\" />\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/gf4.png\" alt=\"Efficient RoPE\" />\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/gf5.png\" alt=\"Efficient RoPE\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a9b01a4e73443a922faf31ef95b74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make sure you have an account to generate access token \n",
    "# make sure t have the right premission to access the repo from huggingface\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "LLAMA_SIZE_STR = \"1B\" if LLAMA32_CONFIG[\"emb_dim\"] == 2048 else \"3B\"\n",
    "\n",
    "\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 11, 1268, 527, 499, 30]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(tokenizer_file_path)\n",
    "\n",
    "print(tokenizer.model.encode(\"Hello, how are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of the `assign` and `load_weights_into_llama` Functions\n",
    "\n",
    "The `assign` function is responsible for verifying the shape consistency between two tensors and then copying the data from one tensor (right) to another (left). If the `right` tensor is a PyTorch tensor, it is cloned and detached before being returned as a `torch.nn.Parameter`. If not, a tensor is created from the data and returned as a `torch.nn.Parameter`.\n",
    "\n",
    "The `load_weights_into_llama` function uses the `assign` function to load pre-trained weights into the LLaMA model. It ensures that the model's weights are correctly initialized from a given `params` dictionary. The function handles multiple layers, including the token embedding layer, attention layers, feed-forward layers, normalization layers, and the output layer. It also checks for weight tying between the embedding and output layers.\n",
    "\n",
    "Together, these functions facilitate loading and validating the weights of a model, ensuring that the parameters align correctly, which is critical for the model to perform well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right, tensor_name=\"unknown\"):\n",
    "    # Ensure that the shapes of the tensors are the same\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    # If 'right' is already a tensor, clone and detach it to avoid modifying the original tensor\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        # If 'right' is not a tensor, create a new tensor and return it as a Parameter\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    # Load the token embedding weight from the 'params' dictionary into the model\n",
    "    model.token_emedding.weight = assign(model.token_emedding.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "\n",
    "    # Loop through each layer in the model and load the corresponding weights\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights for the current layer\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # Load FeedForward network weights for the current layer\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # Load the final normalization layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    # If the model has a language model head weight, load it\n",
    "    if \"lm_head.weight\" in params.keys():\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        # If no language model head weight exists, tie the embedding and output head weights\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "        print(\"Model uses weight tying.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Model Weight Loading in Neural Networks\n",
    "\n",
    "When working with large-scale pre-trained models, the size of the model's weight files can become a bottleneck for downloading, loading, and memory management. Sequential model weight loading is a strategy designed to handle such challenges efficiently. Here's a breakdown of the concept and its implementation:\n",
    "\n",
    "## Concept of Sequential Model Weight Loading\n",
    "\n",
    "### Purpose:\n",
    "Sequential model weight loading allows for:\n",
    "- **Efficient handling of large model weights**: Breaking large weight files into smaller parts reduces memory pressure during download and loading.\n",
    "- **Scalability**: Supports models of varying sizes, from smaller configurations to massive models that require splitting.\n",
    "- **Flexibility**: Enables selective loading or merging of weights based on storage formats and requirements.\n",
    "\n",
    "### How It Works:\n",
    "1. **Small Models**: If the model weights fit into a single file, the weights can be directly loaded into memory.\n",
    "2. **Large Models**: \n",
    "   - The weight files are split into smaller chunks, such as `model-00001-of-00002`, `model-00002-of-00002`, etc.\n",
    "   - These parts are loaded sequentially, and their contents are combined into a unified dictionary.\n",
    "   - Once all parts are merged, the combined weights are loaded into the model.\n",
    "\n",
    "### Benefits:\n",
    "- **Memory Optimization**: Downloads and processes smaller chunks rather than loading everything into memory at once.\n",
    "- **Ease of Distribution**: Makes it easier to share and store large model files by dividing them into smaller parts.\n",
    "- **Consistency**: The model is loaded with all weights in a unified manner after combination.\n",
    "\n",
    "## Example Implementation\n",
    "The following demonstrates sequential weight loading:\n",
    "This script handles the loading of pre-trained weights for the LLaMA model, using the SafeTensors format, which is efficient and safe for model storage. The logic adjusts depending on the model size, either downloading a single file for smaller models or multiple files for larger ones, combining them as needed before loading the weights into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uses weight tying.\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file  # Import the function to load weights in the SafeTensors format.\n",
    "\n",
    "# Check the size of the model being used\n",
    "if LLAMA_SIZE_STR == \"1B\":\n",
    "    # If the model is 1 billion parameters, download a single weights file\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",  # Repository name on Hugging Face hub\n",
    "        filename=f\"model.safetensors\",                              # Single file for smaller models\n",
    "        local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"            # Directory to store downloaded weights\n",
    "    )\n",
    "    combined_weights = load_file(weights_file)  # Load the weights into memory.\n",
    "\n",
    "else:\n",
    "    # For larger models, weights are split across multiple files. Combine them.\n",
    "    combined_weights = {}\n",
    "    for i in range(1, 3):  # Assuming weights are split into two parts.\n",
    "        weights_file = hf_hub_download(\n",
    "            repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",  # Repository name on Hugging Face hub\n",
    "            filename=f\"model-0000{i}-of-00002.safetensors\",             # Access each part by index\n",
    "            local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"            # Directory to store downloaded weights\n",
    "        )\n",
    "        current_weights = load_file(weights_file)  # Load the current part of the weights.\n",
    "        combined_weights.update(current_weights)   # Merge the current part into the combined weights.\n",
    "\n",
    "# Load the combined weights into the LLaMA model\n",
    "load_weights_into_llama(model, LLAMA32_CONFIG, combined_weights)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Free up memory by deleting the combined weights after loading them into the model\n",
    "del combined_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêé Summary of  Notebook 3.2: Building LLaMA 3.2 (1B and 3B Parameters) ‚Äì A Reverse Journey!  \n",
    "\n",
    "üéâ **Congratulations on completing this notebook!** Here's a recap of what we achieved:  \n",
    "\n",
    "\n",
    "## **What We Did**  \n",
    "\n",
    "### **From GPT-2 to LLaMA 3.2**  \n",
    "We started with a strong foundation by building GPT-2, and then took a step up to construct LLaMA 3.2 models with **1B** and **3B parameters**.  \n",
    "- **Key Differentiation**: We highlighted the architectural differences between GPT-2 and LLaMA models.  \n",
    "- **Reverse (Top-Down) Approach**: By taking a top-down perspective, we gained new insights into building transformers from the big picture to the fine details.  \n",
    "\n",
    "\n",
    "### **Customized Tokenizer**  \n",
    "To suit LLaMA‚Äôs needs, we extended the capabilities of the `tiktoken` tokenizer, showcasing how custom solutions can enhance flexibility and performance.  \n",
    "\n",
    "\n",
    "### **Loading Weights Efficiently**  \n",
    "Handling large-scale models comes with its challenges, but we tackled them with innovative techniques:  \n",
    "- **Weight Tying**: Reusing weights where possible to reduce redundancy and save memory.  \n",
    "- **Lower Precision Loading**: Boosting speed and reducing memory usage by leveraging lower-precision weights.  \n",
    "- **Sequential Loading**: Loading weights for large models in smaller, efficient chunks to optimize resources.  \n",
    "\n",
    "\n",
    "## **Why This Notebook Matters**  \n",
    "This notebook demonstrates the transition from **architectural understanding** to **practical mastery** of building state-of-the-art models like LLaMA. By experimenting with **innovative methods**, you‚Äôve pushed your boundaries and achieved a deeper grasp of transformers.  \n",
    "\n",
    "üéâ **Well done for completing Notebook 4.1!** Your hard work and dedication are paving the way to mastering advanced AI concepts. Here's to your continued success in building even more impressive models! üöÄ  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
