{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ2Tm6PKikIA"
   },
   "source": [
    "# üìö Notebook 6.3: Parameter-Efficient Fine-Tuning (PEFT) with LoRA and QLoRA - Finetuning gpt2(1.5B) on Alpaca(50k)\n",
    "\n",
    "Welcome back to the series! üéâ In *Notebook 6.2*, we explored foundational techniques for fine-tuning large language models, and now, it‚Äôs time to tackle an even more efficient approach. This notebook focuses on **Parameter-Efficient Fine-Tuning (PEFT)**, a method designed to fine-tune large models with minimal computational resources.\n",
    "\n",
    "### What's the Goal? üèÜ\n",
    "\n",
    "In this notebook, we‚Äôll dive into two PEFT techniques, **Low-Rank Adaptation (LoRA)** and **Quantized LoRA (QLoRA)**. These methods enable us to adapt large-scale models like GPT-2-XL for specific tasks with fewer trainable parameters and reduced memory usage, making fine-tuning accessible even on limited hardware.\n",
    "\n",
    "### What's Inside? üîç\n",
    "\n",
    "This notebook is structured to give you a practical and theoretical understanding of PEFT, along with a hands-on implementation. Here‚Äôs what we‚Äôll cover:\n",
    "\n",
    "#### **Section 1: Introduction to LoRA and QLoRA** üß†\n",
    "We‚Äôll explore the motivation and mechanics behind LoRA and QLoRA, including:\n",
    "- How LoRA introduces low-rank matrices to optimize memory usage\n",
    "- The role of quantization in QLoRA to further reduce resource needs\n",
    "\n",
    "#### **Section 2: Implementing LoRA from Scratch** üîÑ\n",
    "In this section, we‚Äôll go through:\n",
    "- Building a custom LoRA layer in PyTorch\n",
    "- Testing its integration within the GPT-2 architecture\n",
    "\n",
    "\n",
    "#### **Section 3: Fine-Tuning GPT-2-XL on the Alpaca Dataset** ü¶ô\n",
    "Finally, we‚Äôll put our implementation to the test by fine-tuning GPT-2-XL:\n",
    "- Preparing the Alpaca instruction-following dataset (50,000 entries)\n",
    "- Applying LoRA and QLoRA for fine-tuning and evaluating the results\n",
    "\n",
    "Let‚Äôs dive into this exploration of efficient fine-tuning, and see how LoRA and QLoRA can make LLM fine-tuning more accessible than ever! üåü\n",
    "\n",
    "# Introduction to PEFT\n",
    "\n",
    "Parameter-Efficient Fine-Tuning (PEFT) is a technique that reduces the computational cost of fine-tuning large language models. Instead of adjusting every weight in the model, PEFT allows us to tune only a small set of parameters, leveraging the knowledge already embedded in the model's layers. LoRA and QLoRA are two powerful PEFT methods, enabling us to train with fewer resources while maintaining performance.\n",
    "\n",
    "## Key Concepts in LoRA and QLoRA:\n",
    "\n",
    "- **Efficiency**: LoRA introduces low-rank matrices to reduce the number of trainable parameters.\n",
    "- **Quantization in QLoRA**: By using lower precision for certain weights, QLoRA further reduces memory needs without compromising much on accuracy.\n",
    "- **Application in NLP**: PEFT techniques are valuable for adapting models like GPT-2 for specific tasks, making fine-tuning feasible on consumer-grade hardware.\n",
    "\n",
    "By utilizing PEFT methods, we aim to efficiently train a GPT-2-XL model on the Alpaca dataset for instruction-following tasks, achieving significant memory savings and faster training.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/peft.jpg\" alt=\"PEFT Overview\" />\n",
    "</p>\n",
    "\n",
    "## LET'S GET STARTED üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mSMEqz4ikIG"
   },
   "source": [
    "## Section 1: LoRA - Low-Rank Adaptation for Large Language Models (qLoRA)\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "LoRA, or **Low-Rank Adaptation**, is a technique created to simplify and optimize the fine-tuning of large language models. It allows for flexible, resource-efficient fine-tuning, which reduces computational requirements significantly.\n",
    "\n",
    "### Why Use LoRA?\n",
    "\n",
    "Fine-tuning large models is often costly in terms of both time and resources. Additionally, the same base model may need to be fine-tuned multiple times for different tasks. LoRA offers an efficient solution by allowing multiple adapters to be plugged into a base model as needed, achieving high accuracy without the typical computational costs.\n",
    "\n",
    "The core insight behind LoRA is that many model parameters are redundant. By reducing redundancy, LoRA captures essential model information with fewer parameters, resulting in a much more efficient tuning process.\n",
    "\n",
    "### How Does LoRA Work?\n",
    "\n",
    "Rather than modifying the main weight matrix W  directly, LoRA introduces two smaller matrices,  W_a  and W_b , which approximate the adaptation needed. This setup significantly reduces the number of parameters without sacrificing model adaptability.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/lora_f.png\" alt=\"PEFT Overview\" />\n",
    "</p>\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider a weight matrix  W with dimensions  5000 x 1000 . The total number of parameters is:\n",
    "\n",
    "num_params = M * N = 5000 * 1000 = 5000000\n",
    "\n",
    "\n",
    "With LoRA, two smaller matrices are created:\n",
    "- Wa with dimensions M x r\n",
    "- W_b with dimensions  N x r\n",
    "\n",
    "where  r  is a small rank, often set to a low number such as 1. The number of parameters with LoRA is then:\n",
    "\n",
    "\n",
    "( M * r + N * r )\n",
    "\n",
    "\n",
    "For example, if  r = 1 :\n",
    "\n",
    "Wa = 5000 * 1 = 5000\n",
    "Wb = 1000 * 1 = 1000\n",
    "num_params = Wa + Wb = 6000\n",
    "\n",
    "\n",
    "### Fine-Tuning with LoRA: Backpropagation\n",
    "\n",
    "During fine-tuning, backpropagation is performed only on Wa and Wb, not on the original weight matrix W. This setup preserves the base model weights, reducing memory and computational costs since only the smaller LoRA matrices are updated during training.\n",
    "\n",
    "\n",
    "## qLoRA: Quantized Low-Rank Adaptation\n",
    "\n",
    "### What is qLoRA?\n",
    "\n",
    "qLoRA, or **Quantized Low-Rank Adaptation**, builds on LoRA by introducing quantization. Quantization compresses model weights into lower-precision values (e.g., from 32-bit to 8-bit), reducing memory use and speeding up computation further without losing significant accuracy.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/qlora.jpeg\" alt=\"PEFT Overview\" />\n",
    "</p>\n",
    "\n",
    "### Why Use qLoRA?\n",
    "\n",
    "While LoRA reduces parameters with low-rank matrices, qLoRA compresses these matrices even further by quantizing them. This combination results in a highly efficient, lightweight model adaptation process, ideal for deployment in resource-constrained environments.\n",
    "\n",
    "### How Does qLoRA Work?\n",
    "\n",
    "qLoRA applies quantization on the low-rank matrices Wa and Wb, using fewer bits to represent each parameter. This approach decreases the memory footprint and allows the model to run faster on hardware with limited capacity. Quantization also enables efficient scaling, especially useful for deploying large models on edge devices.\n",
    "\n",
    "\n",
    "## Mathematical Intuition Behind LoRA and qLoRA\n",
    "\n",
    "### Hypothesis: Redundancy and Low-Rank Approximation\n",
    "\n",
    "The idea behind LoRA and qLoRA is that many parameters in large models are redundant and can be approximated by a smaller set of parameters. This reduction is possible without sacrificing significant information in the model.\n",
    "\n",
    "### Singular Value Decomposition (SVD) and Low-Rank Approximation\n",
    "\n",
    "#### What is Singular Value Decomposition (SVD)?\n",
    "\n",
    "SVD decomposes a matrix \\( W \\) as:\n",
    "\n",
    "\\[\n",
    "W = U \\Sigma V^T\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( U \\) and \\( V \\) are orthogonal matrices.\n",
    "- \\( \\Sigma \\) is a diagonal matrix with singular values that reflect the importance of each component in \\( W \\).\n",
    "\n",
    "#### Linear Redundancy in Large Models\n",
    "\n",
    "In large models, many singular values of W  are near zero, indicating redundancy. This redundancy allows W to be approximated by focusing only on the largest singular values, reducing matrix complexity without significant information loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJmUF3fuikIH"
   },
   "source": [
    "## Section 2: Implementation from Scratch\n",
    "\n",
    "### Why Implement from Scratch?\n",
    "\n",
    "While there are countless out-of-the-box tools and libraries that make it easier to fine-tune large models and adapt them using techniques like LoRA and qLoRA, our approach has always been to understand everything from first principles. By building our solutions from scratch, we gain a deeper understanding of the underlying mechanics, which empowers us to optimize and customize the methods to our specific needs.\n",
    "\n",
    "In this section, we‚Äôll break down the process of implementing LoRA and qLoRA from scratch, including the mathematical foundations, the architecture, and the steps involved in training the model. Once we‚Äôve understood the implementation, we will integrate **qLoRA** and plug it into the LoRA model to test the results.\n",
    "\n",
    "## first lets start with SVD which lora build on  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isZIR66TikII",
    "outputId": "7438ac74-cd45-46cc-d91c-7fc8bed3cdd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix W:\n",
      " tensor([[-1.0328,  0.0464,  0.9360],\n",
      "        [-0.8173, -0.7201, -1.4533],\n",
      "        [-1.3988, -0.6507, -0.5185],\n",
      "        [-1.4505, -1.2822,  0.5440],\n",
      "        [-0.0512, -1.2063,  0.5130]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Create a random matrix W with 5 rows and 3 columns\n",
    "# This matrix represents the weight matrix in our SVD process\n",
    "W = torch.randn(5, 3)\n",
    "\n",
    "# Print the original matrix W\n",
    "# This matrix will be decomposed using Singular Value Decomposition\n",
    "print(\"Original Matrix W:\\n\", W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFWH9VPZikIM",
    "outputId": "560a0b21-7fa7-4e0b-97f6-3eb138a9ade7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U matrix:\n",
      " tensor([[-0.2597,  0.4954,  0.5552],\n",
      "        [-0.3932, -0.7234, -0.0544],\n",
      "        [-0.5251, -0.2361,  0.2857],\n",
      "        [-0.6586,  0.3152, -0.1077],\n",
      "        [-0.2616,  0.2760, -0.7717]])\n",
      "\n",
      "Singular values (S):\n",
      " tensor([2.9002, 1.9518, 1.1983])\n",
      "\n",
      "V transpose (Vh):\n",
      " tensor([[ 0.7905,  0.6113,  0.0373],\n",
      "        [-0.0315, -0.0202,  0.9993],\n",
      "        [-0.6116,  0.7912, -0.0033]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Perform Singular Value Decomposition (SVD)\n",
    "# torch.linalg.svd performs SVD on the matrix W\n",
    "# U, S, and Vh are the three components resulting from the SVD\n",
    "U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "\n",
    "# Print the resulting matrices\n",
    "# U contains the left singular vectors\n",
    "# S contains the singular values (a 1D tensor)\n",
    "# Vh is the transpose of V (the right singular vectors)\n",
    "print(\"\\nU matrix:\\n\", U)\n",
    "print(\"\\nSingular values (S):\\n\", S)\n",
    "print(\"\\nV transpose (Vh):\\n\", Vh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpDcqCdPikIN",
    "outputId": "92b27172-dc82-48c0-db08-6162be74cf13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstructed Matrix W (U * S * V^T):\n",
      " tensor([[-1.0328,  0.0464,  0.9360],\n",
      "        [-0.8173, -0.7201, -1.4533],\n",
      "        [-1.3988, -0.6507, -0.5185],\n",
      "        [-1.4505, -1.2822,  0.5440],\n",
      "        [-0.0512, -1.2063,  0.5130]])\n",
      "\n",
      "Reconstruction Close to Original: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Reconstruct the original matrix using the formula W = U * S * V^T\n",
    "# Since S is a 1D array, we need to convert it to a diagonal matrix\n",
    "S_matrix = torch.diag(S)\n",
    "\n",
    "# Reconstruct W by multiplying U, S, and Vh (the transpose of V)\n",
    "W_reconstructed = U @ S_matrix @ Vh\n",
    "\n",
    "# Step 4: Print the reconstructed matrix and check if it is close to the original matrix\n",
    "print(\"\\nReconstructed Matrix W (U * S * V^T):\\n\", W_reconstructed)\n",
    "\n",
    "# Check if the reconstruction is close to the original matrix\n",
    "print(\"\\nReconstruction Close to Original:\", torch.allclose(W, W_reconstructed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7FGjjTQikIO"
   },
   "source": [
    "## Concrete Example: Training on the MNIST Dataset\n",
    "\n",
    "In this section, we'll go through a practical example using the classic MNIST dataset:\n",
    "\n",
    "1. **Training the Model**:  \n",
    "   We‚Äôll train a model (intentionally larger than necessary) to simulate a more complex scenario and evaluate how well it performs.\n",
    "\n",
    "2. **Evaluating Model Performance**:  \n",
    "   After training, we'll evaluate the model's performance, focusing on how well it performs across each individual class (digit), and highlight any potential shortcomings.\n",
    "\n",
    "3. **Introducing LoRA**:  \n",
    "   Finally, we‚Äôll introduce Low-Rank Adaptation (LoRA) to see how it improves the model‚Äôs performance and efficiency compared to the original approach.\n",
    "\n",
    "By doing this, we aim to better understand the impact of LoRA in optimizing the model's results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Eoi4rvRikIO",
    "outputId": "29503ded-8eb4-4444-af8e-1c17547383b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:02<00:00, 4.55MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 134kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 4.29MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import  tqdm\n",
    "\n",
    "# Define transformations for the dataset\n",
    "# - ToTensor() converts the image into a tensor.\n",
    "# - Normalize() normalizes the image with the given mean and standard deviation values for MNIST.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the images into tensor format, which is needed by PyTorch\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalizes the tensor by the mean and standard deviation of MNIST\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "\n",
    "train_data = datasets.MNIST('mnist', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a DataLoader to load the training data in batches of 10 images at a time.\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the test dataset\n",
    "\n",
    "test_data = datasets.MNIST('mnist', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl4a3UvqikIP"
   },
   "outputs": [],
   "source": [
    "# we will use a larger hidden layer size here for the porpose of simulating a larger model with much num of parameters\n",
    "\n",
    "class Mnist(nn.Module):\n",
    "    def __init__(self,hidden_size1=1000, hidden_size2=2000):\n",
    "        super(Mnist, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1gScE8kikIP",
    "outputId": "635dd151-99ea-4966-e4dd-43e186769c2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:51<00:00, 117.43it/s, loss=0.237]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1, 28*28))\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "train(train_loader, model, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyXWp-q2ikIP"
   },
   "source": [
    "Lets save the model weights to compare after LoRa implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVxCvuRKikIQ"
   },
   "outputs": [],
   "source": [
    "og_weights = {}\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    og_weights[name] = param.clone().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBWpGuXmikIQ",
    "outputId": "78c5c3d7-e0e5-4380-af30-00e6be4f368e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03<00:00, 288.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.963\n",
      "wrong counts for the digit 0: 57\n",
      "wrong counts for the digit 1: 17\n",
      "wrong counts for the digit 2: 40\n",
      "wrong counts for the digit 3: 59\n",
      "wrong counts for the digit 4: 22\n",
      "wrong counts for the digit 5: 35\n",
      "wrong counts for the digit 6: 38\n",
      "wrong counts for the digit 7: 27\n",
      "wrong counts for the digit 8: 25\n",
      "wrong counts for the digit 9: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] +=1\n",
    "                total +=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l76VChZgikIQ"
   },
   "source": [
    "# Fine-Tuning MNIST Model with LoRA on Class 9\n",
    "\n",
    "We will fine-tune the MNIST model using **LoRA (Low-Rank Adaptation)** to improve performance specifically on **class 9** (digit 9). Instead of retraining the entire model, we will focus on adjusting the model's parameters related to class 9, while keeping the rest of the model frozen.\n",
    "\n",
    "lets visulize the network para,etrs before the implementaoion of LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkqMcYAmikIQ",
    "outputId": "a6546f62-64fb-45d7-b52c-31fdd56edfc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2807010\n",
      "Parameter fc1.weight: torch.Size([1000, 784])\n",
      "Parameter fc1.bias: torch.Size([1000])\n",
      "Parameter fc2.weight: torch.Size([2000, 1000])\n",
      "Parameter fc2.bias: torch.Size([2000])\n",
      "Parameter fc3.weight: torch.Size([10, 2000])\n",
      "Parameter fc3.bias: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name}: {param.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy6FFUE2ikIR"
   },
   "source": [
    "Now lets implement the LoRa paramerization according to the original papaer [ttps://arxiv.org/abs/2106.09685 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKAa5nyyikIR"
   },
   "outputs": [],
   "source": [
    "class LoRaParams(nn.Module):\n",
    "    def __init__(self, feat_in, feat_out, rank, alpha=1.0, device='cpu'):\n",
    "        super(LoRaParams, self).__init__()\n",
    "\n",
    "        # Low-rank decomposition matrices A and B\n",
    "        self.A = nn.Parameter(torch.zeros(rank, feat_out).to(device))  # (rank, feat_in)\n",
    "        self.B = nn.Parameter(torch.randn(feat_in,rank).to(device))  # (rank, feat_out)\n",
    "        self.scale = alpha / rank\n",
    "\n",
    "        self.lora = True\n",
    "\n",
    "    def forward(self, og_weights):\n",
    "        # Apply LoRA if enabled, otherwise return the original weights\n",
    "        if self.lora:\n",
    "            # Matrix multiplication for low-rank update: A * B.T\n",
    "            return og_weights + torch.matmul(self.B, self.A).view(og_weights.shape) * self.scale\n",
    "        else:\n",
    "            # Return original weights if LoRA is not enabled\n",
    "            return og_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGJZ2T2uikIR"
   },
   "source": [
    "### Injecting LoRA Parameterization into the Model using `torch.nn.utils.Parameterize`\n",
    "\n",
    "To integrate LoRA (Low-Rank Adaptation) parameterization into the model, we will use PyTorch's `torch.nn.utils.Parameterize` API. This allows us to modify the weights of specific layers in the model by applying low-rank updates, optimizing the model without retraining it from scratch.\n",
    "\n",
    "### Steps for LoRA Injection:\n",
    "\n",
    "1. **Identify Target Layers**: Determine which layers will benefit from low-rank adaptation. In this case, we apply LoRA to the weight matrices of certain layers.\n",
    "  \n",
    "2. **Apply LoRA Parameterization**: For each of the selected layers, we replace their weights with LoRA-parameterized weights during the forward pass.\n",
    "\n",
    "3. **Use `torch.nn.utils.Parameterize`**: The `torch.nn.utils.Parameterize` API ensures that LoRA matrices are treated as trainable parameters during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gx6WIPT9ikIR",
    "outputId": "6a32bc43-95b5-4a75-ca60-a0f5bddd2fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.parametrizations.weight.original: torch.Size([1000, 784])\n",
      "fc1.parametrizations.weight.0.A: torch.Size([1, 1000])\n",
      "fc1.parametrizations.weight.0.B: torch.Size([784, 1])\n",
      "fc2.parametrizations.weight.original: torch.Size([2000, 1000])\n",
      "fc2.parametrizations.weight.0.A: torch.Size([1, 2000])\n",
      "fc2.parametrizations.weight.0.B: torch.Size([1000, 1])\n",
      "fc3.parametrizations.weight.original: torch.Size([10, 2000])\n",
      "fc3.parametrizations.weight.0.A: torch.Size([1, 10])\n",
      "fc3.parametrizations.weight.0.B: torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import parametrize\n",
    "\n",
    "# Define the correct LoRaParametrization for each layer\n",
    "def linear_layer_parametrize(layer, rank=1, alpha=1, device=device):\n",
    "    feat_in = layer.in_features\n",
    "    feat_out = layer.out_features\n",
    "    lora_params = LoRaParams(feat_in, feat_out, rank=rank, alpha=alpha, device=device)\n",
    "    return lora_params\n",
    "\n",
    "# Register LoRA parametrizations\n",
    "parametrize.register_parametrization(model.fc1, 'weight', linear_layer_parametrize(model.fc1, device=device))\n",
    "parametrize.register_parametrization(model.fc2, 'weight', linear_layer_parametrize(model.fc2, device=device))\n",
    "parametrize.register_parametrization(model.fc3, 'weight', linear_layer_parametrize(model.fc3,  device=device))\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [model.fc1, model.fc2, model.fc3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy_35Q-jikIS"
   },
   "source": [
    "Lets count the total number of lora prametr5s of the total number of parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUzxh1zpikIS",
    "outputId": "0eea3f17-0926-494d-aed7-cbfb51b33908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2813804\n",
      "LoRA Parameters: 6794\n",
      "Percentage of LoRA Parameters: 0.24%\n"
     ]
    }
   ],
   "source": [
    "# Assuming the model is a simple neural network with LoRA applied\n",
    "def count_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "def count_lora_params(model):\n",
    "    lora_params = 0\n",
    "    # Iterate through the layers where LoRA is applied (e.g., fc1, fc2, fc3)\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if the parameter is a LoRA parameter (e.g., A or B)\n",
    "        if 'A' in name or 'B' in name:\n",
    "            lora_params += param.numel()\n",
    "    return lora_params\n",
    "\n",
    "\n",
    "# Calculate total number of parameters\n",
    "total_params = count_params(model)\n",
    "\n",
    "# Calculate total LoRA parameters (A and B matrices)\n",
    "lora_params = count_lora_params(model)\n",
    "\n",
    "# Calculate percentage of LoRA parameters\n",
    "lora_percentage = (lora_params / total_params) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"LoRA Parameters: {lora_params}\")\n",
    "print(f\"Percentage of LoRA Parameters: {lora_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZgMU_INikIS"
   },
   "source": [
    "As you can say the original model increase bt 6794 with the addiiton with loar paramets , now the total number of parametrs is 2813804 , but how a model hot incresed in size train much faser and with minum lost in accuracy ?\n",
    "\n",
    "### Why Does a Model Increase in Size with LoRA Parameters but Train Faster with Minimal Accuracy Loss?\n",
    "\n",
    "When adding **Low-Rank Adaptation (LoRA)** to a model, the overall number of parameters increases due to the introduction of additional low-rank matrices (`A` and `B`). However, the model can still train faster and with minimal loss in accuracy for the following reasons:\n",
    "\n",
    "1. **Efficient Use of Parameters**: LoRA introduces small, low-rank matrices that are much smaller in size than the full weight matrices. This allows the model to adapt efficiently without drastically increasing the number of parameters.\n",
    "\n",
    "2. **Frozen Pretrained Weights**: In many cases, LoRA freezes the original pretrained weights and only trains the low-rank matrices. This reduces the amount of work the optimizer needs to do, speeding up training.\n",
    "\n",
    "3. **Sparsity in Adaptations**: LoRA's low-rank matrices introduce sparsity, meaning fewer parameters need to be updated during training. This leads to faster convergence without sacrificing accuracy.\n",
    "\n",
    "4. **Minimal Impact on Accuracy**: LoRA's rank adaptations are carefully designed to ensure that the model maintains high accuracy. The added parameters are small and targeted, resulting in only a minimal impact on performance.\n",
    "\n",
    "5. **Faster Fine-Tuning**: By only modifying a small portion of the model, LoRA allows for quicker fine-tuning, reducing the time required for training while still achieving excellent performance.\n",
    "\n",
    "6. **Reduced Overfitting**: LoRA helps prevent overfitting by introducing just enough complexity to adapt to new tasks, without overburdening the model with unnecessary parameters.\n",
    "\n",
    "In conclusion, **LoRA** increases the model size only slightly but significantly speeds up training by focusing on low-rank adaptations. This approach maintains accuracy and reduces the time spent fine-tuning, making it an efficient technique for fast and effective model adaptation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ1RtoWhikIS",
    "outputId": "ba6257ff-2886-4047-bb70-2d0c6ee3f730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.bias: requires_grad=False\n",
      "fc1.parametrizations.weight.original: requires_grad=False\n",
      "fc1.parametrizations.weight.0.A: requires_grad=True\n",
      "fc1.parametrizations.weight.0.B: requires_grad=True\n",
      "fc2.bias: requires_grad=False\n",
      "fc2.parametrizations.weight.original: requires_grad=False\n",
      "fc2.parametrizations.weight.0.A: requires_grad=True\n",
      "fc2.parametrizations.weight.0.B: requires_grad=True\n",
      "fc3.bias: requires_grad=False\n",
      "fc3.parametrizations.weight.original: requires_grad=False\n",
      "fc3.parametrizations.weight.0.A: requires_grad=True\n",
      "fc3.parametrizations.weight.0.B: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Freeze all layers except LoRA layers\n",
    "def freeze_layers_except_lora(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Freeze layers that are not LoRA parameters (i.e., layers that do not contain 'A' or 'B' in the name)\n",
    "        if 'A' not in name and 'B' not in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            # Unfreeze LoRA layers (those with 'A' or 'B' in the name)\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Apply the freeze function to the model\n",
    "freeze_layers_except_lora(model)\n",
    "\n",
    "# Check if layers are frozen properly by printing the requires_grad status\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSZSPnSjikIT",
    "outputId": "27115546-2a34-4bf7-d160-0cd91d882c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:11<00:00, 899kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 134kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:06<00:00, 244kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 3.24MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:00<00:00, 127.56it/s, loss=0.0616]\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 9\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
    "train(train_loader, model, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYAPHlaqikIT",
    "outputId": "f54d3fd9-3adc-4551-b3a6-4c180830a6b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03<00:00, 265.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.767\n",
      "wrong counts for the digit 0: 460\n",
      "wrong counts for the digit 1: 150\n",
      "wrong counts for the digit 2: 106\n",
      "wrong counts for the digit 3: 205\n",
      "wrong counts for the digit 4: 481\n",
      "wrong counts for the digit 5: 318\n",
      "wrong counts for the digit 6: 73\n",
      "wrong counts for the digit 7: 335\n",
      "wrong counts for the digit 8: 202\n",
      "wrong counts for the digit 9: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHAKxnFGikIT"
   },
   "source": [
    "### Observing the Speed and Accuracy with LoRA Fine-Tuning\n",
    "\n",
    "Did you see how incredibly fast the training became? With LoRA (Low-Rank Adaptation), not only did we achieve lightning-fast fine-tuning, but we also managed to significantly improve the accuracy on the previously excluded class‚Äîdigit \"9.\" Moving from a high count of incorrect classifications to just a single misclassification, the improvement is clear.\n",
    "\n",
    "However, it‚Äôs worth noting that accuracy on other classes did decline. This drop occurs because the model reallocated its capacity to focus on the specific class we trained on (digit \"9\"), a trade-off common in fine-tuning. In this case, though, this reduction in accuracy on other classes is acceptable, given the simplicity of the dataset and our specific objective of enhancing performance for a single class.\n",
    "\n",
    "The takeaway? With just a fraction of the model's total weights being adjusted, LoRA allowed us to fine-tune efficiently, achieving remarkable results in both speed and performance for the targeted class! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99FDig1oikIU"
   },
   "source": [
    "## Adding Quantization top the Equation\n",
    "\n",
    "Quantization involves reducing the precision of the model‚Äôs weights and activations, typically from 32-bit floating point numbers (float32) to lower bit-width formats such as:\n",
    "\n",
    "- 6-bit (half precision)\n",
    "- 8-bit (int8)\n",
    "- 4-bit (int4)\n",
    "\n",
    "This can significantly reduce memory usage and computational overhead without sacrificing too much performance, especially when applied during fine-tuning.\n",
    "\n",
    "Here are the steps for implementing **QLoRA** (Quantized Low-Rank Adaptation) :\n",
    "\n",
    "## How the Quantization Works\n",
    "Here is the breakdown of the steps involved in quantizing a tensor:\n",
    "\n",
    "1- Determine the Maximum Value:\n",
    "\n",
    "We first calculate the maximum absolute value in the tensor. This will be used to scale the tensor values so that they fit within the range of the desired bit-width.\n",
    "Scaling:\n",
    "\n",
    "The scaling factor is determined based on the bit-width. For example, for an 8-bit quantization, the range is from -127 to 127 (since -2^(n-1) + 1 to 2^(n-1) - 1 for an n-bit signed integer). The scaling factor ensures the tensor's values fit within this range.\n",
    "The scaling factor is calculated as:\n",
    "\n",
    "scale = max¬†absolute¬†value¬†of¬†tensor(2(bit-width‚àí1)‚àí1)\n",
    "scale=\n",
    "(2\n",
    "(bit-width‚àí1)\n",
    " ‚àí1)\n",
    "max¬†absolute¬†value¬†of¬†tensor\n",
    "‚Äã\n",
    "\n",
    "Quantization:\n",
    "\n",
    "Each element of the tensor is divided by the scaling factor, rounded to the nearest integer, and then multiplied by the scaling factor again. This maps the floating-point values into integer values that fit the specified bit-width.\n",
    "Casting to Integer Type:\n",
    "\n",
    "Finally, the tensor is cast to an integer type (e.g., torch.int8 for 8-bit).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRGmZJbJABr_",
    "outputId": "bb456e46-5e79-4064-8360-62594e6e3061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 3.0999999046325684\n",
      "Scaling factor: 0.024409448727965355\n",
      "Quantized tensor (scaled and rounded): tensor([ 0.4882, -1.1961,  2.2945,  3.1000, -0.3906])\n",
      "Quantized tensor as int8: tensor([ 0, -1,  2,  3,  0], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create an example tensor (input tensor)\n",
    "tensor = torch.tensor([0.5, -1.2, 2.3, 3.1, -0.4], dtype=torch.float32)\n",
    "\n",
    "# Step 2: Find the maximum absolute value in the tensor\n",
    "# This is used to calculate the scaling factor for quantization\n",
    "max_val = tensor.abs().max()  # max_val = 3.1\n",
    "print(f\"Maximum value: {max_val}\")\n",
    "\n",
    "# Step 3: Compute the scale factor for quantization\n",
    "# For 8-bit quantization, the range is [-127, 127] (signed int8).\n",
    "# The scaling factor ensures that the max value in the tensor fits within the 8-bit range.\n",
    "scale = max_val / (2 ** (8 - 1) - 1)  # scale = 3.1 / 127 = 0.0244\n",
    "print(f\"Scaling factor: {scale}\")\n",
    "\n",
    "# Step 4: Perform quantization\n",
    "# 1. Divide the tensor by the scaling factor to scale it.\n",
    "# 2. Round the values to the nearest integer.\n",
    "# 3. Multiply by the scaling factor again to restore the range of the tensor.\n",
    "quantized_tensor = torch.round(tensor / scale) * scale\n",
    "print(f\"Quantized tensor (scaled and rounded): {quantized_tensor}\")\n",
    "\n",
    "# Step 5: Cast the tensor to int8 type for 8-bit quantization\n",
    "# This ensures the values are represented as 8-bit integers.\n",
    "quantized_tensor = quantized_tensor.to(torch.int8)\n",
    "print(f\"Quantized tensor as int8: {quantized_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2JOQChFdFic"
   },
   "source": [
    "# QLoRA Components\n",
    "\n",
    "## 1. 4-Bit Normal Flow\n",
    "In QLoRA, the typical low-rank approximation flow starts with the computation of the matrix product between two low-rank matrices, \\( A \\) and \\( B \\), to approximate the original weight matrix. After this approximation is obtained, quantization is applied to the resulting low-rank matrix. For 4-bit quantization, the values are scaled and rounded to fit within the range of \\( 0 \\) to \\( 15 \\) (i.e., the range for 4-bit values). This reduces the memory and computation overhead associated with full-precision matrices, making it efficient for training large models, especially in the context of LLMs (Large Language Models).\n",
    "\n",
    "### Process:\n",
    "- **Low-rank approximation**: \\( \\mathbf{A} \\times \\mathbf{B} \\)\n",
    "- **Quantization**: Values are scaled and clipped to fit within the range of 4-bit precision.\n",
    "  \n",
    "## 2. Double Quantization\n",
    "Double quantization refers to the process of applying two layers of quantization. The first layer quantizes the low-rank approximation matrix produced by \\( A \\times B \\), and the second layer applies a separate quantization mechanism to refine the matrix further. This can be used to compress the matrix to an even lower bit-depth, effectively reducing memory usage and speeding up computations. Double quantization is particularly useful in resource-constrained environments or when trying to push performance limits by further reducing model size.\n",
    "\n",
    "### Process:\n",
    "- **First Quantization**: Quantize the low-rank approximation matrix to a lower bit-width (e.g., 4-bit).\n",
    "- **Second Quantization**: Apply an additional quantization layer to the already quantized matrix to further reduce its precision.\n",
    "\n",
    "## 3. Paged Optimizer\n",
    "The paged optimizer is designed to efficiently manage and update large weight matrices during training. Instead of storing the entire matrix in memory, it breaks the matrix into smaller \"pages,\" or chunks, which are processed separately. This reduces the memory footprint during training, allowing for large-scale models to be trained on hardware with limited memory resources. By applying the paged optimizer, QLoRA can handle the massive size of weights in LLMs and distribute them across devices or memory layers effectively.\n",
    "\n",
    "### Process:\n",
    "- **Paging**: Split the matrix or parameters into manageable chunks (pages).\n",
    "- **Optimization**: Update each chunk independently to reduce memory load.\n",
    "  \n",
    "## 4. LoRA (Low-Rank Adaptation)\n",
    "LoRA (Low-Rank Adaptation) is a technique used to reduce the number of trainable parameters in large models, such as LLMs. By using a low-rank decomposition, the model approximates the weight matrix using two smaller matrices \\( A \\) and \\( B \\), where the rank defines the approximation complexity. The rank can be adjusted based on the desired trade-off between model performance and memory usage. LoRA helps in adapting large pre-trained models to new tasks by injecting low-rank matrices into the model, minimizing the additional computational overhead.\n",
    "\n",
    "### Process:\n",
    "- **Low-rank approximation**: The weight matrix is approximated as \\( \\mathbf{A} \\times \\mathbf{B} \\), where \\( \\mathbf{A} \\) and \\( \\mathbf{B} \\) are much smaller than the original matrix.\n",
    "- **Model adaptation**: By injecting the low-rank approximation into the original model's weights, you modify the behavior of the pre-trained model with minimal parameter updates.\n",
    "\n",
    "## lets now try to implemet this :\n",
    "## Lora + Quantization = QloRa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzkYrWA9eogc"
   },
   "outputs": [],
   "source": [
    "class QLoRaParams(nn.Module):\n",
    "    def __init__(self, feat_in, feat_out, rank, alpha=1.0, device='cpu', quantization_bit=4, lora=True):\n",
    "        super(QLoRaParams, self).__init__()\n",
    "\n",
    "        # Initialize low-rank matrices A and B\n",
    "        self.A = nn.Parameter(torch.zeros(rank, feat_out).to(device))  # (rank, feat_out)\n",
    "        self.B = nn.Parameter(torch.randn(feat_in, rank).to(device))  # (feat_in, rank)\n",
    "        self.scale = alpha / rank\n",
    "\n",
    "        self.lora = lora\n",
    "        self.quantization_bit = quantization_bit\n",
    "        self.device = device\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        \"\"\"\n",
    "        Quantize tensor to the specified bit-width.\n",
    "        \"\"\"\n",
    "        min_val, max_val = tensor.min(), tensor.max()\n",
    "        range_val = max_val - min_val + 1e-6  # Prevent division by zero\n",
    "\n",
    "        # Scale tensor values to the quantization range\n",
    "        scale = (2 ** self.quantization_bit - 1) / range_val\n",
    "        quantized_tensor = torch.round((tensor - min_val) * scale)\n",
    "\n",
    "        # Map back to the original range\n",
    "        return (quantized_tensor / scale) + min_val\n",
    "\n",
    "    def forward(self, og_weights):\n",
    "        \"\"\"\n",
    "        Apply LoRA with quantization and return updated weights.\n",
    "        \"\"\"\n",
    "        if self.lora:\n",
    "            # Quantize A and B matrices\n",
    "            quantized_A = self.quantize(self.A)\n",
    "            quantized_B = self.quantize(self.B)\n",
    "\n",
    "            # Compute low-rank update\n",
    "            low_rank_update = torch.matmul(quantized_B, quantized_A).view(og_weights.shape) * self.scale\n",
    "\n",
    "            # Quantize the low-rank update\n",
    "            quantized_low_rank = self.quantize(low_rank_update)\n",
    "\n",
    "            # Add quantized update to original weights\n",
    "            return og_weights + quantized_low_rank\n",
    "        else:\n",
    "            # If LoRA is disabled, return original weights\n",
    "            return og_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5bRB1vArTUK",
    "outputId": "ce26ad18-9ddf-40d2-cf94-e44df7241341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.parametrizations.weight.original: torch.Size([1000, 784])\n",
      "fc1.parametrizations.weight.0.A: torch.Size([1, 1000])\n",
      "fc1.parametrizations.weight.0.B: torch.Size([784, 1])\n",
      "fc1.parametrizations.weight.1.A: torch.Size([1, 1000])\n",
      "fc1.parametrizations.weight.1.B: torch.Size([784, 1])\n",
      "fc2.parametrizations.weight.original: torch.Size([2000, 1000])\n",
      "fc2.parametrizations.weight.0.A: torch.Size([1, 2000])\n",
      "fc2.parametrizations.weight.0.B: torch.Size([1000, 1])\n",
      "fc2.parametrizations.weight.1.A: torch.Size([1, 2000])\n",
      "fc2.parametrizations.weight.1.B: torch.Size([1000, 1])\n",
      "fc3.parametrizations.weight.original: torch.Size([10, 2000])\n",
      "fc3.parametrizations.weight.0.A: torch.Size([1, 10])\n",
      "fc3.parametrizations.weight.0.B: torch.Size([2000, 1])\n",
      "fc3.parametrizations.weight.1.A: torch.Size([1, 10])\n",
      "fc3.parametrizations.weight.1.B: torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import parametrize\n",
    "\n",
    "# Define the correct LoRaParametrization for each layer\n",
    "def linear_layer_parametrize(layer, rank=1, alpha=1, device=device):\n",
    "    feat_in = layer.in_features\n",
    "    feat_out = layer.out_features\n",
    "    lora_params = QLoRaParams(feat_in, feat_out, rank=rank, alpha=alpha, device=device)\n",
    "    return lora_params\n",
    "\n",
    "# Register LoRA parametrizations\n",
    "parametrize.register_parametrization(model.fc1, 'weight', linear_layer_parametrize(model.fc1, device=device))\n",
    "parametrize.register_parametrization(model.fc2, 'weight', linear_layer_parametrize(model.fc2, device=device))\n",
    "parametrize.register_parametrization(model.fc3, 'weight', linear_layer_parametrize(model.fc3,  device=device))\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=False):\n",
    "    for layer in [model.fc1, model.fc2, model.fc3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_6gRu05pivF",
    "outputId": "8a36f288-4648-4c26-82af-50975259c5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.bias: requires_grad=False\n",
      "fc1.parametrizations.weight.original: requires_grad=False\n",
      "fc1.parametrizations.weight.0.A: requires_grad=True\n",
      "fc1.parametrizations.weight.0.B: requires_grad=True\n",
      "fc1.parametrizations.weight.1.A: requires_grad=True\n",
      "fc1.parametrizations.weight.1.B: requires_grad=True\n",
      "fc2.bias: requires_grad=False\n",
      "fc2.parametrizations.weight.original: requires_grad=False\n",
      "fc2.parametrizations.weight.0.A: requires_grad=True\n",
      "fc2.parametrizations.weight.0.B: requires_grad=True\n",
      "fc2.parametrizations.weight.1.A: requires_grad=True\n",
      "fc2.parametrizations.weight.1.B: requires_grad=True\n",
      "fc3.bias: requires_grad=False\n",
      "fc3.parametrizations.weight.original: requires_grad=False\n",
      "fc3.parametrizations.weight.0.A: requires_grad=True\n",
      "fc3.parametrizations.weight.0.B: requires_grad=True\n",
      "fc3.parametrizations.weight.1.A: requires_grad=True\n",
      "fc3.parametrizations.weight.1.B: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Freeze all layers except QLoRA layers\n",
    "def freeze_layers_except_lora(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Freeze layers that are not LoRA parameters (i.e., layers that do not contain 'A' or 'B' in the name)\n",
    "        if 'A' not in name and 'B' not in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            # Unfreeze LoRA layers (those with 'A' or 'B' in the name)\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Apply the freeze function to the model\n",
    "freeze_layers_except_lora(model)\n",
    "\n",
    "# Check if layers are frozen properly by printing the requires_grad status\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMHh69oSruO5",
    "outputId": "8ad74a09-6c1b-4e6d-fb92-123ddb97f6b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2820598\n",
      "LoRA Parameters: 13588\n",
      "Percentage of LoRA Parameters: 0.48%\n"
     ]
    }
   ],
   "source": [
    "# Assuming the model is a simple neural network with LoRA applied\n",
    "def count_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "def count_lora_params(model):\n",
    "    lora_params = 0\n",
    "    # Iterate through the layers where LoRA is applied (e.g., fc1, fc2, fc3)\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if the parameter is a LoRA parameter (e.g., A or B)\n",
    "        if 'A' in name or 'B' in name:\n",
    "            lora_params += param.numel()\n",
    "    return lora_params\n",
    "\n",
    "\n",
    "# Calculate total number of parameters\n",
    "total_params = count_params(model)\n",
    "\n",
    "# Calculate total LoRA parameters (A and B matrices)\n",
    "lora_params = count_lora_params(model)\n",
    "\n",
    "# Calculate percentage of LoRA parameters\n",
    "lora_percentage = (lora_params / total_params) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"LoRA Parameters: {lora_params}\")\n",
    "print(f\"Percentage of LoRA Parameters: {lora_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQSwEViTD4Ar"
   },
   "source": [
    "## Let‚Äôs Recap Before Moving Forward  \n",
    "\n",
    "Up to this point, we‚Äôve introduced the concept of **LoRA** (Low-Rank Adaptation) and implemented it from scratch. We tested this implementation with a simple linear model on the MNIST dataset.  \n",
    "\n",
    "Next, we discussed **quantization** and integrated its implementation into the LoRA framework to create a quantized LoRA (qLoRA).  \n",
    "\n",
    "We briefly mentioned that qLoRA has four key components:  \n",
    "\n",
    "1. **4-bit Normal Float**  \n",
    "   - This is achieved by loading the model using the **BitsAndBytes** library, which allows us to load a quantized version of the model with 4-bit precision.  \n",
    "\n",
    "2. **Double Quantization**  \n",
    "   - Already implemented in our earlier steps.  \n",
    "\n",
    "3. *... (Steps for the third component go here, if applicable)*  \n",
    "\n",
    "4. **LoRA Application**  \n",
    "   - Completed in our earlier implementation.  \n",
    "\n",
    "\n",
    "### What's Missing?  \n",
    "\n",
    "At this point, the only remaining piece is the **PAGeS Optimizer**, which plays a critical role in optimizing the model during training. This will be the next step in our implementation journey!  \n",
    "\n",
    "Let‚Äôs dive in. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7TZmbZ7kEMv"
   },
   "source": [
    "#### **Section 3: Fine-Tuning GPT-2-XL on the Alpaca Dataset** ü¶ô  \n",
    "\n",
    "The **Alpaca dataset** is a curated collection designed to enhance instruction-following capabilities in language models. It provides examples of user prompts and corresponding responses, covering a wide range of instructions. This dataset helps fine-tune models to better understand and generate human-like responses to instructions, making them more effective and versatile in real-world applications.  \n",
    "\n",
    "To fine-tune GPT-2-XL on this dataset for instruction-following tasks, we will follow a systematic approach involving preprocessing, model quantization, and training. Below is the step-by-step flow of the project:  \n",
    "\n",
    "1. **Load and Preprocess the Dataset**  \n",
    "   - Format the dataset to match the input-output requirements of GPT-2-XL for instruction-following.  \n",
    "   - Create efficient data loaders for streamlined training.  \n",
    "\n",
    "2. **Load the Quantized Model**  \n",
    "   - Use **BitsAndBytes** to load a 4-bit quantized version of GPT-2-XL, which reduces memory usage and speeds up training.  \n",
    "\n",
    "3. **Inject the QLoRA Layer**  \n",
    "   - Add **QLoRA layers** into the GPT-2-XL architecture to enable parameter-efficient fine-tuning while preserving the original model's weights.  \n",
    "\n",
    "4. **Fine-Tune the Model**  \n",
    "   - Train the model on the Alpaca dataset with the injected QLoRA layers, utilizing the **PAGeS optimizer** to optimize the fine-tuning process efficiently.  \n",
    "\n",
    "By following this structured workflow, we can adapt GPT-2-XL to excel in instruction-following tasks while keeping computational overhead minimal. Let's get started! üöÄ\n",
    "\n",
    "## 1- Loading and Preparing the Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SnabGfhokOFm",
    "outputId": "2ed0f379-e9c8-4bf1-a947-ece102ce1bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 52002\n",
      "Entry to the list:\n",
      " [{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}, {'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.'}, {'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path to the local dataset file\n",
    "dataset_file = \"/content/alpaca_data.json\"\n",
    "\n",
    "# Load and inspect the data\n",
    "if os.path.isfile(dataset_file):\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(data))\n",
    "# Print an example entry from the list (entry at index 10)\n",
    "print(\"Entry to the list:\\n\", data[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0y-elhwGf5x"
   },
   "source": [
    "We will peform the same data prepartion as we did in the previous notebookk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A4h2nsyk8CW"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    # Create the instruction text using the instruction from the entry\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"  # Include the instruction in the formatted text\n",
    "    )\n",
    "\n",
    "    # Check if the input exists; if so, format it accordingly\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    # Combine the instruction text and input text (if any) and return the result\n",
    "    return instruction_text + input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVGIe3lapEYS",
    "outputId": "f9cd649e-13d8-4c74-ff17-55c722306a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Edit the following sentence to make it more concise.\n",
      "\n",
      "### Input:\n",
      "He ran to the bus stop in order to catch the bus that was due to arrive in five minutes.\n",
      "\n",
      "### Response:\n",
      "He ran to the bus stop, due to arrive in five minutes.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1BVi6q_pI0e",
    "outputId": "32cd365b-50a7-4174-fd30-4b16684c0d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 44201\n",
      "Validation data size: 2600\n",
      "Test data size: 5201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, temp_data = train_test_split(data, test_size=0.15, random_state=42)  # 85% train, 15% temp\n",
    "\n",
    "# Split temp data into 5% validation and 10% test\n",
    "val_data, test_data = train_test_split(temp_data, test_size=(10/15), random_state=42)  # 5% val, 10% test\n",
    "\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1E2KdKApQu7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        # Initialize the dataset with data and a tokenizer\n",
    "        self.data = data\n",
    "        self.encode_data = []\n",
    "\n",
    "        # Loop through each entry in the dataset\n",
    "        for entry in data:\n",
    "            # Format the input entry into a structured text\n",
    "            instruction_text = format_input(entry) ## using the function defined above\n",
    "            # Prepare the desired response format\n",
    "            desired_response = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            # Combine instruction and response into full text\n",
    "            full_text = instruction_text + desired_response\n",
    "            # Encode the full text using the provided tokenizer and store it\n",
    "            self.encode_data.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encode_data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5PnmJUKpWzw",
    "outputId": "a84dd4ab-bae0-4253-cfa8-ff0dad7e076f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special = {\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzN0jJ75pcmj"
   },
   "outputs": [],
   "source": [
    "def collate_fn3(batch, pad_token_id=50256, ignore_index=-100, allowed_max_len=None, device=\"cpu\"):\n",
    "    # Determine the maximum length of the sequences in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    input_ls, target_ls = [], []\n",
    "\n",
    "    # Iterate through each item in the batch\n",
    "    for item in batch:\n",
    "        # Create a copy of the item and append the padding token\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        # Pad the item to the maximum length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "\n",
    "        # Separate inputs and targets; inputs exclude the last token, targets exclude the first token\n",
    "        inputs = torch.tensor(padded[:-1])  # Input tensor\n",
    "        target = torch.tensor(padded[1:])   # Target tensor\n",
    "\n",
    "        # Create a mask for the padding tokens in the target tensor\n",
    "        mask = target == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()  # Get indices of padding tokens\n",
    "\n",
    "        # If there are multiple padding tokens, ignore the subsequent ones in the loss calculation\n",
    "        if indices.numel() > 1:\n",
    "            target[indices[1:]] = ignore_index  # Set ignore index for padding\n",
    "\n",
    "        # Optionally limit the length of inputs and targets\n",
    "        if allowed_max_len is not None:\n",
    "            inputs = inputs[:allowed_max_len]\n",
    "            target = target[:allowed_max_len]\n",
    "\n",
    "        # Append the processed inputs and targets to the respective lists\n",
    "        input_ls.append(inputs)\n",
    "        target_ls.append(target)\n",
    "\n",
    "    # Stack the lists into tensors and move them to the specified device\n",
    "    input_tensor = torch.stack(input_ls).to(device)\n",
    "    target_tensor = torch.stack(target_ls).to(device)\n",
    "\n",
    "    return input_tensor, target_tensor  # Return the batched input and target tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "es-x5Qy-ph2M",
    "outputId": "1ad7329a-5452-49a1-d2f4-7eb298b4e505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader: 44201\n",
      "Validation dataloader: 2600\n",
      "Test dataloader: 5201\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "custom_collate_fn = partial(collate_fn3,\n",
    "                            device=device,\n",
    "                            allowed_max_len=1024)\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "print(\"Train dataloader:\", len(train_dataloader))\n",
    "print(\"Validation dataloader:\", len(val_dataloader))\n",
    "print(\"Test dataloader:\", len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3fymUSdYrA0",
    "outputId": "b0403a1f-a914-44ab-dcca-9002e0eb696c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleaned up.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we working with a limited RAM make sure to clean up the memory from time to time\n",
    "# Clean up temporary variables and free memory\n",
    "import gc\n",
    "\n",
    "# Remove temp_data (no longer needed after splitting)\n",
    "del temp_data\n",
    "\n",
    "# Clear any large unused variables\n",
    "del model_input, desired_response  # Output used for testing only\n",
    "\n",
    "# Delete the data after the split to free up memory\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# You can also delete any other intermediate variables if needed:\n",
    "# del train_data, val_data, test_data\n",
    "# gc.collect()\n",
    "\n",
    "# Optionally, after creating DataLoader objects, delete unnecessary intermediate variables\n",
    "del train_dataset, val_dataset, test_dataset\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleaned up.\")\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear GPU memory\n",
    "gc.collect()  # Run garbage collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A80Y5ATmG06L"
   },
   "source": [
    "## 2- Loading the Quantized Model  \n",
    "\n",
    "To fine-tune large language models efficiently, it's crucial to manage memory and computational costs. This is where **BitsAndBytes** comes into play.  \n",
    "\n",
    "### **Introduction to BitsAndBytes**  \n",
    "**BitsAndBytes** is a library designed to enable efficient training and inference of large-scale language models by using low-bit precision. Instead of relying on traditional 16-bit or 32-bit floating-point formats, BitsAndBytes supports **4-bit and 8-bit quantization**, significantly reducing memory usage without compromising model performance.  \n",
    "\n",
    "Some key features of BitsAndBytes include:  \n",
    "- **Quantized Model Loading**: Load pre-trained models in quantized formats (e.g., 4-bit) to save memory and increase speed.  \n",
    "- **Compatibility**: Integrates seamlessly with popular frameworks like Hugging Face Transformers.  \n",
    "- **Fine-Tuning Ready**: Ideal for adapting large language models using efficient fine-tuning methods like **QLoRA**.  \n",
    "\n",
    "### **Why Use BitsAndBytes?**  \n",
    "Quantization with BitsAndBytes allows us to:  \n",
    "1. **Reduce Memory Footprint**: Handle large models like GPT-2-XL on smaller hardware setups.  \n",
    "2. **Boost Training Efficiency**: Train faster due to reduced computation requirements.  \n",
    "3. **Retain Performance**: Minimize the trade-off between accuracy and efficiency.  \n",
    "\n",
    "In this section, we'll leverage BitsAndBytes to load a **4-bit quantized version** of GPT-2-XL, preparing it for the integration of QLoRA layers and fine-tuning. Let's dive into the process! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4-UaJOQHPOk",
    "outputId": "492a9a57-85d4-4c33-f76c-66199d3599e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXDwLQrqHpMq",
    "outputId": "1e270373-0b3e-416f-c6db-8ebf2d04c245"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM , BitsAndBytesConfig , AutoTokenizer\n",
    "\n",
    "model_id = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n",
    "\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJ2nbNLrK9wr"
   },
   "source": [
    "### **Understanding the BitsAndBytesConfig**\n",
    "\n",
    "The `BitsAndBytesConfig` defines how the model is quantized when it is loaded. Here's a breakdown of the key components:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Quantization Parameters**\n",
    "1. **`load_in_4bit=True`**  \n",
    "   - Specifies that the model weights should be loaded in **4-bit precision**.  \n",
    "   - **Benefit**: Significantly reduces memory requirements compared to the usual 16-bit or 32-bit precision.\n",
    "\n",
    "2. **`bnb_4bit_quant_type=\"nf4\"`**  \n",
    "   - Indicates the quantization type.  \n",
    "   - **NF4 (Normal Float 4)**:  \n",
    "     - An advanced quantization scheme that retains more dynamic range compared to uniform quantization.  \n",
    "     - Improves accuracy for low-precision models.\n",
    "\n",
    "3. **`bnb_4bit_use_double_quant=True`**  \n",
    "   - Enables **double quantization**, which involves:  \n",
    "     1. Quantizing weights initially.  \n",
    "     2. Applying a secondary quantization.  \n",
    "   - **Benefit**: Further reduces memory usage while minimizing numerical degradation.\n",
    "\n",
    "4. **`bnb_4bit_compute_dtype=torch.bfloat16`**  \n",
    "   - Sets the compute precision to **bfloat16 (Brain Floating Point 16-bit)**.  \n",
    "   - **Why bfloat16?**  \n",
    "     - Balances precision and memory efficiency.  \n",
    "     - Optimized for modern accelerators like GPUs and TPUs.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Under the Hood: Benefits of This Configuration**\n",
    "1. **Memory Efficiency**:  \n",
    "   - Reducing weights to **4 bits** drastically decreases memory usage compared to standard **16/32-bit weights**.\n",
    "\n",
    "2. **Speed**:  \n",
    "   - Smaller weights mean faster data transfers between GPU/CPU and memory.\n",
    "\n",
    "3. **Accuracy**:  \n",
    "   - Using **NF4 quantization** retains higher accuracy compared to simpler schemes like uniform quantization.\n",
    "\n",
    "4. **Compatibility**:  \n",
    "   - Leveraging **bfloat16** for computations ensures compatibility with modern GPUs while minimizing numerical instability.\n",
    "\n",
    "---\n",
    "\n",
    "This configuration allows large language models like **GPT-2-XL** to be fine-tuned or used for inference efficiently, even on **consumer-grade hardware**.\n",
    "\n",
    "## now lets test our loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGnaGllIL0C5",
    "outputId": "8c89ff9e-6a12-4bc0-9de8-f441f65604bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "Model configuration: GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"gpt2-xl\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1600,\n",
      "  \"n_head\": 25,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 48,\n",
      "  \"n_positions\": 1024,\n",
      "  \"output_past\": true,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "\n",
      "Model parameter data types:\n",
      "transformer.wte.weight: torch.float16\n",
      "transformer.wpe.weight: torch.float16\n",
      "transformer.h.0.ln_1.weight: torch.float16\n",
      "transformer.h.0.ln_1.bias: torch.float16\n",
      "transformer.h.0.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.0.attn.c_attn.bias: torch.float16\n",
      "transformer.h.0.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.0.attn.c_proj.bias: torch.float16\n",
      "transformer.h.0.ln_2.weight: torch.float16\n",
      "transformer.h.0.ln_2.bias: torch.float16\n",
      "transformer.h.0.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.0.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.0.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.0.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.1.ln_1.weight: torch.float16\n",
      "transformer.h.1.ln_1.bias: torch.float16\n",
      "transformer.h.1.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.1.attn.c_attn.bias: torch.float16\n",
      "transformer.h.1.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.1.attn.c_proj.bias: torch.float16\n",
      "transformer.h.1.ln_2.weight: torch.float16\n",
      "transformer.h.1.ln_2.bias: torch.float16\n",
      "transformer.h.1.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.1.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.1.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.1.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.2.ln_1.weight: torch.float16\n",
      "transformer.h.2.ln_1.bias: torch.float16\n",
      "transformer.h.2.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.2.attn.c_attn.bias: torch.float16\n",
      "transformer.h.2.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.2.attn.c_proj.bias: torch.float16\n",
      "transformer.h.2.ln_2.weight: torch.float16\n",
      "transformer.h.2.ln_2.bias: torch.float16\n",
      "transformer.h.2.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.2.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.2.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.2.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.3.ln_1.weight: torch.float16\n",
      "transformer.h.3.ln_1.bias: torch.float16\n",
      "transformer.h.3.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.3.attn.c_attn.bias: torch.float16\n",
      "transformer.h.3.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.3.attn.c_proj.bias: torch.float16\n",
      "transformer.h.3.ln_2.weight: torch.float16\n",
      "transformer.h.3.ln_2.bias: torch.float16\n",
      "transformer.h.3.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.3.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.3.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.3.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.4.ln_1.weight: torch.float16\n",
      "transformer.h.4.ln_1.bias: torch.float16\n",
      "transformer.h.4.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.4.attn.c_attn.bias: torch.float16\n",
      "transformer.h.4.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.4.attn.c_proj.bias: torch.float16\n",
      "transformer.h.4.ln_2.weight: torch.float16\n",
      "transformer.h.4.ln_2.bias: torch.float16\n",
      "transformer.h.4.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.4.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.4.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.4.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.5.ln_1.weight: torch.float16\n",
      "transformer.h.5.ln_1.bias: torch.float16\n",
      "transformer.h.5.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.5.attn.c_attn.bias: torch.float16\n",
      "transformer.h.5.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.5.attn.c_proj.bias: torch.float16\n",
      "transformer.h.5.ln_2.weight: torch.float16\n",
      "transformer.h.5.ln_2.bias: torch.float16\n",
      "transformer.h.5.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.5.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.5.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.5.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.6.ln_1.weight: torch.float16\n",
      "transformer.h.6.ln_1.bias: torch.float16\n",
      "transformer.h.6.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.6.attn.c_attn.bias: torch.float16\n",
      "transformer.h.6.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.6.attn.c_proj.bias: torch.float16\n",
      "transformer.h.6.ln_2.weight: torch.float16\n",
      "transformer.h.6.ln_2.bias: torch.float16\n",
      "transformer.h.6.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.6.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.6.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.6.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.7.ln_1.weight: torch.float16\n",
      "transformer.h.7.ln_1.bias: torch.float16\n",
      "transformer.h.7.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.7.attn.c_attn.bias: torch.float16\n",
      "transformer.h.7.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.7.attn.c_proj.bias: torch.float16\n",
      "transformer.h.7.ln_2.weight: torch.float16\n",
      "transformer.h.7.ln_2.bias: torch.float16\n",
      "transformer.h.7.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.7.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.7.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.7.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.8.ln_1.weight: torch.float16\n",
      "transformer.h.8.ln_1.bias: torch.float16\n",
      "transformer.h.8.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.8.attn.c_attn.bias: torch.float16\n",
      "transformer.h.8.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.8.attn.c_proj.bias: torch.float16\n",
      "transformer.h.8.ln_2.weight: torch.float16\n",
      "transformer.h.8.ln_2.bias: torch.float16\n",
      "transformer.h.8.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.8.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.8.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.8.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.9.ln_1.weight: torch.float16\n",
      "transformer.h.9.ln_1.bias: torch.float16\n",
      "transformer.h.9.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.9.attn.c_attn.bias: torch.float16\n",
      "transformer.h.9.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.9.attn.c_proj.bias: torch.float16\n",
      "transformer.h.9.ln_2.weight: torch.float16\n",
      "transformer.h.9.ln_2.bias: torch.float16\n",
      "transformer.h.9.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.9.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.9.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.9.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.10.ln_1.weight: torch.float16\n",
      "transformer.h.10.ln_1.bias: torch.float16\n",
      "transformer.h.10.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.10.attn.c_attn.bias: torch.float16\n",
      "transformer.h.10.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.10.attn.c_proj.bias: torch.float16\n",
      "transformer.h.10.ln_2.weight: torch.float16\n",
      "transformer.h.10.ln_2.bias: torch.float16\n",
      "transformer.h.10.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.10.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.10.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.10.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.11.ln_1.weight: torch.float16\n",
      "transformer.h.11.ln_1.bias: torch.float16\n",
      "transformer.h.11.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.11.attn.c_attn.bias: torch.float16\n",
      "transformer.h.11.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.11.attn.c_proj.bias: torch.float16\n",
      "transformer.h.11.ln_2.weight: torch.float16\n",
      "transformer.h.11.ln_2.bias: torch.float16\n",
      "transformer.h.11.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.11.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.11.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.11.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.12.ln_1.weight: torch.float16\n",
      "transformer.h.12.ln_1.bias: torch.float16\n",
      "transformer.h.12.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.12.attn.c_attn.bias: torch.float16\n",
      "transformer.h.12.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.12.attn.c_proj.bias: torch.float16\n",
      "transformer.h.12.ln_2.weight: torch.float16\n",
      "transformer.h.12.ln_2.bias: torch.float16\n",
      "transformer.h.12.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.12.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.12.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.12.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.13.ln_1.weight: torch.float16\n",
      "transformer.h.13.ln_1.bias: torch.float16\n",
      "transformer.h.13.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.13.attn.c_attn.bias: torch.float16\n",
      "transformer.h.13.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.13.attn.c_proj.bias: torch.float16\n",
      "transformer.h.13.ln_2.weight: torch.float16\n",
      "transformer.h.13.ln_2.bias: torch.float16\n",
      "transformer.h.13.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.13.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.13.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.13.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.14.ln_1.weight: torch.float16\n",
      "transformer.h.14.ln_1.bias: torch.float16\n",
      "transformer.h.14.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.14.attn.c_attn.bias: torch.float16\n",
      "transformer.h.14.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.14.attn.c_proj.bias: torch.float16\n",
      "transformer.h.14.ln_2.weight: torch.float16\n",
      "transformer.h.14.ln_2.bias: torch.float16\n",
      "transformer.h.14.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.14.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.14.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.14.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.15.ln_1.weight: torch.float16\n",
      "transformer.h.15.ln_1.bias: torch.float16\n",
      "transformer.h.15.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.15.attn.c_attn.bias: torch.float16\n",
      "transformer.h.15.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.15.attn.c_proj.bias: torch.float16\n",
      "transformer.h.15.ln_2.weight: torch.float16\n",
      "transformer.h.15.ln_2.bias: torch.float16\n",
      "transformer.h.15.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.15.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.15.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.15.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.16.ln_1.weight: torch.float16\n",
      "transformer.h.16.ln_1.bias: torch.float16\n",
      "transformer.h.16.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.16.attn.c_attn.bias: torch.float16\n",
      "transformer.h.16.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.16.attn.c_proj.bias: torch.float16\n",
      "transformer.h.16.ln_2.weight: torch.float16\n",
      "transformer.h.16.ln_2.bias: torch.float16\n",
      "transformer.h.16.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.16.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.16.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.16.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.17.ln_1.weight: torch.float16\n",
      "transformer.h.17.ln_1.bias: torch.float16\n",
      "transformer.h.17.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.17.attn.c_attn.bias: torch.float16\n",
      "transformer.h.17.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.17.attn.c_proj.bias: torch.float16\n",
      "transformer.h.17.ln_2.weight: torch.float16\n",
      "transformer.h.17.ln_2.bias: torch.float16\n",
      "transformer.h.17.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.17.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.17.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.17.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.18.ln_1.weight: torch.float16\n",
      "transformer.h.18.ln_1.bias: torch.float16\n",
      "transformer.h.18.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.18.attn.c_attn.bias: torch.float16\n",
      "transformer.h.18.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.18.attn.c_proj.bias: torch.float16\n",
      "transformer.h.18.ln_2.weight: torch.float16\n",
      "transformer.h.18.ln_2.bias: torch.float16\n",
      "transformer.h.18.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.18.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.18.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.18.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.19.ln_1.weight: torch.float16\n",
      "transformer.h.19.ln_1.bias: torch.float16\n",
      "transformer.h.19.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.19.attn.c_attn.bias: torch.float16\n",
      "transformer.h.19.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.19.attn.c_proj.bias: torch.float16\n",
      "transformer.h.19.ln_2.weight: torch.float16\n",
      "transformer.h.19.ln_2.bias: torch.float16\n",
      "transformer.h.19.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.19.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.19.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.19.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.20.ln_1.weight: torch.float16\n",
      "transformer.h.20.ln_1.bias: torch.float16\n",
      "transformer.h.20.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.20.attn.c_attn.bias: torch.float16\n",
      "transformer.h.20.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.20.attn.c_proj.bias: torch.float16\n",
      "transformer.h.20.ln_2.weight: torch.float16\n",
      "transformer.h.20.ln_2.bias: torch.float16\n",
      "transformer.h.20.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.20.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.20.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.20.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.21.ln_1.weight: torch.float16\n",
      "transformer.h.21.ln_1.bias: torch.float16\n",
      "transformer.h.21.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.21.attn.c_attn.bias: torch.float16\n",
      "transformer.h.21.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.21.attn.c_proj.bias: torch.float16\n",
      "transformer.h.21.ln_2.weight: torch.float16\n",
      "transformer.h.21.ln_2.bias: torch.float16\n",
      "transformer.h.21.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.21.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.21.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.21.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.22.ln_1.weight: torch.float16\n",
      "transformer.h.22.ln_1.bias: torch.float16\n",
      "transformer.h.22.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.22.attn.c_attn.bias: torch.float16\n",
      "transformer.h.22.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.22.attn.c_proj.bias: torch.float16\n",
      "transformer.h.22.ln_2.weight: torch.float16\n",
      "transformer.h.22.ln_2.bias: torch.float16\n",
      "transformer.h.22.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.22.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.22.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.22.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.23.ln_1.weight: torch.float16\n",
      "transformer.h.23.ln_1.bias: torch.float16\n",
      "transformer.h.23.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.23.attn.c_attn.bias: torch.float16\n",
      "transformer.h.23.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.23.attn.c_proj.bias: torch.float16\n",
      "transformer.h.23.ln_2.weight: torch.float16\n",
      "transformer.h.23.ln_2.bias: torch.float16\n",
      "transformer.h.23.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.23.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.23.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.23.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.24.ln_1.weight: torch.float16\n",
      "transformer.h.24.ln_1.bias: torch.float16\n",
      "transformer.h.24.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.24.attn.c_attn.bias: torch.float16\n",
      "transformer.h.24.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.24.attn.c_proj.bias: torch.float16\n",
      "transformer.h.24.ln_2.weight: torch.float16\n",
      "transformer.h.24.ln_2.bias: torch.float16\n",
      "transformer.h.24.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.24.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.24.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.24.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.25.ln_1.weight: torch.float16\n",
      "transformer.h.25.ln_1.bias: torch.float16\n",
      "transformer.h.25.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.25.attn.c_attn.bias: torch.float16\n",
      "transformer.h.25.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.25.attn.c_proj.bias: torch.float16\n",
      "transformer.h.25.ln_2.weight: torch.float16\n",
      "transformer.h.25.ln_2.bias: torch.float16\n",
      "transformer.h.25.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.25.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.25.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.25.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.26.ln_1.weight: torch.float16\n",
      "transformer.h.26.ln_1.bias: torch.float16\n",
      "transformer.h.26.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.26.attn.c_attn.bias: torch.float16\n",
      "transformer.h.26.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.26.attn.c_proj.bias: torch.float16\n",
      "transformer.h.26.ln_2.weight: torch.float16\n",
      "transformer.h.26.ln_2.bias: torch.float16\n",
      "transformer.h.26.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.26.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.26.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.26.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.27.ln_1.weight: torch.float16\n",
      "transformer.h.27.ln_1.bias: torch.float16\n",
      "transformer.h.27.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.27.attn.c_attn.bias: torch.float16\n",
      "transformer.h.27.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.27.attn.c_proj.bias: torch.float16\n",
      "transformer.h.27.ln_2.weight: torch.float16\n",
      "transformer.h.27.ln_2.bias: torch.float16\n",
      "transformer.h.27.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.27.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.27.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.27.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.28.ln_1.weight: torch.float16\n",
      "transformer.h.28.ln_1.bias: torch.float16\n",
      "transformer.h.28.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.28.attn.c_attn.bias: torch.float16\n",
      "transformer.h.28.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.28.attn.c_proj.bias: torch.float16\n",
      "transformer.h.28.ln_2.weight: torch.float16\n",
      "transformer.h.28.ln_2.bias: torch.float16\n",
      "transformer.h.28.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.28.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.28.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.28.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.29.ln_1.weight: torch.float16\n",
      "transformer.h.29.ln_1.bias: torch.float16\n",
      "transformer.h.29.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.29.attn.c_attn.bias: torch.float16\n",
      "transformer.h.29.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.29.attn.c_proj.bias: torch.float16\n",
      "transformer.h.29.ln_2.weight: torch.float16\n",
      "transformer.h.29.ln_2.bias: torch.float16\n",
      "transformer.h.29.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.29.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.29.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.29.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.30.ln_1.weight: torch.float16\n",
      "transformer.h.30.ln_1.bias: torch.float16\n",
      "transformer.h.30.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.30.attn.c_attn.bias: torch.float16\n",
      "transformer.h.30.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.30.attn.c_proj.bias: torch.float16\n",
      "transformer.h.30.ln_2.weight: torch.float16\n",
      "transformer.h.30.ln_2.bias: torch.float16\n",
      "transformer.h.30.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.30.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.30.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.30.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.31.ln_1.weight: torch.float16\n",
      "transformer.h.31.ln_1.bias: torch.float16\n",
      "transformer.h.31.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.31.attn.c_attn.bias: torch.float16\n",
      "transformer.h.31.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.31.attn.c_proj.bias: torch.float16\n",
      "transformer.h.31.ln_2.weight: torch.float16\n",
      "transformer.h.31.ln_2.bias: torch.float16\n",
      "transformer.h.31.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.31.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.31.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.31.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.32.ln_1.weight: torch.float16\n",
      "transformer.h.32.ln_1.bias: torch.float16\n",
      "transformer.h.32.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.32.attn.c_attn.bias: torch.float16\n",
      "transformer.h.32.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.32.attn.c_proj.bias: torch.float16\n",
      "transformer.h.32.ln_2.weight: torch.float16\n",
      "transformer.h.32.ln_2.bias: torch.float16\n",
      "transformer.h.32.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.32.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.32.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.32.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.33.ln_1.weight: torch.float16\n",
      "transformer.h.33.ln_1.bias: torch.float16\n",
      "transformer.h.33.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.33.attn.c_attn.bias: torch.float16\n",
      "transformer.h.33.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.33.attn.c_proj.bias: torch.float16\n",
      "transformer.h.33.ln_2.weight: torch.float16\n",
      "transformer.h.33.ln_2.bias: torch.float16\n",
      "transformer.h.33.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.33.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.33.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.33.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.34.ln_1.weight: torch.float16\n",
      "transformer.h.34.ln_1.bias: torch.float16\n",
      "transformer.h.34.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.34.attn.c_attn.bias: torch.float16\n",
      "transformer.h.34.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.34.attn.c_proj.bias: torch.float16\n",
      "transformer.h.34.ln_2.weight: torch.float16\n",
      "transformer.h.34.ln_2.bias: torch.float16\n",
      "transformer.h.34.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.34.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.34.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.34.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.35.ln_1.weight: torch.float16\n",
      "transformer.h.35.ln_1.bias: torch.float16\n",
      "transformer.h.35.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.35.attn.c_attn.bias: torch.float16\n",
      "transformer.h.35.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.35.attn.c_proj.bias: torch.float16\n",
      "transformer.h.35.ln_2.weight: torch.float16\n",
      "transformer.h.35.ln_2.bias: torch.float16\n",
      "transformer.h.35.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.35.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.35.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.35.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.36.ln_1.weight: torch.float16\n",
      "transformer.h.36.ln_1.bias: torch.float16\n",
      "transformer.h.36.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.36.attn.c_attn.bias: torch.float16\n",
      "transformer.h.36.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.36.attn.c_proj.bias: torch.float16\n",
      "transformer.h.36.ln_2.weight: torch.float16\n",
      "transformer.h.36.ln_2.bias: torch.float16\n",
      "transformer.h.36.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.36.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.36.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.36.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.37.ln_1.weight: torch.float16\n",
      "transformer.h.37.ln_1.bias: torch.float16\n",
      "transformer.h.37.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.37.attn.c_attn.bias: torch.float16\n",
      "transformer.h.37.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.37.attn.c_proj.bias: torch.float16\n",
      "transformer.h.37.ln_2.weight: torch.float16\n",
      "transformer.h.37.ln_2.bias: torch.float16\n",
      "transformer.h.37.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.37.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.37.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.37.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.38.ln_1.weight: torch.float16\n",
      "transformer.h.38.ln_1.bias: torch.float16\n",
      "transformer.h.38.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.38.attn.c_attn.bias: torch.float16\n",
      "transformer.h.38.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.38.attn.c_proj.bias: torch.float16\n",
      "transformer.h.38.ln_2.weight: torch.float16\n",
      "transformer.h.38.ln_2.bias: torch.float16\n",
      "transformer.h.38.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.38.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.38.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.38.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.39.ln_1.weight: torch.float16\n",
      "transformer.h.39.ln_1.bias: torch.float16\n",
      "transformer.h.39.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.39.attn.c_attn.bias: torch.float16\n",
      "transformer.h.39.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.39.attn.c_proj.bias: torch.float16\n",
      "transformer.h.39.ln_2.weight: torch.float16\n",
      "transformer.h.39.ln_2.bias: torch.float16\n",
      "transformer.h.39.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.39.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.39.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.39.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.40.ln_1.weight: torch.float16\n",
      "transformer.h.40.ln_1.bias: torch.float16\n",
      "transformer.h.40.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.40.attn.c_attn.bias: torch.float16\n",
      "transformer.h.40.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.40.attn.c_proj.bias: torch.float16\n",
      "transformer.h.40.ln_2.weight: torch.float16\n",
      "transformer.h.40.ln_2.bias: torch.float16\n",
      "transformer.h.40.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.40.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.40.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.40.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.41.ln_1.weight: torch.float16\n",
      "transformer.h.41.ln_1.bias: torch.float16\n",
      "transformer.h.41.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.41.attn.c_attn.bias: torch.float16\n",
      "transformer.h.41.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.41.attn.c_proj.bias: torch.float16\n",
      "transformer.h.41.ln_2.weight: torch.float16\n",
      "transformer.h.41.ln_2.bias: torch.float16\n",
      "transformer.h.41.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.41.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.41.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.41.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.42.ln_1.weight: torch.float16\n",
      "transformer.h.42.ln_1.bias: torch.float16\n",
      "transformer.h.42.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.42.attn.c_attn.bias: torch.float16\n",
      "transformer.h.42.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.42.attn.c_proj.bias: torch.float16\n",
      "transformer.h.42.ln_2.weight: torch.float16\n",
      "transformer.h.42.ln_2.bias: torch.float16\n",
      "transformer.h.42.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.42.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.42.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.42.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.43.ln_1.weight: torch.float16\n",
      "transformer.h.43.ln_1.bias: torch.float16\n",
      "transformer.h.43.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.43.attn.c_attn.bias: torch.float16\n",
      "transformer.h.43.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.43.attn.c_proj.bias: torch.float16\n",
      "transformer.h.43.ln_2.weight: torch.float16\n",
      "transformer.h.43.ln_2.bias: torch.float16\n",
      "transformer.h.43.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.43.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.43.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.43.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.44.ln_1.weight: torch.float16\n",
      "transformer.h.44.ln_1.bias: torch.float16\n",
      "transformer.h.44.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.44.attn.c_attn.bias: torch.float16\n",
      "transformer.h.44.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.44.attn.c_proj.bias: torch.float16\n",
      "transformer.h.44.ln_2.weight: torch.float16\n",
      "transformer.h.44.ln_2.bias: torch.float16\n",
      "transformer.h.44.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.44.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.44.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.44.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.45.ln_1.weight: torch.float16\n",
      "transformer.h.45.ln_1.bias: torch.float16\n",
      "transformer.h.45.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.45.attn.c_attn.bias: torch.float16\n",
      "transformer.h.45.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.45.attn.c_proj.bias: torch.float16\n",
      "transformer.h.45.ln_2.weight: torch.float16\n",
      "transformer.h.45.ln_2.bias: torch.float16\n",
      "transformer.h.45.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.45.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.45.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.45.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.46.ln_1.weight: torch.float16\n",
      "transformer.h.46.ln_1.bias: torch.float16\n",
      "transformer.h.46.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.46.attn.c_attn.bias: torch.float16\n",
      "transformer.h.46.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.46.attn.c_proj.bias: torch.float16\n",
      "transformer.h.46.ln_2.weight: torch.float16\n",
      "transformer.h.46.ln_2.bias: torch.float16\n",
      "transformer.h.46.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.46.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.46.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.46.mlp.c_proj.bias: torch.float16\n",
      "transformer.h.47.ln_1.weight: torch.float16\n",
      "transformer.h.47.ln_1.bias: torch.float16\n",
      "transformer.h.47.attn.c_attn.weight: torch.uint8\n",
      "transformer.h.47.attn.c_attn.bias: torch.float16\n",
      "transformer.h.47.attn.c_proj.weight: torch.uint8\n",
      "transformer.h.47.attn.c_proj.bias: torch.float16\n",
      "transformer.h.47.ln_2.weight: torch.float16\n",
      "transformer.h.47.ln_2.bias: torch.float16\n",
      "transformer.h.47.mlp.c_fc.weight: torch.uint8\n",
      "transformer.h.47.mlp.c_fc.bias: torch.float16\n",
      "transformer.h.47.mlp.c_proj.weight: torch.uint8\n",
      "transformer.h.47.mlp.c_proj.bias: torch.float16\n",
      "transformer.ln_f.weight: torch.float16\n",
      "transformer.ln_f.bias: torch.float16\n",
      "\n",
      "Running a forward pass...\n",
      "Model output: Hello, world!\n",
      "\n",
      "This is a simple\n",
      "\n",
      "Memory allocated (MB): 957.5537109375\n"
     ]
    }
   ],
   "source": [
    "# Verify model type and configuration\n",
    "print(\"Model type:\", type(model_nf4))\n",
    "print(\"Model configuration:\", model_nf4.config)\n",
    "\n",
    "# Check the dtype of model parameters to confirm quantization\n",
    "print(\"\\nModel parameter data types:\")\n",
    "for name, param in model_nf4.named_parameters():\n",
    "    print(f\"{name}: {param.dtype}\")\n",
    "\n",
    "# Run a simple forward pass\n",
    "test_input = tokenizer(\"Hello, world!\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Move to device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_nf4 = model_nf4.to(device)\n",
    "test_input = test_input.to(device)\n",
    "\n",
    "print(\"\\nRunning a forward pass...\")\n",
    "with torch.no_grad():\n",
    "    output = model_nf4.generate(test_input, max_length=10)\n",
    "print(\"Model output:\", tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nMemory allocated (MB):\", torch.cuda.memory_allocated(device) / 1024**2)\n",
    "else:\n",
    "    print(\"\\nRunning on CPU; memory usage check skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCs44A9o27FJ"
   },
   "source": [
    "## Methods to Inject QLoRA Layers into the Model\n",
    "\n",
    "We have three methods for injecting QLoRA layers into the model:\n",
    "\n",
    "### 1. **Wrapper Class Approach**  \n",
    "This involves copying the model‚Äôs source code from the Hugging Face implementation and creating a wrapper class on top of the original model. While not the most practical, this approach helps us understand what‚Äôs happening under the hood. We'll test it to gain insights, even though it's not ideal for larger-scale use.\n",
    "\n",
    "### 2. **Compact Custom Implementation**  \n",
    "This method involves designing a compact class, as we've been doing in this notebook. We inject the QLoRA layers into the model by looping through and identifying the appropriate layer for insertion. This approach is more efficient and scalable compared to the wrapper class.\n",
    "\n",
    "### 3. **PEFT from Hugging Face** (Production-Ready)  \n",
    "The most production-ready solution is using the PEFT (Parameter-Efficient Fine-Tuning) library from Hugging Face. While we need to learn this method first, it‚Äôs the most optimized for production environments and will integrate seamlessly with existing Hugging Face tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYiC-T8O27FJ"
   },
   "source": [
    "### 1. **Wrapper Class Approach**  \n",
    "\n",
    "Here's the compact markdown for implementing LoRA (Low-Rank Adaptation) in a GPT model:\n",
    "\n",
    "```markdown\n",
    "# Reimplementing LoRA in GPT Model\n",
    "\n",
    "The goal is to adapt the self-attention mechanism in a GPT model (such as `GPT2` from Huggingface) to incorporate Low-Rank Adaptation (LoRA). We'll replace the original self-attention module with a new class `LoraGPT2SelfAttention` that adds LoRA matrices.\n",
    "\n",
    "### Step 1: Define `LoraGPT2SelfAttention`\n",
    "\n",
    "We extend the `GPT2SelfAttention` module to include LoRA matrices. These matrices will modify the query and value components while keeping the rest of the attention mechanism unchanged.\n",
    "\n",
    "```python\n",
    "class LoraGPT2SelfAttention(GPT2SelfAttention):\n",
    "    \"\"\"\n",
    "    Extends GPT2SelfAttention with LoRA (Low-Rank Adaptation) matrices.\n",
    "    LoRA enhances efficiency by only updating the query and value matrices.\n",
    "    \"\"\"\n",
    "    def __init__(self, r=8, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        d = self.attn_head_size\n",
    "        \n",
    "        # Initialize LoRA matrices for query and value\n",
    "        self.lora_query_matrix_B = nn.Parameter(torch.zeros(d, r))\n",
    "        self.lora_query_matrix_A = nn.Parameter(torch.randn(r, d))\n",
    "        self.lora_value_matrix_B = nn.Parameter(torch.zeros(d, r))\n",
    "        self.lora_value_matrix_A = nn.Parameter(torch.randn(r, d))\n",
    "    \n",
    "    def lora_query(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the query component.\n",
    "        \"\"\"\n",
    "        lora_query_weights = torch.matmul(self.lora_query_matrix_B, self.lora_query_matrix_A)\n",
    "        return self.query(x) + F.linear(x, lora_query_weights)\n",
    "\n",
    "    def lora_value(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the value component.\n",
    "        \"\"\"\n",
    "        lora_value_weights = torch.matmul(self.lora_value_matrix_B, self.lora_value_matrix_A)\n",
    "        return self.value(x) + F.linear(x, lora_value_weights)\n",
    "```\n",
    "\n",
    "### Step 2: Modify the `forward` Method\n",
    "\n",
    "To apply the LoRA modifications, we overwrite the `forward` method of `GPT2SelfAttention`. We replace calls to the original `query` and `value` functions with our LoRA-enhanced versions.\n",
    "\n",
    "```python\n",
    "class LoraGPT2SelfAttention(GPT2SelfAttention):\n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        # Use LoRA-enhanced query and value\n",
    "        mixed_query_layer = self.lora_query(hidden_states)\n",
    "        \n",
    "        # Regular key layer, no LoRA\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        \n",
    "        # Use LoRA-enhanced value\n",
    "        value_layer = self.transpose_for_scores(self.lora_value(hidden_states))\n",
    "        \n",
    "        # ... (rest of the forward code, unchanged)\n",
    "```\n",
    "\n",
    "### Step 3: Replace Attention Modules in GPT Model\n",
    "\n",
    "We replace the `GPT2SelfAttention` module in the original GPT model with `LoraGPT2SelfAttention`. This ensures that all attention layers use the LoRA mechanism.\n",
    "\n",
    "```python\n",
    "class LoraWrapperGPT2(nn.Module):\n",
    "    def __init__(self, task_type, num_classes=None, dropout_rate=0.1, model_id=\"gpt2\",\n",
    "                 lora_rank=8, train_biases=True, train_embedding=False, train_layer_norms=True):\n",
    "        \"\"\"\n",
    "        A wrapper for GPT2 with Low-Rank Adaptation (LoRA) for various NLP tasks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "        self.model = GPT2Model.from_pretrained(model_id)\n",
    "        self.model_config = self.model.config\n",
    "\n",
    "        # Add task-specific layer\n",
    "        d_model = self.model_config.n_embd\n",
    "        self.finetune_head_norm = nn.LayerNorm(d_model)\n",
    "        self.finetune_head_dropout = nn.Dropout(dropout_rate)\n",
    "        self.finetune_head_classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        # Set up LoRA model for training\n",
    "        self.replace_multihead_attention()\n",
    "        self.freeze_parameters_except_lora_and_bias()\n",
    "\n",
    "    def replace_multihead_attention(self):\n",
    "        \"\"\"\n",
    "        Replaces all attention modules in GPT2 with LoRA-modified ones.\n",
    "        \"\"\"\n",
    "        self.replace_multihead_attention_recursion(self.model)\n",
    "\n",
    "    def replace_multihead_attention_recursion(self, model):\n",
    "        \"\"\"\n",
    "        Recursively replaces GPT2SelfAttention with LoraGPT2SelfAttention in the model.\n",
    "        \"\"\"\n",
    "        for name, module in model.named_children():\n",
    "            if isinstance(module, GPT2SelfAttention):\n",
    "                new_layer = LoraGPT2SelfAttention(r=self.lora_rank, config=self.model_config)\n",
    "                new_layer.load_state_dict(module.state_dict(), strict=False)\n",
    "                setattr(model, name, new_layer)\n",
    "            else:\n",
    "                self.replace_multihead_attention_recursion(module)\n",
    "```\n",
    "\n",
    "### Step 4: Fine-Tuning for NLP Tasks\n",
    "\n",
    "The wrapper also allows fine-tuning for different NLP tasks (e.g., GLUE, SQuAD). The attention layers are replaced by LoRA-enabled ones, while ensuring that only LoRA parameters and select others (biases, layer norms) are trainable.\n",
    "\n",
    "```python\n",
    "class LoraWrapperGPT2(nn.Module):\n",
    "    # ... (same as above)\n",
    "\n",
    "    def freeze_parameters_except_lora_and_bias(self):\n",
    "        \"\"\"\n",
    "        Freezes all model parameters except those related to LoRA and biases.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.finetune_head_classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.get_parameters():\n",
    "            if \"lora\" in param.name:\n",
    "                param.requires_grad = True\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We have successfully integrated LoRA into the GPT2 self-attention mechanism. This new model efficiently adapts query and value components during fine-tuning, while preserving the original model's parameters. The wrapper enables fine-tuning for specific tasks while using LoRA for efficient adaptation.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RD2WaMLR27FK"
   },
   "source": [
    "### 2. **Compact Custom Implementation**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMReyWUp27FL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def quantize_tensor(tensor, num_bits=8):\n",
    "    \"\"\"\n",
    "    Simple uniform quantization function.\n",
    "    Scales tensor values to the [0, 2^num_bits - 1] range, rounds to integers, and rescales.\n",
    "    \"\"\"\n",
    "    scale = torch.max(torch.abs(tensor))  # Find the max value for scaling\n",
    "    q_max = 2 ** num_bits - 1  # Max value for the given bit width\n",
    "    tensor_scaled = tensor / scale  # Scale tensor to the range [-1, 1]\n",
    "    tensor_quantized = torch.round(tensor_scaled * q_max)  # Round to integer values\n",
    "    tensor_quantized = tensor_quantized / q_max  # Rescale back to the range [-1, 1]\n",
    "    return tensor_quantized * scale  # Rescale to the original range\n",
    "\n",
    "class LoraLinear(nn.Linear):\n",
    "    \"\"\"\n",
    "    Extends a PyTorch linear layer with Low-Rank Adaptation (LoRA) and double quantization.\n",
    "    LoRA adds two matrices to the layer, allowing for efficient training of large models.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, r=8, num_bits=8, *args, **kwargs):\n",
    "        super().__init__(in_features, out_features, *args, **kwargs)\n",
    "\n",
    "        # Initialize LoRA matrices\n",
    "        self.lora_matrix_B = nn.Parameter(torch.zeros(out_features, r))  # B matrix of shape (out_features, r)\n",
    "        self.lora_matrix_A = nn.Parameter(torch.randn(r, in_features))  # A matrix of shape (r, in_features)\n",
    "\n",
    "        # Store the quantization bit-width for later use\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "        # Freeze the original weight matrix (no gradients)\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantize A and B matrices\n",
    "        qA = quantize_tensor(self.lora_matrix_A, num_bits=self.num_bits)\n",
    "        qB = quantize_tensor(self.lora_matrix_B, num_bits=self.num_bits)\n",
    "\n",
    "        # Compute LoRA weight adjustment by multiplying quantized B and A\n",
    "        lora_weights = torch.matmul(qB, qA)\n",
    "\n",
    "        # Quantize the result of the multiplication\n",
    "        quantized_lora_weights = quantize_tensor(lora_weights, num_bits=self.num_bits)\n",
    "\n",
    "        # Apply the original and LoRA-adjusted linear transformations\n",
    "        return super().forward(x) + F.linear(x, quantized_lora_weights)  # x @ W + x @ LoRA_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE39SXhS27FL"
   },
   "source": [
    "## Key Points:\n",
    "LoRA Mechanism: Only a small subset of parameters (the LoRA parameters lora_a and lora_b) are updated during training, which makes the process efficient.\n",
    "Quantization: Optionally, the original weights can be quantized to reduce memory usage.\n",
    "Freezing Original Weights: The original weights are frozen, ensuring that only the low-rank components are fine-tuned.\n",
    "This implementation is beneficial in transfer learning scenarios, where a pretrained model can be adapted to a new task without retraining all the parameters. Instead, the low-rank matrices efficiently modify the behavior of the pretrained model.\n",
    "\n",
    "Now lets replace the old layers with qlora as follow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQL0AsFg27FM"
   },
   "outputs": [],
   "source": [
    "def replace_with_qlora(model, rank=8, num_bits=4):\n",
    "    # Access the transformer part of the model (which is of type GPT2Model)\n",
    "    transformer = model.transformer  # GPT2Model\n",
    "\n",
    "    # Iterate over each GPT2Block inside the transformer model\n",
    "    for block in transformer.h:  # transformer.h is a ModuleList of GPT2Block modules\n",
    "        # In GPT2SdpaAttention (attn)\n",
    "        # Check if 'c_attn' and 'c_proj' are instances of Linear (generic type)\n",
    "        if isinstance(block.attn.c_attn, torch.nn.Linear):  # Check if it's a standard Linear layer\n",
    "            block.attn.c_attn = LoraLinear(block.attn.c_attn.in_features, block.attn.c_attn.out_features, r=rank)\n",
    "\n",
    "        if isinstance(block.attn.c_proj, torch.nn.Linear):  # Check if it's a standard Linear layer\n",
    "            block.attn.c_proj = LoraLinear(block.attn.c_proj.in_features, block.attn.c_proj.out_features, r=rank)\n",
    "\n",
    "        # In GPT2MLP (mlp)\n",
    "        if isinstance(block.mlp.c_fc, torch.nn.Linear):  # Check if it's a standard Linear layer\n",
    "            block.mlp.c_fc = LoraLinear(block.mlp.c_fc.in_features, block.mlp.c_fc.out_features, r=rank)\n",
    "\n",
    "        if isinstance(block.mlp.c_proj, torch.nn.Linear):  # Check if it's a standard Linear layer\n",
    "            block.mlp.c_proj = LoraLinear(block.mlp.c_proj.in_features, block.mlp.c_proj.out_features, r=rank)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuvtbThw27FM"
   },
   "source": [
    "Lets apply the function and countn the number of the parametrs before and after the injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFEx1B5x27FN",
    "outputId": "e9b88805-2828-4bff-82b2-0ee8a09b7739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1567441600\n",
      "LoRA Parameters: 9830400\n",
      "Percentage of LoRA Parameters: 0.63%\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): LoraLinear(in_features=1600, out_features=4800, bias=True)\n",
      "          (c_proj): LoraLinear(in_features=1600, out_features=1600, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): LoraLinear(in_features=1600, out_features=6400, bias=True)\n",
      "          (c_proj): LoraLinear(in_features=6400, out_features=1600, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_nf4_lora = replace_with_qlora(model_nf4, rank=8, num_bits=4)\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    # Total parameters in the model (includes LoRA and original weights)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "def count_lora_params(model):\n",
    "    lora_params = 0\n",
    "    # Iterate through the layers in the model to find LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if the parameter is part of a QLoRALinear layer (the LoRA layers)\n",
    "        if 'lora_matrix_A' in name or 'lora_matrix_B' in name:\n",
    "            lora_params += param.numel()\n",
    "    return lora_params\n",
    "\n",
    "# Assuming `model_nf4_lora` is the model with LoRA applied\n",
    "# Calculate total number of parameters in the model\n",
    "total_params = count_params(model_nf4_lora)\n",
    "\n",
    "# Calculate total LoRA parameters (A and B matrices)\n",
    "lora_params = count_lora_params(model_nf4_lora)\n",
    "\n",
    "# Calculate percentage of LoRA parameters\n",
    "lora_percentage = (lora_params / total_params) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"LoRA Parameters: {lora_params}\")\n",
    "print(f\"Percentage of LoRA Parameters: {lora_percentage:.2f}%\")\n",
    "\n",
    "print(model_nf4_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_-Aa8JxETdj"
   },
   "source": [
    "Freezing the modle graph expect for the lorfa parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HN8q1aQlEgDe",
    "outputId": "862dfc80-f88c-4e92-da04-b26874d964b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight: requires_grad=False\n",
      "transformer.wpe.weight: requires_grad=False\n",
      "transformer.h.0.ln_1.weight: requires_grad=False\n",
      "transformer.h.0.ln_1.bias: requires_grad=False\n",
      "transformer.h.0.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.0.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.0.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.0.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.0.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.0.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.0.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.0.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.0.ln_2.weight: requires_grad=False\n",
      "transformer.h.0.ln_2.bias: requires_grad=False\n",
      "transformer.h.0.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.0.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.0.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.0.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.0.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.0.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.0.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.0.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.1.ln_1.weight: requires_grad=False\n",
      "transformer.h.1.ln_1.bias: requires_grad=False\n",
      "transformer.h.1.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.1.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.1.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.1.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.1.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.1.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.1.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.1.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.1.ln_2.weight: requires_grad=False\n",
      "transformer.h.1.ln_2.bias: requires_grad=False\n",
      "transformer.h.1.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.1.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.1.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.1.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.1.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.1.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.1.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.1.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.2.ln_1.weight: requires_grad=False\n",
      "transformer.h.2.ln_1.bias: requires_grad=False\n",
      "transformer.h.2.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.2.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.2.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.2.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.2.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.2.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.2.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.2.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.2.ln_2.weight: requires_grad=False\n",
      "transformer.h.2.ln_2.bias: requires_grad=False\n",
      "transformer.h.2.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.2.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.2.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.2.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.2.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.2.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.2.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.2.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.3.ln_1.weight: requires_grad=False\n",
      "transformer.h.3.ln_1.bias: requires_grad=False\n",
      "transformer.h.3.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.3.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.3.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.3.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.3.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.3.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.3.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.3.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.3.ln_2.weight: requires_grad=False\n",
      "transformer.h.3.ln_2.bias: requires_grad=False\n",
      "transformer.h.3.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.3.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.3.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.3.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.3.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.3.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.3.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.3.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.4.ln_1.weight: requires_grad=False\n",
      "transformer.h.4.ln_1.bias: requires_grad=False\n",
      "transformer.h.4.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.4.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.4.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.4.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.4.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.4.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.4.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.4.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.4.ln_2.weight: requires_grad=False\n",
      "transformer.h.4.ln_2.bias: requires_grad=False\n",
      "transformer.h.4.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.4.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.4.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.4.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.4.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.4.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.4.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.4.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.5.ln_1.weight: requires_grad=False\n",
      "transformer.h.5.ln_1.bias: requires_grad=False\n",
      "transformer.h.5.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.5.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.5.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.5.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.5.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.5.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.5.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.5.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.5.ln_2.weight: requires_grad=False\n",
      "transformer.h.5.ln_2.bias: requires_grad=False\n",
      "transformer.h.5.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.5.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.5.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.5.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.5.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.5.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.5.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.5.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.6.ln_1.weight: requires_grad=False\n",
      "transformer.h.6.ln_1.bias: requires_grad=False\n",
      "transformer.h.6.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.6.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.6.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.6.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.6.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.6.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.6.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.6.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.6.ln_2.weight: requires_grad=False\n",
      "transformer.h.6.ln_2.bias: requires_grad=False\n",
      "transformer.h.6.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.6.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.6.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.6.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.6.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.6.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.6.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.6.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.7.ln_1.weight: requires_grad=False\n",
      "transformer.h.7.ln_1.bias: requires_grad=False\n",
      "transformer.h.7.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.7.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.7.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.7.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.7.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.7.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.7.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.7.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.7.ln_2.weight: requires_grad=False\n",
      "transformer.h.7.ln_2.bias: requires_grad=False\n",
      "transformer.h.7.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.7.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.7.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.7.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.7.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.7.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.7.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.7.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.8.ln_1.weight: requires_grad=False\n",
      "transformer.h.8.ln_1.bias: requires_grad=False\n",
      "transformer.h.8.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.8.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.8.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.8.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.8.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.8.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.8.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.8.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.8.ln_2.weight: requires_grad=False\n",
      "transformer.h.8.ln_2.bias: requires_grad=False\n",
      "transformer.h.8.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.8.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.8.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.8.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.8.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.8.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.8.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.8.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.9.ln_1.weight: requires_grad=False\n",
      "transformer.h.9.ln_1.bias: requires_grad=False\n",
      "transformer.h.9.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.9.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.9.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.9.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.9.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.9.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.9.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.9.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.9.ln_2.weight: requires_grad=False\n",
      "transformer.h.9.ln_2.bias: requires_grad=False\n",
      "transformer.h.9.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.9.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.9.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.9.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.9.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.9.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.9.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.9.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.10.ln_1.weight: requires_grad=False\n",
      "transformer.h.10.ln_1.bias: requires_grad=False\n",
      "transformer.h.10.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.10.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.10.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.10.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.10.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.10.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.10.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.10.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.10.ln_2.weight: requires_grad=False\n",
      "transformer.h.10.ln_2.bias: requires_grad=False\n",
      "transformer.h.10.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.10.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.10.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.10.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.10.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.10.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.10.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.10.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.11.ln_1.weight: requires_grad=False\n",
      "transformer.h.11.ln_1.bias: requires_grad=False\n",
      "transformer.h.11.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.11.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.11.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.11.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.11.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.11.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.11.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.11.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.11.ln_2.weight: requires_grad=False\n",
      "transformer.h.11.ln_2.bias: requires_grad=False\n",
      "transformer.h.11.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.11.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.11.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.11.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.11.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.11.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.11.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.11.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.12.ln_1.weight: requires_grad=False\n",
      "transformer.h.12.ln_1.bias: requires_grad=False\n",
      "transformer.h.12.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.12.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.12.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.12.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.12.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.12.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.12.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.12.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.12.ln_2.weight: requires_grad=False\n",
      "transformer.h.12.ln_2.bias: requires_grad=False\n",
      "transformer.h.12.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.12.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.12.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.12.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.12.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.12.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.12.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.12.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.13.ln_1.weight: requires_grad=False\n",
      "transformer.h.13.ln_1.bias: requires_grad=False\n",
      "transformer.h.13.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.13.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.13.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.13.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.13.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.13.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.13.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.13.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.13.ln_2.weight: requires_grad=False\n",
      "transformer.h.13.ln_2.bias: requires_grad=False\n",
      "transformer.h.13.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.13.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.13.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.13.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.13.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.13.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.13.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.13.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.14.ln_1.weight: requires_grad=False\n",
      "transformer.h.14.ln_1.bias: requires_grad=False\n",
      "transformer.h.14.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.14.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.14.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.14.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.14.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.14.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.14.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.14.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.14.ln_2.weight: requires_grad=False\n",
      "transformer.h.14.ln_2.bias: requires_grad=False\n",
      "transformer.h.14.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.14.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.14.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.14.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.14.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.14.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.14.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.14.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.15.ln_1.weight: requires_grad=False\n",
      "transformer.h.15.ln_1.bias: requires_grad=False\n",
      "transformer.h.15.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.15.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.15.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.15.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.15.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.15.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.15.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.15.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.15.ln_2.weight: requires_grad=False\n",
      "transformer.h.15.ln_2.bias: requires_grad=False\n",
      "transformer.h.15.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.15.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.15.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.15.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.15.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.15.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.15.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.15.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.16.ln_1.weight: requires_grad=False\n",
      "transformer.h.16.ln_1.bias: requires_grad=False\n",
      "transformer.h.16.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.16.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.16.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.16.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.16.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.16.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.16.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.16.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.16.ln_2.weight: requires_grad=False\n",
      "transformer.h.16.ln_2.bias: requires_grad=False\n",
      "transformer.h.16.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.16.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.16.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.16.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.16.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.16.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.16.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.16.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.17.ln_1.weight: requires_grad=False\n",
      "transformer.h.17.ln_1.bias: requires_grad=False\n",
      "transformer.h.17.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.17.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.17.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.17.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.17.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.17.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.17.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.17.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.17.ln_2.weight: requires_grad=False\n",
      "transformer.h.17.ln_2.bias: requires_grad=False\n",
      "transformer.h.17.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.17.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.17.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.17.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.17.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.17.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.17.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.17.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.18.ln_1.weight: requires_grad=False\n",
      "transformer.h.18.ln_1.bias: requires_grad=False\n",
      "transformer.h.18.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.18.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.18.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.18.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.18.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.18.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.18.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.18.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.18.ln_2.weight: requires_grad=False\n",
      "transformer.h.18.ln_2.bias: requires_grad=False\n",
      "transformer.h.18.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.18.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.18.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.18.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.18.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.18.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.18.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.18.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.19.ln_1.weight: requires_grad=False\n",
      "transformer.h.19.ln_1.bias: requires_grad=False\n",
      "transformer.h.19.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.19.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.19.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.19.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.19.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.19.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.19.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.19.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.19.ln_2.weight: requires_grad=False\n",
      "transformer.h.19.ln_2.bias: requires_grad=False\n",
      "transformer.h.19.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.19.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.19.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.19.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.19.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.19.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.19.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.19.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.20.ln_1.weight: requires_grad=False\n",
      "transformer.h.20.ln_1.bias: requires_grad=False\n",
      "transformer.h.20.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.20.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.20.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.20.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.20.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.20.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.20.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.20.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.20.ln_2.weight: requires_grad=False\n",
      "transformer.h.20.ln_2.bias: requires_grad=False\n",
      "transformer.h.20.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.20.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.20.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.20.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.20.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.20.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.20.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.20.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.21.ln_1.weight: requires_grad=False\n",
      "transformer.h.21.ln_1.bias: requires_grad=False\n",
      "transformer.h.21.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.21.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.21.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.21.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.21.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.21.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.21.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.21.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.21.ln_2.weight: requires_grad=False\n",
      "transformer.h.21.ln_2.bias: requires_grad=False\n",
      "transformer.h.21.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.21.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.21.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.21.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.21.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.21.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.21.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.21.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.22.ln_1.weight: requires_grad=False\n",
      "transformer.h.22.ln_1.bias: requires_grad=False\n",
      "transformer.h.22.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.22.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.22.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.22.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.22.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.22.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.22.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.22.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.22.ln_2.weight: requires_grad=False\n",
      "transformer.h.22.ln_2.bias: requires_grad=False\n",
      "transformer.h.22.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.22.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.22.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.22.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.22.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.22.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.22.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.22.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.23.ln_1.weight: requires_grad=False\n",
      "transformer.h.23.ln_1.bias: requires_grad=False\n",
      "transformer.h.23.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.23.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.23.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.23.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.23.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.23.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.23.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.23.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.23.ln_2.weight: requires_grad=False\n",
      "transformer.h.23.ln_2.bias: requires_grad=False\n",
      "transformer.h.23.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.23.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.23.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.23.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.23.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.23.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.23.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.23.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.24.ln_1.weight: requires_grad=False\n",
      "transformer.h.24.ln_1.bias: requires_grad=False\n",
      "transformer.h.24.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.24.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.24.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.24.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.24.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.24.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.24.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.24.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.24.ln_2.weight: requires_grad=False\n",
      "transformer.h.24.ln_2.bias: requires_grad=False\n",
      "transformer.h.24.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.24.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.24.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.24.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.24.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.24.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.24.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.24.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.25.ln_1.weight: requires_grad=False\n",
      "transformer.h.25.ln_1.bias: requires_grad=False\n",
      "transformer.h.25.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.25.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.25.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.25.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.25.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.25.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.25.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.25.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.25.ln_2.weight: requires_grad=False\n",
      "transformer.h.25.ln_2.bias: requires_grad=False\n",
      "transformer.h.25.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.25.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.25.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.25.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.25.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.25.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.25.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.25.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.26.ln_1.weight: requires_grad=False\n",
      "transformer.h.26.ln_1.bias: requires_grad=False\n",
      "transformer.h.26.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.26.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.26.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.26.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.26.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.26.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.26.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.26.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.26.ln_2.weight: requires_grad=False\n",
      "transformer.h.26.ln_2.bias: requires_grad=False\n",
      "transformer.h.26.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.26.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.26.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.26.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.26.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.26.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.26.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.26.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.27.ln_1.weight: requires_grad=False\n",
      "transformer.h.27.ln_1.bias: requires_grad=False\n",
      "transformer.h.27.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.27.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.27.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.27.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.27.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.27.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.27.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.27.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.27.ln_2.weight: requires_grad=False\n",
      "transformer.h.27.ln_2.bias: requires_grad=False\n",
      "transformer.h.27.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.27.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.27.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.27.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.27.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.27.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.27.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.27.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.28.ln_1.weight: requires_grad=False\n",
      "transformer.h.28.ln_1.bias: requires_grad=False\n",
      "transformer.h.28.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.28.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.28.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.28.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.28.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.28.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.28.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.28.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.28.ln_2.weight: requires_grad=False\n",
      "transformer.h.28.ln_2.bias: requires_grad=False\n",
      "transformer.h.28.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.28.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.28.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.28.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.28.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.28.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.28.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.28.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.29.ln_1.weight: requires_grad=False\n",
      "transformer.h.29.ln_1.bias: requires_grad=False\n",
      "transformer.h.29.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.29.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.29.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.29.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.29.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.29.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.29.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.29.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.29.ln_2.weight: requires_grad=False\n",
      "transformer.h.29.ln_2.bias: requires_grad=False\n",
      "transformer.h.29.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.29.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.29.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.29.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.29.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.29.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.29.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.29.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.30.ln_1.weight: requires_grad=False\n",
      "transformer.h.30.ln_1.bias: requires_grad=False\n",
      "transformer.h.30.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.30.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.30.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.30.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.30.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.30.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.30.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.30.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.30.ln_2.weight: requires_grad=False\n",
      "transformer.h.30.ln_2.bias: requires_grad=False\n",
      "transformer.h.30.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.30.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.30.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.30.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.30.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.30.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.30.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.30.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.31.ln_1.weight: requires_grad=False\n",
      "transformer.h.31.ln_1.bias: requires_grad=False\n",
      "transformer.h.31.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.31.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.31.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.31.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.31.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.31.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.31.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.31.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.31.ln_2.weight: requires_grad=False\n",
      "transformer.h.31.ln_2.bias: requires_grad=False\n",
      "transformer.h.31.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.31.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.31.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.31.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.31.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.31.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.31.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.31.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.32.ln_1.weight: requires_grad=False\n",
      "transformer.h.32.ln_1.bias: requires_grad=False\n",
      "transformer.h.32.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.32.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.32.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.32.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.32.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.32.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.32.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.32.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.32.ln_2.weight: requires_grad=False\n",
      "transformer.h.32.ln_2.bias: requires_grad=False\n",
      "transformer.h.32.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.32.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.32.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.32.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.32.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.32.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.32.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.32.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.33.ln_1.weight: requires_grad=False\n",
      "transformer.h.33.ln_1.bias: requires_grad=False\n",
      "transformer.h.33.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.33.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.33.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.33.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.33.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.33.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.33.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.33.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.33.ln_2.weight: requires_grad=False\n",
      "transformer.h.33.ln_2.bias: requires_grad=False\n",
      "transformer.h.33.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.33.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.33.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.33.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.33.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.33.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.33.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.33.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.34.ln_1.weight: requires_grad=False\n",
      "transformer.h.34.ln_1.bias: requires_grad=False\n",
      "transformer.h.34.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.34.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.34.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.34.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.34.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.34.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.34.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.34.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.34.ln_2.weight: requires_grad=False\n",
      "transformer.h.34.ln_2.bias: requires_grad=False\n",
      "transformer.h.34.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.34.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.34.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.34.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.34.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.34.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.34.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.34.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.35.ln_1.weight: requires_grad=False\n",
      "transformer.h.35.ln_1.bias: requires_grad=False\n",
      "transformer.h.35.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.35.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.35.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.35.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.35.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.35.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.35.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.35.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.35.ln_2.weight: requires_grad=False\n",
      "transformer.h.35.ln_2.bias: requires_grad=False\n",
      "transformer.h.35.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.35.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.35.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.35.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.35.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.35.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.35.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.35.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.36.ln_1.weight: requires_grad=False\n",
      "transformer.h.36.ln_1.bias: requires_grad=False\n",
      "transformer.h.36.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.36.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.36.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.36.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.36.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.36.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.36.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.36.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.36.ln_2.weight: requires_grad=False\n",
      "transformer.h.36.ln_2.bias: requires_grad=False\n",
      "transformer.h.36.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.36.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.36.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.36.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.36.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.36.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.36.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.36.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.37.ln_1.weight: requires_grad=False\n",
      "transformer.h.37.ln_1.bias: requires_grad=False\n",
      "transformer.h.37.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.37.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.37.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.37.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.37.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.37.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.37.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.37.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.37.ln_2.weight: requires_grad=False\n",
      "transformer.h.37.ln_2.bias: requires_grad=False\n",
      "transformer.h.37.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.37.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.37.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.37.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.37.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.37.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.37.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.37.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.38.ln_1.weight: requires_grad=False\n",
      "transformer.h.38.ln_1.bias: requires_grad=False\n",
      "transformer.h.38.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.38.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.38.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.38.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.38.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.38.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.38.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.38.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.38.ln_2.weight: requires_grad=False\n",
      "transformer.h.38.ln_2.bias: requires_grad=False\n",
      "transformer.h.38.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.38.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.38.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.38.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.38.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.38.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.38.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.38.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.39.ln_1.weight: requires_grad=False\n",
      "transformer.h.39.ln_1.bias: requires_grad=False\n",
      "transformer.h.39.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.39.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.39.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.39.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.39.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.39.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.39.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.39.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.39.ln_2.weight: requires_grad=False\n",
      "transformer.h.39.ln_2.bias: requires_grad=False\n",
      "transformer.h.39.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.39.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.39.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.39.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.39.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.39.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.39.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.39.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.40.ln_1.weight: requires_grad=False\n",
      "transformer.h.40.ln_1.bias: requires_grad=False\n",
      "transformer.h.40.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.40.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.40.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.40.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.40.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.40.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.40.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.40.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.40.ln_2.weight: requires_grad=False\n",
      "transformer.h.40.ln_2.bias: requires_grad=False\n",
      "transformer.h.40.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.40.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.40.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.40.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.40.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.40.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.40.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.40.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.41.ln_1.weight: requires_grad=False\n",
      "transformer.h.41.ln_1.bias: requires_grad=False\n",
      "transformer.h.41.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.41.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.41.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.41.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.41.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.41.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.41.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.41.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.41.ln_2.weight: requires_grad=False\n",
      "transformer.h.41.ln_2.bias: requires_grad=False\n",
      "transformer.h.41.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.41.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.41.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.41.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.41.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.41.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.41.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.41.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.42.ln_1.weight: requires_grad=False\n",
      "transformer.h.42.ln_1.bias: requires_grad=False\n",
      "transformer.h.42.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.42.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.42.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.42.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.42.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.42.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.42.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.42.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.42.ln_2.weight: requires_grad=False\n",
      "transformer.h.42.ln_2.bias: requires_grad=False\n",
      "transformer.h.42.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.42.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.42.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.42.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.42.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.42.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.42.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.42.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.43.ln_1.weight: requires_grad=False\n",
      "transformer.h.43.ln_1.bias: requires_grad=False\n",
      "transformer.h.43.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.43.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.43.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.43.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.43.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.43.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.43.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.43.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.43.ln_2.weight: requires_grad=False\n",
      "transformer.h.43.ln_2.bias: requires_grad=False\n",
      "transformer.h.43.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.43.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.43.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.43.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.43.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.43.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.43.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.43.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.44.ln_1.weight: requires_grad=False\n",
      "transformer.h.44.ln_1.bias: requires_grad=False\n",
      "transformer.h.44.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.44.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.44.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.44.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.44.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.44.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.44.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.44.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.44.ln_2.weight: requires_grad=False\n",
      "transformer.h.44.ln_2.bias: requires_grad=False\n",
      "transformer.h.44.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.44.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.44.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.44.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.44.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.44.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.44.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.44.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.45.ln_1.weight: requires_grad=False\n",
      "transformer.h.45.ln_1.bias: requires_grad=False\n",
      "transformer.h.45.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.45.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.45.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.45.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.45.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.45.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.45.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.45.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.45.ln_2.weight: requires_grad=False\n",
      "transformer.h.45.ln_2.bias: requires_grad=False\n",
      "transformer.h.45.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.45.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.45.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.45.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.45.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.45.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.45.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.45.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.46.ln_1.weight: requires_grad=False\n",
      "transformer.h.46.ln_1.bias: requires_grad=False\n",
      "transformer.h.46.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.46.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.46.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.46.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.46.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.46.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.46.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.46.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.46.ln_2.weight: requires_grad=False\n",
      "transformer.h.46.ln_2.bias: requires_grad=False\n",
      "transformer.h.46.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.46.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.46.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.46.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.46.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.46.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.46.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.46.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.47.ln_1.weight: requires_grad=False\n",
      "transformer.h.47.ln_1.bias: requires_grad=False\n",
      "transformer.h.47.attn.c_attn.weight: requires_grad=False\n",
      "transformer.h.47.attn.c_attn.bias: requires_grad=False\n",
      "transformer.h.47.attn.c_attn.lora_matrix_B: requires_grad=True\n",
      "transformer.h.47.attn.c_attn.lora_matrix_A: requires_grad=True\n",
      "transformer.h.47.attn.c_proj.weight: requires_grad=False\n",
      "transformer.h.47.attn.c_proj.bias: requires_grad=False\n",
      "transformer.h.47.attn.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.47.attn.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.h.47.ln_2.weight: requires_grad=False\n",
      "transformer.h.47.ln_2.bias: requires_grad=False\n",
      "transformer.h.47.mlp.c_fc.weight: requires_grad=False\n",
      "transformer.h.47.mlp.c_fc.bias: requires_grad=False\n",
      "transformer.h.47.mlp.c_fc.lora_matrix_B: requires_grad=True\n",
      "transformer.h.47.mlp.c_fc.lora_matrix_A: requires_grad=True\n",
      "transformer.h.47.mlp.c_proj.weight: requires_grad=False\n",
      "transformer.h.47.mlp.c_proj.bias: requires_grad=False\n",
      "transformer.h.47.mlp.c_proj.lora_matrix_B: requires_grad=True\n",
      "transformer.h.47.mlp.c_proj.lora_matrix_A: requires_grad=True\n",
      "transformer.ln_f.weight: requires_grad=False\n",
      "transformer.ln_f.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "def freeze_all_except_lora(model):\n",
    "    # Iterate through all the modules in the model\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if the parameter belongs to a LoraLinear layer\n",
    "        if isinstance(param, nn.Parameter) and 'lora_matrix' in name:\n",
    "            # Don't freeze LoRA parameters\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            # Freeze other parameters\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Apply this function to the model\n",
    "freeze_all_except_lora(model_nf4_lora)\n",
    "\n",
    "def check_freezing_status(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "check_freezing_status(model_nf4_lora)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTk2Ro5hqGEU"
   },
   "source": [
    "Again lets delete variables we dont need anymore , we dont want the kernel to crash and us loosing all the progress we made (trust me i learned the hard way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LIOmspNaYGO",
    "outputId": "792e2918-877c-4f07-d352-70b54a76f019"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory allocated (MB): 6013.6416015625\n",
      "Model ready for training with accelerate!\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize accelerator\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Don't manually move the model; let accelerator handle the device placement\n",
    "model_nf4_lora.to(device)  # This will ensure the model is placed on the right device\n",
    "\n",
    "# Free memory by deleting unnecessary objects\n",
    "del model_nf4 , total_params , lora_params ,  lora_percentage , model_nf4_lora# Delete the original model if it's no longer needed\n",
    "\n",
    "# Manually clean up unused variables\n",
    "gc.collect()  # Run garbage collection to release memory\n",
    "\n",
    "# Check memory usage on the current device (GPU if available, otherwise CPU)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nMemory allocated (MB):\", torch.cuda.memory_allocated() / 1024**2)\n",
    "else:\n",
    "    print(\"\\nRunning on CPU; memory usage check skipped.\")\n",
    "\n",
    "# Now the model_nf4_lora is ready to be trained using accelerate\n",
    "print(\"Model ready for training with accelerate!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWK19j6wfCvB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "! PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3CcZqfAsbjc"
   },
   "source": [
    "### 3. **PEFT from Hugging Face** (Production-Ready)  \n",
    "\n",
    "# The Five Commandments of Low-Rank Adaptation\n",
    "\n",
    "1. **Utilize LoRA**  \n",
    "   Leverage LoRA for efficient model fine-tuning, focusing on keeping parameter sizes minimal.\n",
    "\n",
    "2. **Employ the PEFT Library**  \n",
    "   Use the PEFT library for LoRA implementation, avoiding the need for complex coding.\n",
    "\n",
    "3. **Extend LoRA Adaptations**  \n",
    "   Apply LoRA to all linear layers to enhance the overall model capabilities.\n",
    "\n",
    "4. **Keep Biases and Layer Norms Trainable**  \n",
    "   Maintain biases and layer norms as trainable parameters since they are critical for model adaptability and don‚Äôt require low-rank adaptations.\n",
    "\n",
    "5. **Apply Quantized-LoRA (QLoRA)**  \n",
    "   Use QLoRA to conserve GPU VRAM and enable the training of larger models efficiently.\n",
    "\n",
    "Hugging Face and PEFT (Parameter-Efficient Fine-Tuning) have revolutionized fine-tuning by making it simpler and more accessible. With libraries like Transformers and PEFT, users can quickly adapt large models using techniques like LoRA without deep dives into complex code. These tools focus on efficiency, enabling fine-tuning with minimal compute resources and parameter updates.\n",
    "\n",
    "However, in alignment with our motto *\"Understand it before you use it,\"* we take a hands-on approach. By building and experimenting with alternative methods from scratch, we deepen our understanding of how fine-tuning truly works, empowering us to innovate beyond pre-built solutions.\n",
    "\n",
    "Now lets stream the whole project step by step :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loRu1WKnqi1W",
    "outputId": "96f4d33e-c955-4dfe-edb3-6b6ef702c6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install peft datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNygxUDxtQM7"
   },
   "source": [
    "## 1- Load the Quantized model using BitsandBytes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "9ce22d26bdf44013996486faf871be3f",
      "66d4cb4ddfca489faf284ff66ab756ef",
      "aa8aa9af31814e71ae154bc1b258a1e0",
      "a284ef5c8a904858bed09f86f905a5fa",
      "3f340a0e28664c0097c61e6ffcaa834d",
      "c4bfa98efa0240d3871398575ade8880",
      "77baa7e8eaef459a8bc84d4ccd14f887",
      "e06b49e11a9741b8a2360d5924d63f3c",
      "c584dc1b243e4495ba6ec09c80e0ecd5",
      "af8784c247e34297b8348f72cdd70dae",
      "9ec8c4de5a474be7be5f2e5ffd9be5fc",
      "dc16b8d4845d4edb86853819ff9f368a",
      "9a00fb2b9da0486094f34e7a5e295dbc",
      "9871bf19abef4ea99c7b697c12ef2e3a",
      "9346f887049f45ce8e6807fdb9944db9",
      "a33f45bb990f4f788afe8cdae0a47a11",
      "0cbcd612636d420ca62706a25cfe9779",
      "87ded425cd004fde91d0ae174473f5cc",
      "df1ed03149814f2caf834693cc6720c1",
      "3050796c442f4187b3b4d57142ae368b",
      "14e91a525f9e49978a09a2c519bed716",
      "a11ec819807f4421868c0b739b8341e0",
      "f8fa288daadf487b8d809f30b06e195a",
      "390e2e4757d74b67ade614a36fbf0ca0",
      "54382fd7200a4477bf80eb2c0ba72192",
      "2b7f2572ee2f492c8e2a2a18d0c11b6e",
      "76c0a8e7160646cab895c37bd0e18709",
      "43881b3a6e034e5b9b255437fb71ea7f",
      "41ebca76a3134e67975fbd4ff1fac6ef",
      "94c5d44b4f2843a9943323be9446de41",
      "c08c932aa7b44c919aac6fa9e17ac49c",
      "91d6a2b572f84e7eb77c2fa0edf1f197",
      "62983b76354f452e9a2ab2eff51b66c4",
      "ae75793d57fc46efb9618dc2fd4140b1",
      "052938e031ae466181d25800f7043a09",
      "4c548ec48cf1455895f7a54f0498b5e5",
      "bcc3e06319e445ac9cee544e11fe0bca",
      "f9ead382ec11442f8a0e7e0fd18656e0",
      "48e8a83e57a84d89bf386fca6224e168",
      "61c1561376f64599b2e90e4928cbc013",
      "a4b56ebde5904b2e9f7e281d681b1aaf",
      "4097b564c5a74a29835becc504db6682",
      "e2a765b16ca84263ad6f8e3b9a51ea24",
      "fd44a05c1b1040a3b888cc83ef8da2c1",
      "8c2751a47a3a456ba8601bbd3d7f09db",
      "e584d1badbea48f98508b0c0bb75a69f",
      "69e6895a5c324b6f8226e8cde9fd140a",
      "ea1b7ca260db4bf6bea88324e5de465a",
      "bbd161fa39d54872ab77c43973f10154",
      "68b3f2079804426ca6b7b24369c777d0",
      "fceefefe2ae64c4ea97a62e64c591227",
      "754296f1b020494290bb8772cd3071f9",
      "456275993bfc453f8cf9296579478a4f",
      "e0cc42735e5a44e4bd8d9b483da11463",
      "92c0342f233649ef84a9701c42f37b9d",
      "ed37ab9bbcd1490da3c0af4d8e3f16ef",
      "c19e1ba09658415d86cc7c66b7c769ad",
      "6bf936caaff44d3cb87bcb401fcb5b05",
      "4ffc13a5c2024ff6b4ac8998a8364017",
      "2c0cba92c3bd44cfa33dcd894b1953f8",
      "bb897481bf0744348eeb2220c474a3ee",
      "2ad150411c1b472d9d762a94d3dc4e8d",
      "b3faeaad66ef42afb0f8fd50e7ea7362",
      "cbc3169942e54b79850f467173fa98ab",
      "2c6db44a74b54e549379cffff3a3736c",
      "d42595d98e0a418ab72a6187a53a0cca",
      "4f66a5b23687484e919df70031026b7a",
      "d66ebaa704e8499da9f1158b4865cabb",
      "3dd5010265954c38b762c29082fe9143",
      "2a28de2e45184dd1afd917bdb8596e14",
      "42fd8c94e31f49e0802fb12b78492df1",
      "5efbe24c06f74267963d32461f7281e9",
      "6f61aa230ea54f86b214293b1720010a",
      "074e10022dad42278e8a385a89a3d689",
      "dacc01e53cf84a2aba9d4a1d39b97d4a",
      "e7c4a4558ecb4178b32d550858cdf24a",
      "9a72aee2787146188abf81f08f5c5c78"
     ]
    },
    "id": "saffrkP7BTok",
    "outputId": "d3543f7f-0ff1-4445-a259-adf15387aa24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce22d26bdf44013996486faf871be3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc16b8d4845d4edb86853819ff9f368a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fa288daadf487b8d809f30b06e195a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae75793d57fc46efb9618dc2fd4140b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2751a47a3a456ba8601bbd3d7f09db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed37ab9bbcd1490da3c0af4d8e3f16ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f66a5b23687484e919df70031026b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM , BitsAndBytesConfig , AutoTokenizer\n",
    "\n",
    "model_id = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n",
    "\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTgqprStCkMl",
    "outputId": "31f41243-6b98-4d75-f2d0-3e5c08397744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Linear4bit(in_features=1600, out_features=4800, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=1600, out_features=1600, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Linear4bit(in_features=1600, out_features=6400, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=6400, out_features=1600, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_nf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr0Mb7NmuGue"
   },
   "source": [
    "## 2-Preparing the Dataset\n",
    "\n",
    "The dataset preparation involves a custom class (`InstructionDataset`) tailored for different training needs. Both classes achieve similar goals but are designed for distinct workflows. Here's an expanded explanation of the differences:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. First Class: Suitable for Custom PyTorch Training Loop**\n",
    "#### **Key Characteristics:**\n",
    "- Pre-encodes all data during initialization:\n",
    "  - Combines input (`instruction`, `input`) and output (`output`) into a single text format.\n",
    "  - Uses `tokenizer.encode()` to preprocess and store the tokenized data upfront.\n",
    "\n",
    "#### **Advantages:**\n",
    "- **Preprocessed Efficiency:**\n",
    "  - Tokenization is completed once during initialization, saving time during training epochs.\n",
    "- **Simple Output Structure:**\n",
    "  - The `__getitem__` method returns pre-tokenized sequences as plain tensors. This works well when you control the training loop and manage batching manually.\n",
    "\n",
    "#### **Limitations:**\n",
    "- **Lacks Flexibility for Adjustments:**\n",
    "  - Hardcoded tokenization settings (e.g., no dynamic padding or truncation). Changes like modifying `max_length` require reinitializing the dataset.\n",
    "- **No Attention Masks:**\n",
    "  - Hugging Face models rely on `attention_mask` to distinguish padding tokens from meaningful input. This is missing, potentially causing issues during training.\n",
    "- **Not Hugging Face Compatible:**\n",
    "  - Hugging Face's `Trainer` expects inputs like `input_ids`, `attention_mask`, and `labels` in a dictionary format. Additional preprocessing would be required to use this dataset with the `Trainer`.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Second Class: Flexible and Hugging Face Trainer Compatible**\n",
    "#### **Key Characteristics:**\n",
    "- Dynamically tokenizes data during `__getitem__`:\n",
    "  - Separately tokenizes input (`instruction + input`) and target (`output`).\n",
    "  - Adds padding and truncation dynamically based on `max_length`.\n",
    "  - Returns `input_ids`, `attention_mask`, and `labels` as a dictionary.\n",
    "\n",
    "#### **Advantages:**\n",
    "- **Dynamic Tokenization:**\n",
    "  - Allows adjustments to tokenization parameters like `max_length` without reinitializing the dataset.\n",
    "  - Suitable for a variety of tasks, including those requiring sequence-to-sequence modeling or causal language modeling.\n",
    "- **Attention Mask Support:**\n",
    "  - Generates `attention_mask`, ensuring that padding tokens are ignored during training, which is crucial for models like GPT-2.\n",
    "- **Hugging Face `Trainer` Ready:**\n",
    "  - Directly provides inputs (`input_ids`, `attention_mask`) and outputs (`labels`) in the format expected by Hugging Face models and `Trainer`.\n",
    "\n",
    "#### **Limitations:**\n",
    "- **Tokenization Overhead:**\n",
    "  - Repeated tokenization during each `__getitem__` call adds some computational overhead compared to pre-encoded data.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Each Class**\n",
    "#### **First Class:**\n",
    "- Use it when:\n",
    "  - You're implementing a custom PyTorch training loop.\n",
    "  - You want to pre-tokenize data for efficiency and control batching manually.\n",
    "  - The task doesn't require attention masks or specific input-output separation.\n",
    "\n",
    "#### **Second Class:**\n",
    "- Use it when:\n",
    "  - You're working with Hugging Face's ecosystem (e.g., `Trainer`, `DataCollator`).\n",
    "  - You need flexibility in handling different tokenization parameters (e.g., padding, truncation).\n",
    "  - Your task involves sequence-to-sequence learning or other NLP tasks requiring explicit `input_ids`, `attention_mask`, and `labels`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Technical Comparison**\n",
    "| Feature                          | First Class                       | Second Class                       |\n",
    "|-----------------------------------|------------------------------------|-------------------------------------|\n",
    "| **Tokenization**                  | Pre-encoded during initialization | Dynamic during `__getitem__`       |\n",
    "| **Attention Mask**                | Not included                      | Included                           |\n",
    "| **Output Format**                 | Single tensor                     | Dictionary with `input_ids`, `attention_mask`, and `labels` |\n",
    "| **Adjustable `max_length`**       | Requires reinitialization          | Dynamic                            |\n",
    "| **Hugging Face Compatibility**    | No                                | Yes                                |\n",
    "| **Efficiency**                    | Faster during training             | More flexible, but slightly slower |\n",
    "\n",
    "---\n",
    "\n",
    "In summary, the **first class** is ideal for a **custom PyTorch training loop**, while the **second class** is better suited for **Hugging Face's Trainer** and tasks requiring greater flexibility and model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_qbWeTP7OoG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # Combine 'instruction' and 'input' for the input text\n",
    "        input_text = item['instruction']\n",
    "        if item['input']:\n",
    "            input_text += f\"\\n{item['input']}\"\n",
    "\n",
    "        # Output is the 'output' key\n",
    "        target_text = item['output']\n",
    "\n",
    "        # Tokenize input and target text\n",
    "        inputs = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        targets = self.tokenizer(target_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvHMEZuwvT0I"
   },
   "source": [
    "## Load the GPT-2 Tokenizer with EOS Token\n",
    "\n",
    "To prepare our tokenizer for the Hugging Face Trainer, we'll load the GPT-2 tokenizer and ensure that the EOS (End of Sequence) token is added. The EOS token is **crucial** for our process as it signifies the end of a sequence, enabling the model to learn proper sequence boundaries.\n",
    "\n",
    "This tokenizer will now be ready to be passed to the Hugging Face Trainer, ensuring compatibility and proper handling of sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266,
     "referenced_widgets": [
      "f9b9bd3f77ec4912a4b465d8b226e79f",
      "d7b742b3d6a54dfe9b522c75a149c0ef",
      "0fbd40abe7bc4f5581ec9d06ffcc214e",
      "6e4083f264794b039a98295a3be84703",
      "f8518c3822f945e2bcb665eeca6270c5",
      "980824867d9e41c5b28f02f8a383ea17",
      "e5e3ffae9a9a4926a945a5b912085e5c",
      "35c47dbcefd44f579c594d9f92186b1c",
      "f0e77eb93c8b47cd91e9efec37dfea4f",
      "7d2b09634f59426287b07765210cbe48",
      "bbffa17f92174c74919c634523667d5a",
      "45eea6aca58e4f76ab137b66052e688e",
      "e76d3b0b0f2445c88861c40385fa680f",
      "9e9afb14e1544b0eb12e37dd211a0307",
      "9c9af8bf6aba4e8fb2cd87a2a377e7a5",
      "6cd1041767904f6ea7f4b4fcb9190f61",
      "1d72bea62bee413684318f5043d574db",
      "9c1a394fb1fb45fe97f80aacc51e911c",
      "6f51113c8aa4417cb426b4e6610b679b",
      "b634f8cc57364893ab63516c6fd24b8e",
      "6fbeb9b5953c409f9c1e0bf2d4a84a62",
      "ce678aef93d04fabbb0a8bf15fc0ca72",
      "1d83834a29d84ce58fe3394a915787e1",
      "08ca42b7858e489fb19c80e1fdf6f65a",
      "5b83f62050ac45b5be691968060f472a",
      "adbed114c062428c80ee217da5100e88",
      "1bb870bfb3a544e2b6fabfaefc2c6382",
      "d3a903ce4e4146e49d1f718a942a380d",
      "a32b755c148a4fad857b756ed3a2f2ed",
      "dea1e28509a2498ea973b4f6fa0fa28d",
      "1ffca17046e24dfeb73ce5e38110ff6f",
      "ae37131dfb5543a9aa975ba0d7b3c684",
      "9d8e297150454217b5f88d962056d61d",
      "94043165d53a40078d35a28784f416dc",
      "3bf8115f84224333b4ae73e778f9465e",
      "344050bd05c841e18eebbf71a2dee312",
      "d6fbc59c646b451e8cbdcd26a8017be1",
      "077174bb6e524ab2b7c649ed0d5d80ca",
      "6fd082a94e20495ca42930e2c4c0aaa2",
      "421521f674664eea94f6f1a3fdd4ecf1",
      "b1ad86dc630f49f58172898b1e82b03b",
      "7944eecd664043618d086a137d85e6d6",
      "a98ff2d5268049cab0e0d72aa0a9ff61",
      "fd91d931b3a94c508f723d8000c3d74b",
      "4fa96b6608d545b08a119a2e6a04942f",
      "97a9902a8ac5467581d3144fcfb6502d",
      "0f21f6622eb4401c9e0c6c5bf9a4c08d",
      "6cb2353b59d24bc3994eee4d26ea7ab5",
      "54dec3e8699848f9aaa4ed47bf1cf522",
      "811fb8cff6c349acaf2c529a957f06aa",
      "467a1c43b01e42da92a89c11ae90f8c7",
      "c8beb7bbb4c14977a04e187f07872506",
      "9f7fc6f92712453db6bf545f93e90781",
      "94e1ce57ca244b1c8a5d4dc749c9be1b",
      "105b5a5502f04adcb9f31c91ed524258"
     ]
    },
    "id": "z_CIi31p_Cos",
    "outputId": "a9d2caf8-27c7-4d88-882f-c27cc1d92d91"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b9bd3f77ec4912a4b465d8b226e79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eea6aca58e4f76ab137b66052e688e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d83834a29d84ce58fe3394a915787e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94043165d53a40078d35a28784f416dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa96b6608d545b08a119a2e6a04942f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token set to: <|endoftext|> (ID: 50256)\n",
      "{'input_ids': tensor([ 2061,   318,   262,  ..., 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([  464, 14069,  6264,  ..., 50256, 50256, 50256])}\n",
      "{'input_ids': tensor([ 4550,   530,  1627,  ..., 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([ 4299,  1720, 33529,  ..., 50256, 50256, 50256])}\n",
      "{'input_ids': tensor([30003,  6525,   262,  ..., 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([15363,    70,  9215,  ..., 50256, 50256, 50256])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token explicitly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Verify the padding token\n",
    "print(f\"Padding token set to: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "\n",
    "# Test a sample\n",
    "for i in range(3):\n",
    "    print(train_dataset[i])  # Inspect the structure of the tokenized data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95dF6T0BwtWL"
   },
   "source": [
    "## 3-Configuring LoRA (Low-Rank Adaptation)\n",
    "\n",
    "In this step, we configure the LoRA settings using the `LoraConfig` from the PEFT library. LoRA is a technique that adds low-rank updates to specific parts of a neural network, reducing the number of parameters to be fine-tuned while still achieving effective adaptation. Here's a breakdown of the configuration options:\n",
    "\n",
    "- **r**: The low-rank dimension, controlling the rank of the approximation.\n",
    "- **lora_alpha**: A scaling factor that adjusts the influence of the low-rank matrices.\n",
    "- **target_modules**: Specifies which parts of the model to apply LoRA to. We focus on the attention layers (`c_attn`), projection layers (`c_proj`), and fully connected layers (`c_fc`).\n",
    "- **lora_dropout**: A dropout rate applied to the LoRA layers to prevent overfitting.\n",
    "- **bias**: We specify `none` to leave bias terms untouched by LoRA updates.\n",
    "- **task_type**: Defines the task we're targeting. In this case, we set it to `CAUSAL_LM` for causal language modeling.\n",
    "- **modules_to_save**: Lists the modules (like the final LayerNorm and the language modeling head) that will be saved without LoRA adaptation.\n",
    "\n",
    "After configuring the settings, the LoRA configuration is printed to confirm the setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w84x6O75_4BU",
    "outputId": "5b77b6c1-391a-4e99-b27e-6980b7d96b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config created!\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                              # Low-rank dimension\n",
    "    lora_alpha=16,                    # Scaling factor\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # Targeted modules\n",
    "    lora_dropout=0.1,                 # Dropout rate for LoRA layers\n",
    "    bias=\"none\",                      # Leave biases untouched\n",
    "    task_type=\"CAUSAL_LM\",            # Task type\n",
    "    modules_to_save=[\"ln_f\", \"lm_head\"]  # Keep LayerNorm and classifier untouched\n",
    ")\n",
    "\n",
    "# Confirmation message\n",
    "print(\"LoRA config created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMEzhHTRw8yV"
   },
   "source": [
    "Apply the config to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFsAGydKCNep",
    "outputId": "820d9a63-2c5f-45cc-8bc6-0d7f2b32d554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA applied to the quantized model!\n"
     ]
    }
   ],
   "source": [
    "model_nf4_lora = get_peft_model(model_nf4, lora_config)\n",
    "\n",
    "print(\"LoRA applied to the quantized model!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X17m8JvPw__9"
   },
   "source": [
    "## 4-Configuring Training Arguments\n",
    "\n",
    "In this step, we configure the training arguments using the `TrainingArguments` from the Hugging Face `transformers` library. These settings control various aspects of the training process, such as where to save the model, how often to evaluate, and the optimization parameters. Here's an explanation of each parameter:\n",
    "\n",
    "- **output_dir**: Directory where model checkpoints will be saved.\n",
    "- **evaluation_strategy**: Specifies when to perform evaluation. Here, we evaluate the model at the end of each epoch.\n",
    "- **save_strategy**: Defines when to save the model. In this case, we save the model after each epoch.\n",
    "- **learning_rate**: The learning rate for the optimizer, which controls the step size during optimization.\n",
    "- **per_device_train_batch_size**: The batch size used during training on each device.\n",
    "- **per_device_eval_batch_size**: The batch size used during evaluation on each device.\n",
    "- **num_train_epochs**: Number of epochs to train the model.\n",
    "- **weight_decay**: Regularization parameter that helps prevent overfitting.\n",
    "- **logging_dir**: Directory where training logs will be saved.\n",
    "- **logging_steps**: Defines how frequently the metrics should be logged (every 10 steps in this case).\n",
    "- **load_best_model_at_end**: Ensures the best model (based on evaluation) is loaded at the end of training.\n",
    "\n",
    "Once configured, these arguments will be used to control the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PNxpf4MQEDF",
    "outputId": "281ecdf2-ff37-4130-bf34-259e7d5d6f25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Directory for saving model checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # Save model at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=2,   # Batch size for training\n",
    "    per_device_eval_batch_size=2,    # Batch size for evaluation\n",
    "    num_train_epochs=5,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",            # Directory for logs\n",
    "    logging_steps=10,                # Log metrics every 10 steps\n",
    "    load_best_model_at_end=True,     # Load the best model after training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "177IR-FxxmYl"
   },
   "source": [
    "Setting up the trainer with the training args  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElWTgv8p1gh9"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_nf4_lora,                # PEFT fine-tuned model\n",
    "    args=training_args,              # Training arguments\n",
    "    train_dataset=train_dataset,     # Training dataset\n",
    "    eval_dataset=val_dataset,        # Validation dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t27MRB3NxxK3"
   },
   "source": [
    "## And Finally, We TRAIN!!! üöÄ\n",
    "\n",
    "See how simple and convenient it is to set up training with Hugging Face's Transformers and PEFT! But now, the difference is‚Äîyou have a **pretty good understanding** of what's going on **under the hood**. You've configured the tokenizer, LoRA, and training arguments, giving you full control over your training process.\n",
    "\n",
    "What's even more exciting is that with this knowledge, you can **appreciate** the **amazing engineering** happening behind the scenes in these open-source libraries, especially in **Hugging Face**, which is truly pushing the **state-of-the-art (SOTA)** in the field of natural language processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "bqMuhlxaD_B1",
    "outputId": "c8ad5506-1e0b-404a-8cb0-3ab813be3819"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241119_211313-4wu45aae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/silvapi1994-karabuk-university/huggingface/runs/4wu45aae' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/silvapi1994-karabuk-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/silvapi1994-karabuk-university/huggingface' target=\"_blank\">https://wandb.ai/silvapi1994-karabuk-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/silvapi1994-karabuk-university/huggingface/runs/4wu45aae' target=\"_blank\">https://wandb.ai/silvapi1994-karabuk-university/huggingface/runs/4wu45aae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='281' max='110505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   281/110505 30:12 < 198:52:58, 0.15 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='867' max='110505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   867/110505 1:33:41 < 197:55:34, 0.15 it/s, Epoch 0.04/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nisuqqR-zmgo"
   },
   "source": [
    "## Congratulations on Completing This Milestone! üéâ\n",
    "\n",
    "What an incredible achievement! üéâ You've successfully completed your **third notebook on fine-tuning**, and that‚Äôs no small feat! Fine-tuning is one of the most crucial weapons in your **LLM arsenal**, and by mastering LoRA and QLoRA, you've unlocked new levels of efficiency in adapting large language models. You've come a long way, and your journey in the world of deep learning continues to be both thrilling and rewarding.\n",
    "\n",
    "### Summary: What We Covered\n",
    "\n",
    "In this notebook, we explored **LoRA** (Low-Rank Adaptation) and **QLoRA** (Quantized LoRA) in detail, understanding how these techniques make the fine-tuning process more efficient while retaining the model's performance. Here‚Äôs a recap of what we accomplished:\n",
    "\n",
    "1. **Understanding LoRA and QLoRA**:  \n",
    "   We started by diving deep into LoRA and QLoRA, both designed to inject low-rank updates into pre-trained models, significantly reducing the number of parameters that need to be fine-tuned.\n",
    "\n",
    "2. **Applying LoRA to the Alpaca Dataset**:  \n",
    "   We successfully applied LoRA and QLoRA to the **Alpaca dataset**, modifying the model with low-rank parameters and achieving efficient fine-tuning.\n",
    "\n",
    "3. **Three Methods of Implementation**:  \n",
    "   - **Manual Model Modifications**:\n",
    "     We manually changed the model's source code to incorporate LoRA and QLoRA, understanding the mechanics of the process.  \n",
    "   - **Using a Compact Class**:  \n",
    "     We built a compact class to facilitate the injection of LoRA and QLoRA, streamlining the process.  \n",
    "   - **Production-Ready Approach**:  \n",
    "     Finally, we used **Hugging Face**, **Bits and Bytes**, and **PEFT** for a more production-oriented, scalable solution for fine-tuning.\n",
    "\n",
    "### What‚Äôs Next?\n",
    "\n",
    "We‚Äôve come a long way, and I want to thank you for riding along on this journey with me! üöÄ But we still have several key aspects of LLMs to address. Here's a sneak peek at what‚Äôs to come:\n",
    "\n",
    "- **Inference**:  \n",
    "  Although we‚Äôve done some inference on our trained models and even built a simple UI for it, it‚Äôs time to dive into **optimization techniques** for smoother, more efficient inference deployment. This will improve both the speed and resource utilization during deployment.\n",
    "\n",
    "- **Models**:  \n",
    "  So far, we‚Äôve focused on **GPT-like architectures**, but there are still other important models like **LLaMA** and **BERT** to cover. Don't worry‚Äîit will be much easier now that you have a solid grasp of LLM concepts.\n",
    "\n",
    "- **RAG (Retrieval-Augmented Generation)**:  \n",
    "  I‚Äôm considering diving into this topic next. It‚Äôs more of an **engineering solution** than pure deep learning, but it‚Äôs a critical part of modern LLM applications. Stay tuned!\n",
    "\n",
    "I‚Äôm excited to keep progressing with you. The world of LLMs is vast, and we‚Äôve only scratched the surface. Let‚Äôs keep pushing forward! üí°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz_k_-AnznIG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "052938e031ae466181d25800f7043a09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48e8a83e57a84d89bf386fca6224e168",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_61c1561376f64599b2e90e4928cbc013",
      "value": "merges.txt:‚Äá100%"
     }
    },
    "074e10022dad42278e8a385a89a3d689": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "077174bb6e524ab2b7c649ed0d5d80ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08ca42b7858e489fb19c80e1fdf6f65a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3a903ce4e4146e49d1f718a942a380d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a32b755c148a4fad857b756ed3a2f2ed",
      "value": "vocab.json:‚Äá100%"
     }
    },
    "0cbcd612636d420ca62706a25cfe9779": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f21f6622eb4401c9e0c6c5bf9a4c08d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8beb7bbb4c14977a04e187f07872506",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f7fc6f92712453db6bf545f93e90781",
      "value": 1355256
     }
    },
    "0fbd40abe7bc4f5581ec9d06ffcc214e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35c47dbcefd44f579c594d9f92186b1c",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f0e77eb93c8b47cd91e9efec37dfea4f",
      "value": 26
     }
    },
    "105b5a5502f04adcb9f31c91ed524258": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14e91a525f9e49978a09a2c519bed716": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bb870bfb3a544e2b6fabfaefc2c6382": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d72bea62bee413684318f5043d574db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d83834a29d84ce58fe3394a915787e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08ca42b7858e489fb19c80e1fdf6f65a",
       "IPY_MODEL_5b83f62050ac45b5be691968060f472a",
       "IPY_MODEL_adbed114c062428c80ee217da5100e88"
      ],
      "layout": "IPY_MODEL_1bb870bfb3a544e2b6fabfaefc2c6382"
     }
    },
    "1ffca17046e24dfeb73ce5e38110ff6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a28de2e45184dd1afd917bdb8596e14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7c4a4558ecb4178b32d550858cdf24a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9a72aee2787146188abf81f08f5c5c78",
      "value": "‚Äá124/124‚Äá[00:00&lt;00:00,‚Äá6.99kB/s]"
     }
    },
    "2ad150411c1b472d9d762a94d3dc4e8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b7f2572ee2f492c8e2a2a18d0c11b6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91d6a2b572f84e7eb77c2fa0edf1f197",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_62983b76354f452e9a2ab2eff51b66c4",
      "value": "‚Äá1.04M/1.04M‚Äá[00:00&lt;00:00,‚Äá1.48MB/s]"
     }
    },
    "2c0cba92c3bd44cfa33dcd894b1953f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c6db44a74b54e549379cffff3a3736c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3050796c442f4187b3b4d57142ae368b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "344050bd05c841e18eebbf71a2dee312": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1ad86dc630f49f58172898b1e82b03b",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7944eecd664043618d086a137d85e6d6",
      "value": 456318
     }
    },
    "35c47dbcefd44f579c594d9f92186b1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "390e2e4757d74b67ade614a36fbf0ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43881b3a6e034e5b9b255437fb71ea7f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_41ebca76a3134e67975fbd4ff1fac6ef",
      "value": "vocab.json:‚Äá100%"
     }
    },
    "3bf8115f84224333b4ae73e778f9465e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fd082a94e20495ca42930e2c4c0aaa2",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_421521f674664eea94f6f1a3fdd4ecf1",
      "value": "merges.txt:‚Äá100%"
     }
    },
    "3dd5010265954c38b762c29082fe9143": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_074e10022dad42278e8a385a89a3d689",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dacc01e53cf84a2aba9d4a1d39b97d4a",
      "value": 124
     }
    },
    "3f340a0e28664c0097c61e6ffcaa834d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4097b564c5a74a29835becc504db6682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41ebca76a3134e67975fbd4ff1fac6ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "421521f674664eea94f6f1a3fdd4ecf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42fd8c94e31f49e0802fb12b78492df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43881b3a6e034e5b9b255437fb71ea7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "456275993bfc453f8cf9296579478a4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "45eea6aca58e4f76ab137b66052e688e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e76d3b0b0f2445c88861c40385fa680f",
       "IPY_MODEL_9e9afb14e1544b0eb12e37dd211a0307",
       "IPY_MODEL_9c9af8bf6aba4e8fb2cd87a2a377e7a5"
      ],
      "layout": "IPY_MODEL_6cd1041767904f6ea7f4b4fcb9190f61"
     }
    },
    "467a1c43b01e42da92a89c11ae90f8c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48e8a83e57a84d89bf386fca6224e168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c548ec48cf1455895f7a54f0498b5e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4b56ebde5904b2e9f7e281d681b1aaf",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4097b564c5a74a29835becc504db6682",
      "value": 456318
     }
    },
    "4f66a5b23687484e919df70031026b7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d66ebaa704e8499da9f1158b4865cabb",
       "IPY_MODEL_3dd5010265954c38b762c29082fe9143",
       "IPY_MODEL_2a28de2e45184dd1afd917bdb8596e14"
      ],
      "layout": "IPY_MODEL_42fd8c94e31f49e0802fb12b78492df1"
     }
    },
    "4fa96b6608d545b08a119a2e6a04942f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_97a9902a8ac5467581d3144fcfb6502d",
       "IPY_MODEL_0f21f6622eb4401c9e0c6c5bf9a4c08d",
       "IPY_MODEL_6cb2353b59d24bc3994eee4d26ea7ab5"
      ],
      "layout": "IPY_MODEL_54dec3e8699848f9aaa4ed47bf1cf522"
     }
    },
    "4ffc13a5c2024ff6b4ac8998a8364017": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c6db44a74b54e549379cffff3a3736c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d42595d98e0a418ab72a6187a53a0cca",
      "value": "‚Äá6.43G/6.43G‚Äá[00:36&lt;00:00,‚Äá251MB/s]"
     }
    },
    "54382fd7200a4477bf80eb2c0ba72192": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94c5d44b4f2843a9943323be9446de41",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c08c932aa7b44c919aac6fa9e17ac49c",
      "value": 1042301
     }
    },
    "54dec3e8699848f9aaa4ed47bf1cf522": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b83f62050ac45b5be691968060f472a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dea1e28509a2498ea973b4f6fa0fa28d",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ffca17046e24dfeb73ce5e38110ff6f",
      "value": 1042301
     }
    },
    "5efbe24c06f74267963d32461f7281e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61c1561376f64599b2e90e4928cbc013": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62983b76354f452e9a2ab2eff51b66c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66d4cb4ddfca489faf284ff66ab756ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4bfa98efa0240d3871398575ade8880",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_77baa7e8eaef459a8bc84d4ccd14f887",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "68b3f2079804426ca6b7b24369c777d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69e6895a5c324b6f8226e8cde9fd140a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_754296f1b020494290bb8772cd3071f9",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_456275993bfc453f8cf9296579478a4f",
      "value": 1355256
     }
    },
    "6bf936caaff44d3cb87bcb401fcb5b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3faeaad66ef42afb0f8fd50e7ea7362",
      "max": 6431829964,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cbc3169942e54b79850f467173fa98ab",
      "value": 6431829964
     }
    },
    "6cb2353b59d24bc3994eee4d26ea7ab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94e1ce57ca244b1c8a5d4dc749c9be1b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_105b5a5502f04adcb9f31c91ed524258",
      "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá5.34MB/s]"
     }
    },
    "6cd1041767904f6ea7f4b4fcb9190f61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e4083f264794b039a98295a3be84703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d2b09634f59426287b07765210cbe48",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bbffa17f92174c74919c634523667d5a",
      "value": "‚Äá26.0/26.0‚Äá[00:00&lt;00:00,‚Äá1.13kB/s]"
     }
    },
    "6f51113c8aa4417cb426b4e6610b679b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f61aa230ea54f86b214293b1720010a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fbeb9b5953c409f9c1e0bf2d4a84a62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fd082a94e20495ca42930e2c4c0aaa2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "754296f1b020494290bb8772cd3071f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76c0a8e7160646cab895c37bd0e18709": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77baa7e8eaef459a8bc84d4ccd14f887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7944eecd664043618d086a137d85e6d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7d2b09634f59426287b07765210cbe48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "811fb8cff6c349acaf2c529a957f06aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ded425cd004fde91d0ae174473f5cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c2751a47a3a456ba8601bbd3d7f09db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e584d1badbea48f98508b0c0bb75a69f",
       "IPY_MODEL_69e6895a5c324b6f8226e8cde9fd140a",
       "IPY_MODEL_ea1b7ca260db4bf6bea88324e5de465a"
      ],
      "layout": "IPY_MODEL_bbd161fa39d54872ab77c43973f10154"
     }
    },
    "91d6a2b572f84e7eb77c2fa0edf1f197": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92c0342f233649ef84a9701c42f37b9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9346f887049f45ce8e6807fdb9944db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14e91a525f9e49978a09a2c519bed716",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a11ec819807f4421868c0b739b8341e0",
      "value": "‚Äá689/689‚Äá[00:00&lt;00:00,‚Äá13.9kB/s]"
     }
    },
    "94043165d53a40078d35a28784f416dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3bf8115f84224333b4ae73e778f9465e",
       "IPY_MODEL_344050bd05c841e18eebbf71a2dee312",
       "IPY_MODEL_d6fbc59c646b451e8cbdcd26a8017be1"
      ],
      "layout": "IPY_MODEL_077174bb6e524ab2b7c649ed0d5d80ca"
     }
    },
    "94c5d44b4f2843a9943323be9446de41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94e1ce57ca244b1c8a5d4dc749c9be1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97a9902a8ac5467581d3144fcfb6502d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_811fb8cff6c349acaf2c529a957f06aa",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_467a1c43b01e42da92a89c11ae90f8c7",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "980824867d9e41c5b28f02f8a383ea17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9871bf19abef4ea99c7b697c12ef2e3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df1ed03149814f2caf834693cc6720c1",
      "max": 689,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3050796c442f4187b3b4d57142ae368b",
      "value": 689
     }
    },
    "9a00fb2b9da0486094f34e7a5e295dbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cbcd612636d420ca62706a25cfe9779",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_87ded425cd004fde91d0ae174473f5cc",
      "value": "config.json:‚Äá100%"
     }
    },
    "9a72aee2787146188abf81f08f5c5c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c1a394fb1fb45fe97f80aacc51e911c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c9af8bf6aba4e8fb2cd87a2a377e7a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fbeb9b5953c409f9c1e0bf2d4a84a62",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ce678aef93d04fabbb0a8bf15fc0ca72",
      "value": "‚Äá665/665‚Äá[00:00&lt;00:00,‚Äá43.8kB/s]"
     }
    },
    "9ce22d26bdf44013996486faf871be3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_66d4cb4ddfca489faf284ff66ab756ef",
       "IPY_MODEL_aa8aa9af31814e71ae154bc1b258a1e0",
       "IPY_MODEL_a284ef5c8a904858bed09f86f905a5fa"
      ],
      "layout": "IPY_MODEL_3f340a0e28664c0097c61e6ffcaa834d"
     }
    },
    "9d8e297150454217b5f88d962056d61d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e9afb14e1544b0eb12e37dd211a0307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f51113c8aa4417cb426b4e6610b679b",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b634f8cc57364893ab63516c6fd24b8e",
      "value": 665
     }
    },
    "9ec8c4de5a474be7be5f2e5ffd9be5fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f7fc6f92712453db6bf545f93e90781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a11ec819807f4421868c0b739b8341e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a284ef5c8a904858bed09f86f905a5fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af8784c247e34297b8348f72cdd70dae",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9ec8c4de5a474be7be5f2e5ffd9be5fc",
      "value": "‚Äá26.0/26.0‚Äá[00:00&lt;00:00,‚Äá890B/s]"
     }
    },
    "a32b755c148a4fad857b756ed3a2f2ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a33f45bb990f4f788afe8cdae0a47a11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4b56ebde5904b2e9f7e281d681b1aaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a98ff2d5268049cab0e0d72aa0a9ff61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa8aa9af31814e71ae154bc1b258a1e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e06b49e11a9741b8a2360d5924d63f3c",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c584dc1b243e4495ba6ec09c80e0ecd5",
      "value": 26
     }
    },
    "adbed114c062428c80ee217da5100e88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae37131dfb5543a9aa975ba0d7b3c684",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9d8e297150454217b5f88d962056d61d",
      "value": "‚Äá1.04M/1.04M‚Äá[00:00&lt;00:00,‚Äá22.0MB/s]"
     }
    },
    "ae37131dfb5543a9aa975ba0d7b3c684": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae75793d57fc46efb9618dc2fd4140b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_052938e031ae466181d25800f7043a09",
       "IPY_MODEL_4c548ec48cf1455895f7a54f0498b5e5",
       "IPY_MODEL_bcc3e06319e445ac9cee544e11fe0bca"
      ],
      "layout": "IPY_MODEL_f9ead382ec11442f8a0e7e0fd18656e0"
     }
    },
    "af8784c247e34297b8348f72cdd70dae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1ad86dc630f49f58172898b1e82b03b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3faeaad66ef42afb0f8fd50e7ea7362": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b634f8cc57364893ab63516c6fd24b8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bb897481bf0744348eeb2220c474a3ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbd161fa39d54872ab77c43973f10154": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbffa17f92174c74919c634523667d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcc3e06319e445ac9cee544e11fe0bca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2a765b16ca84263ad6f8e3b9a51ea24",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fd44a05c1b1040a3b888cc83ef8da2c1",
      "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá2.14MB/s]"
     }
    },
    "c08c932aa7b44c919aac6fa9e17ac49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c19e1ba09658415d86cc7c66b7c769ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb897481bf0744348eeb2220c474a3ee",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2ad150411c1b472d9d762a94d3dc4e8d",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "c4bfa98efa0240d3871398575ade8880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c584dc1b243e4495ba6ec09c80e0ecd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8beb7bbb4c14977a04e187f07872506": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbc3169942e54b79850f467173fa98ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce678aef93d04fabbb0a8bf15fc0ca72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3a903ce4e4146e49d1f718a942a380d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d42595d98e0a418ab72a6187a53a0cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d66ebaa704e8499da9f1158b4865cabb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5efbe24c06f74267963d32461f7281e9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6f61aa230ea54f86b214293b1720010a",
      "value": "generation_config.json:‚Äá100%"
     }
    },
    "d6fbc59c646b451e8cbdcd26a8017be1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a98ff2d5268049cab0e0d72aa0a9ff61",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fd91d931b3a94c508f723d8000c3d74b",
      "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá667kB/s]"
     }
    },
    "d7b742b3d6a54dfe9b522c75a149c0ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_980824867d9e41c5b28f02f8a383ea17",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e5e3ffae9a9a4926a945a5b912085e5c",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "dacc01e53cf84a2aba9d4a1d39b97d4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc16b8d4845d4edb86853819ff9f368a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a00fb2b9da0486094f34e7a5e295dbc",
       "IPY_MODEL_9871bf19abef4ea99c7b697c12ef2e3a",
       "IPY_MODEL_9346f887049f45ce8e6807fdb9944db9"
      ],
      "layout": "IPY_MODEL_a33f45bb990f4f788afe8cdae0a47a11"
     }
    },
    "dea1e28509a2498ea973b4f6fa0fa28d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df1ed03149814f2caf834693cc6720c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e06b49e11a9741b8a2360d5924d63f3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0cc42735e5a44e4bd8d9b483da11463": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2a765b16ca84263ad6f8e3b9a51ea24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e584d1badbea48f98508b0c0bb75a69f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68b3f2079804426ca6b7b24369c777d0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fceefefe2ae64c4ea97a62e64c591227",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "e5e3ffae9a9a4926a945a5b912085e5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e76d3b0b0f2445c88861c40385fa680f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d72bea62bee413684318f5043d574db",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9c1a394fb1fb45fe97f80aacc51e911c",
      "value": "config.json:‚Äá100%"
     }
    },
    "e7c4a4558ecb4178b32d550858cdf24a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea1b7ca260db4bf6bea88324e5de465a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0cc42735e5a44e4bd8d9b483da11463",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_92c0342f233649ef84a9701c42f37b9d",
      "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá1.96MB/s]"
     }
    },
    "ed37ab9bbcd1490da3c0af4d8e3f16ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c19e1ba09658415d86cc7c66b7c769ad",
       "IPY_MODEL_6bf936caaff44d3cb87bcb401fcb5b05",
       "IPY_MODEL_4ffc13a5c2024ff6b4ac8998a8364017"
      ],
      "layout": "IPY_MODEL_2c0cba92c3bd44cfa33dcd894b1953f8"
     }
    },
    "f0e77eb93c8b47cd91e9efec37dfea4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8518c3822f945e2bcb665eeca6270c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8fa288daadf487b8d809f30b06e195a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_390e2e4757d74b67ade614a36fbf0ca0",
       "IPY_MODEL_54382fd7200a4477bf80eb2c0ba72192",
       "IPY_MODEL_2b7f2572ee2f492c8e2a2a18d0c11b6e"
      ],
      "layout": "IPY_MODEL_76c0a8e7160646cab895c37bd0e18709"
     }
    },
    "f9b9bd3f77ec4912a4b465d8b226e79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d7b742b3d6a54dfe9b522c75a149c0ef",
       "IPY_MODEL_0fbd40abe7bc4f5581ec9d06ffcc214e",
       "IPY_MODEL_6e4083f264794b039a98295a3be84703"
      ],
      "layout": "IPY_MODEL_f8518c3822f945e2bcb665eeca6270c5"
     }
    },
    "f9ead382ec11442f8a0e7e0fd18656e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fceefefe2ae64c4ea97a62e64c591227": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd44a05c1b1040a3b888cc83ef8da2c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd91d931b3a94c508f723d8000c3d74b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
